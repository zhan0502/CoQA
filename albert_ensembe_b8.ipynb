{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NB54koJNRUmB"
   },
   "source": [
    "https://huggingface.co/transformers/_modules/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 125977,
     "status": "ok",
     "timestamp": 1619447114246,
     "user": {
      "displayName": "Ning Zhang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhRCnmT8mmairzuShAibDId7kDGmrCXjp8yIjrCOw=s64",
      "userId": "12536691088853738123"
     },
     "user_tz": -480
    },
    "id": "MyFzK7mFiW-6",
    "outputId": "ed665f55-f04e-4b8a-8efd-ef341f1290e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n",
      "/content/gdrive/My Drive/AI Sem II/NLP/Project Test/transformers-coqa-master\n",
      "/content/gdrive/My Drive/AI Sem II/NLP/Project Test/transformers-coqa-master\n"
     ]
    }
   ],
   "source": [
    "# For Google Colaboratory\n",
    "import sys, os\n",
    "if 'google.colab' in sys.modules:\n",
    "    # mount google drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    " \n",
    "    path_to_file = '/content/gdrive/My Drive/AI Sem II/NLP/Project Test/'\n",
    "    print(path_to_file)\n",
    "    # change current path to the folder containing \"file_name\"\n",
    "    os.chdir(path_to_file)\n",
    "    !pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 42941,
     "status": "ok",
     "timestamp": 1619447157237,
     "user": {
      "displayName": "Ning Zhang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhRCnmT8mmairzuShAibDId7kDGmrCXjp8yIjrCOw=s64",
      "userId": "12536691088853738123"
     },
     "user_tz": -480
    },
    "id": "MYFgNG28jXnW",
    "outputId": "a3b94381-718b-493f-8d0b-786587f60729"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/huggingface/transformers\n",
      "  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-f954ibne\n",
      "  Running command git clone -q https://github.com/huggingface/transformers /tmp/pip-req-build-f954ibne\n",
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0.dev0) (2.23.0)\n",
      "Collecting huggingface-hub>=0.0.8\n",
      "  Downloading https://files.pythonhosted.org/packages/a1/88/7b1e45720ecf59c6c6737ff332f41c955963090a18e72acbcbeac6b25e86/huggingface_hub-0.0.8-py3-none-any.whl\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0.dev0) (2019.12.20)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0.dev0) (3.10.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0.dev0) (1.19.5)\n",
      "Collecting tokenizers<0.11,>=0.10.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n",
      "\u001b[K     |████████████████████████████████| 3.3MB 7.2MB/s \n",
      "\u001b[?25hCollecting sacremoses\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
      "\u001b[K     |████████████████████████████████| 901kB 43.8MB/s \n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0.dev0) (3.0.12)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0.dev0) (4.41.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0.dev0) (20.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.6.0.dev0) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.6.0.dev0) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.6.0.dev0) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.6.0.dev0) (2020.12.5)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.6.0.dev0) (3.4.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.6.0.dev0) (3.7.4.3)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.6.0.dev0) (1.0.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.6.0.dev0) (1.15.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.6.0.dev0) (7.1.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.6.0.dev0) (2.4.7)\n",
      "Building wheels for collected packages: transformers\n",
      "  Building wheel for transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for transformers: filename=transformers-4.6.0.dev0-cp37-none-any.whl size=2122482 sha256=1c4d0a12d2754494bff508a0c6485246ac09d37c154f11961bedfb72b33ce666\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-mlq2_qtn/wheels/70/d3/52/b3fa4f8b8ef04167ac62e5bb2accb62ae764db2a378247490e\n",
      "Successfully built transformers\n",
      "Installing collected packages: huggingface-hub, tokenizers, sacremoses, transformers\n",
      "Successfully installed huggingface-hub-0.0.8 sacremoses-0.0.45 tokenizers-0.10.2 transformers-4.6.0.dev0\n",
      "Collecting spacy\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1b/d8/0361bbaf7a1ff56b44dca04dace54c82d63dad7475b7d25ea1baefafafb2/spacy-3.0.6-cp37-cp37m-manylinux2014_x86_64.whl (12.8MB)\n",
      "\u001b[K     |████████████████████████████████| 12.8MB 4.1MB/s \n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: typing-extensions<4.0.0.0,>=3.7.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from spacy) (3.7.4.3)\n",
      "Requirement already satisfied, skipping upgrade: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied, skipping upgrade: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (20.9)\n",
      "Collecting catalogue<2.1.0,>=2.0.3\n",
      "  Downloading https://files.pythonhosted.org/packages/82/a5/b5021c74c04cac35a27d34cbf3146d86eb8e173b4491888bc4908c4c8b3b/catalogue-2.0.3-py3-none-any.whl\n",
      "Requirement already satisfied, skipping upgrade: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.11.3)\n",
      "Requirement already satisfied, skipping upgrade: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.8.2)\n",
      "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (56.0.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.19.5)\n",
      "Collecting thinc<8.1.0,>=8.0.3\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/61/87/decceba68a0c6ca356ddcb6aea8b2500e71d9bc187f148aae19b747b7d3c/thinc-8.0.3-cp37-cp37m-manylinux2014_x86_64.whl (1.1MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1MB 46.0MB/s \n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
      "Collecting pydantic<1.8.0,>=1.7.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b3/0a/52ae1c659fc08f13dd7c0ae07b88e4f807ad83fb9954a59b0b0a3d1a8ab6/pydantic-1.7.3-cp37-cp37m-manylinux2014_x86_64.whl (9.1MB)\n",
      "\u001b[K     |████████████████████████████████| 9.1MB 39.3MB/s \n",
      "\u001b[?25hCollecting pathy>=0.3.5\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/13/87/5991d87be8ed60beb172b4062dbafef18b32fa559635a8e2b633c2974f85/pathy-0.5.2-py3-none-any.whl (42kB)\n",
      "\u001b[K     |████████████████████████████████| 51kB 6.3MB/s \n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.5)\n",
      "Requirement already satisfied, skipping upgrade: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.41.1)\n",
      "Collecting srsly<3.0.0,>=2.4.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c3/84/dfdfc9f6f04f6b88207d96d9520b911e5fec0c67ff47a0dea31ab5429a1e/srsly-2.4.1-cp37-cp37m-manylinux2014_x86_64.whl (456kB)\n",
      "\u001b[K     |████████████████████████████████| 460kB 34.2MB/s \n",
      "\u001b[?25hCollecting typer<0.4.0,>=0.3.0\n",
      "  Downloading https://files.pythonhosted.org/packages/90/34/d138832f6945432c638f32137e6c79a3b682f06a63c488dcfaca6b166c64/typer-0.3.2-py3-none-any.whl\n",
      "Requirement already satisfied, skipping upgrade: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.5)\n",
      "Requirement already satisfied, skipping upgrade: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.4\n",
      "  Downloading https://files.pythonhosted.org/packages/8d/67/d4002a18e26bf29b17ab563ddb55232b445ab6a02f97bf17d1345ff34d3f/spacy_legacy-3.0.5-py2.py3-none-any.whl\n",
      "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy) (2.4.7)\n",
      "Requirement already satisfied, skipping upgrade: zipp>=0.5; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.3->spacy) (3.4.1)\n",
      "Requirement already satisfied, skipping upgrade: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy) (1.1.1)\n",
      "Collecting smart-open<4.0.0,>=2.2.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/11/9a/ba2d5f67f25e8d5bbf2fcec7a99b1e38428e83cb715f64dd179ca43a11bb/smart_open-3.0.0.tar.gz (113kB)\n",
      "\u001b[K     |████████████████████████████████| 122kB 51.9MB/s \n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.0->spacy) (7.1.2)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.12.5)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
      "Building wheels for collected packages: smart-open\n",
      "  Building wheel for smart-open (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for smart-open: filename=smart_open-3.0.0-cp37-none-any.whl size=107098 sha256=0467007aeb0f058c27607255acbb921bf61650bfb7ec29158c3335a1a9af44c0\n",
      "  Stored in directory: /root/.cache/pip/wheels/18/88/7c/f06dabd5e9cabe02d2269167bcacbbf9b47d0c0ff7d6ebcb78\n",
      "Successfully built smart-open\n",
      "Installing collected packages: catalogue, pydantic, srsly, thinc, typer, smart-open, pathy, spacy-legacy, spacy\n",
      "  Found existing installation: catalogue 1.0.0\n",
      "    Uninstalling catalogue-1.0.0:\n",
      "      Successfully uninstalled catalogue-1.0.0\n",
      "  Found existing installation: srsly 1.0.5\n",
      "    Uninstalling srsly-1.0.5:\n",
      "      Successfully uninstalled srsly-1.0.5\n",
      "  Found existing installation: thinc 7.4.0\n",
      "    Uninstalling thinc-7.4.0:\n",
      "      Successfully uninstalled thinc-7.4.0\n",
      "  Found existing installation: smart-open 5.0.0\n",
      "    Uninstalling smart-open-5.0.0:\n",
      "      Successfully uninstalled smart-open-5.0.0\n",
      "  Found existing installation: spacy 2.2.4\n",
      "    Uninstalling spacy-2.2.4:\n",
      "      Successfully uninstalled spacy-2.2.4\n",
      "Successfully installed catalogue-2.0.3 pathy-0.5.2 pydantic-1.7.3 smart-open-3.0.0 spacy-3.0.6 spacy-legacy-3.0.5 srsly-2.4.1 thinc-8.0.3 typer-0.3.2\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/huggingface/transformers\n",
    "!pip install -U spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 49282,
     "status": "ok",
     "timestamp": 1619447168249,
     "user": {
      "displayName": "Ning Zhang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhRCnmT8mmairzuShAibDId7kDGmrCXjp8yIjrCOw=s64",
      "userId": "12536691088853738123"
     },
     "user_tz": -480
    },
    "id": "W86G1qWTjwdc",
    "outputId": "decbaa82-e4b0-4010-c06e-07da5f81b33f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;3m⚠ As of spaCy v3.0, shortcuts like 'en' are deprecated. Please use the\n",
      "full pipeline package name 'en_core_web_sm' instead.\u001b[0m\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import string\n",
    "import transformers\n",
    "import re\n",
    "from tqdm import tqdm, trange\n",
    "import spacy \n",
    "import spacy.cli\n",
    "import logging\n",
    "import collections\n",
    "from collections import Counter\n",
    "import math\n",
    "import torch\n",
    "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler, TensorDataset)\n",
    "from transformers import BertModel, BertTokenizer, AdamW\n",
    "#from pytorch_pretrained_bert.modeling import BertModel, BertPretrainedModel\n",
    "#from pytorch_pretrained_bert.tokenization import BertTokenizer\n",
    "#from pytorch_pretrained_bert.optimization import BertAdam, WarmupLinearSchedule\n",
    "#from pytorch_ptrtrained_bert.tokenization import BasicTokenizer, whitespace_tokenize\n",
    "from transformers.models.bert.tokenization_bert import BasicTokenizer\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import json\n",
    "spacy.cli.download('en')\n",
    "spacy.load('en_core_web_sm')\n",
    "import random\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import matplotlib as pplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 50738,
     "status": "ok",
     "timestamp": 1619447172165,
     "user": {
      "displayName": "Ning Zhang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhRCnmT8mmairzuShAibDId7kDGmrCXjp8yIjrCOw=s64",
      "userId": "12536691088853738123"
     },
     "user_tz": -480
    },
    "id": "7kwXwuk3j0Sy",
    "outputId": "c10249b0-9677-4ee7-e1d0-61e4d380daf5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentencepiece\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
      "\r",
      "\u001b[K     |▎                               | 10kB 15.1MB/s eta 0:00:01\r",
      "\u001b[K     |▌                               | 20kB 10.9MB/s eta 0:00:01\r",
      "\u001b[K     |▉                               | 30kB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |█                               | 40kB 7.3MB/s eta 0:00:01\r",
      "\u001b[K     |█▍                              | 51kB 4.4MB/s eta 0:00:01\r",
      "\u001b[K     |█▋                              | 61kB 4.9MB/s eta 0:00:01\r",
      "\u001b[K     |██                              | 71kB 4.9MB/s eta 0:00:01\r",
      "\u001b[K     |██▏                             | 81kB 5.2MB/s eta 0:00:01\r",
      "\u001b[K     |██▌                             | 92kB 5.5MB/s eta 0:00:01\r",
      "\u001b[K     |██▊                             | 102kB 4.3MB/s eta 0:00:01\r",
      "\u001b[K     |███                             | 112kB 4.3MB/s eta 0:00:01\r",
      "\u001b[K     |███▎                            | 122kB 4.3MB/s eta 0:00:01\r",
      "\u001b[K     |███▌                            | 133kB 4.3MB/s eta 0:00:01\r",
      "\u001b[K     |███▉                            | 143kB 4.3MB/s eta 0:00:01\r",
      "\u001b[K     |████                            | 153kB 4.3MB/s eta 0:00:01\r",
      "\u001b[K     |████▍                           | 163kB 4.3MB/s eta 0:00:01\r",
      "\u001b[K     |████▋                           | 174kB 4.3MB/s eta 0:00:01\r",
      "\u001b[K     |█████                           | 184kB 4.3MB/s eta 0:00:01\r",
      "\u001b[K     |█████▏                          | 194kB 4.3MB/s eta 0:00:01\r",
      "\u001b[K     |█████▌                          | 204kB 4.3MB/s eta 0:00:01\r",
      "\u001b[K     |█████▊                          | 215kB 4.3MB/s eta 0:00:01\r",
      "\u001b[K     |██████                          | 225kB 4.3MB/s eta 0:00:01\r",
      "\u001b[K     |██████▎                         | 235kB 4.3MB/s eta 0:00:01\r",
      "\u001b[K     |██████▌                         | 245kB 4.3MB/s eta 0:00:01\r",
      "\u001b[K     |██████▉                         | 256kB 4.3MB/s eta 0:00:01\r",
      "\u001b[K     |███████                         | 266kB 4.3MB/s eta 0:00:01\r",
      "\u001b[K     |███████▍                        | 276kB 4.3MB/s eta 0:00:01\r",
      "\u001b[K     |███████▋                        | 286kB 4.3MB/s eta 0:00:01\r",
      "\u001b[K     |████████                        | 296kB 4.3MB/s eta 0:00:01\r",
      "\u001b[K     |████████▏                       | 307kB 4.3MB/s eta 0:00:01\r",
      "\u001b[K     |████████▍                       | 317kB 4.3MB/s eta 0:00:01\r",
      "\u001b[K     |████████▊                       | 327kB 4.3MB/s eta 0:00:01\r",
      "\u001b[K     |█████████                       | 337kB 4.3MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▎                      | 348kB 4.3MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▌                      | 358kB 4.3MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▉                      | 368kB 4.3MB/s eta 0:00:01\r",
      "\u001b[K     |██████████                      | 378kB 4.3MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▍                     | 389kB 4.3MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▋                     | 399kB 4.3MB/s eta 0:00:01\r",
      "\u001b[K     |███████████                     | 409kB 4.3MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▏                    | 419kB 4.3MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▍                    | 430kB 4.3MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▊                    | 440kB 4.3MB/s eta 0:00:01\r",
      "\u001b[K     |████████████                    | 450kB 4.3MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▎                   | 460kB 4.3MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▌                   | 471kB 4.3MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▉                   | 481kB 4.3MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████                   | 491kB 4.3MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▍                  | 501kB 4.3MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▋                  | 512kB 4.3MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▉                  | 522kB 4.3MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▏                 | 532kB 4.3MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▍                 | 542kB 4.3MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▊                 | 552kB 4.3MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████                 | 563kB 4.3MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▎                | 573kB 4.3MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▌                | 583kB 4.3MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▉                | 593kB 4.3MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████                | 604kB 4.3MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▍               | 614kB 4.3MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▋               | 624kB 4.3MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▉               | 634kB 4.3MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▏              | 645kB 4.3MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▍              | 655kB 4.3MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▊              | 665kB 4.3MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████              | 675kB 4.3MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▎             | 686kB 4.3MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▌             | 696kB 4.3MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▉             | 706kB 4.3MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████             | 716kB 4.3MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▎            | 727kB 4.3MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▋            | 737kB 4.3MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▉            | 747kB 4.3MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▏           | 757kB 4.3MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▍           | 768kB 4.3MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▊           | 778kB 4.3MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████           | 788kB 4.3MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▎          | 798kB 4.3MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▌          | 808kB 4.3MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▉          | 819kB 4.3MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████          | 829kB 4.3MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▎         | 839kB 4.3MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▋         | 849kB 4.3MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▉         | 860kB 4.3MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▏        | 870kB 4.3MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▍        | 880kB 4.3MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▊        | 890kB 4.3MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████        | 901kB 4.3MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▎       | 911kB 4.3MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▌       | 921kB 4.3MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▊       | 931kB 4.3MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████       | 942kB 4.3MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▎      | 952kB 4.3MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▋      | 962kB 4.3MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▉      | 972kB 4.3MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▏     | 983kB 4.3MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▍     | 993kB 4.3MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▊     | 1.0MB 4.3MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████     | 1.0MB 4.3MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▎    | 1.0MB 4.3MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▌    | 1.0MB 4.3MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▊    | 1.0MB 4.3MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████    | 1.1MB 4.3MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▎   | 1.1MB 4.3MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▋   | 1.1MB 4.3MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▉   | 1.1MB 4.3MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▏  | 1.1MB 4.3MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▍  | 1.1MB 4.3MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▊  | 1.1MB 4.3MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████  | 1.1MB 4.3MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▏ | 1.1MB 4.3MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▌ | 1.1MB 4.3MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▊ | 1.2MB 4.3MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████ | 1.2MB 4.3MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▎| 1.2MB 4.3MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▋| 1.2MB 4.3MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▉| 1.2MB 4.3MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 1.2MB 4.3MB/s \n",
      "\u001b[?25hInstalling collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.1.95\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 114,
     "referenced_widgets": [
      "52cdac639c934378b29de9a43ea866e5",
      "10338a2cbc69478c85dfd02f0cba11e8",
      "2f7815e80abc4729b6bc674cedec8c8e",
      "4ce9c85e72624c34a5090f09663d470d",
      "baebbdd4b0564673b44b062deb887cfb",
      "fae6c8b6835042eba391d763c73e828f",
      "84dc144fd3aa430fb420750ce8f1591c",
      "f351803bc8ea402ba7ca255933908197",
      "788bae25caf74591864f3f4f2b32e5bb",
      "30bc2c43965c44379ded9bb74b067767",
      "e250780d9101411b84ea7fb8f107cd21",
      "a61a9d18df7d41f284f91cc33909bd37",
      "04b2d77fb0f9418c86dda5073c790d00",
      "6e8d7fdf087c4211816840a4a3dddfad",
      "663195f9a5e44822b6044593bfa95e63",
      "d8e4e790891d4f478edec319a58b173d"
     ]
    },
    "executionInfo": {
     "elapsed": 50464,
     "status": "ok",
     "timestamp": 1619447178639,
     "user": {
      "displayName": "Ning Zhang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhRCnmT8mmairzuShAibDId7kDGmrCXjp8yIjrCOw=s64",
      "userId": "12536691088853738123"
     },
     "user_tz": -480
    },
    "id": "d4WXOe-3j4SE",
    "outputId": "1b06c54b-9a80-46c8-9e5a-eb04e18c1d63"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52cdac639c934378b29de9a43ea866e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=760289.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "788bae25caf74591864f3f4f2b32e5bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1312669.0, style=ProgressStyle(descript…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "#from transformers.models.roberta.tokenization_roberta import RobertaTokenizer\n",
    "#import sentencepiece\n",
    "#bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "from transformers.models.albert.tokenization_albert import AlbertTokenizer\n",
    "bert_tokenizer = AlbertTokenizer.from_pretrained('albert-base-v2', do_lower_case=True)\n",
    "#model =BertModel.from_pretrained('bert-base-uncased')\n",
    "output_directory = 'outputs'\n",
    "\n",
    "# If output directory doesn't exist create one\n",
    "if not os.path.exists(output_directory):\n",
    "  os.makedirs(output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QL2tymD4lj_6"
   },
   "outputs": [],
   "source": [
    "#Class to store questions, their answers along with the starting index of the answer and end index of answer and tokens in the story\n",
    "class QA(object):\n",
    "  def __init__(\n",
    "            self,\n",
    "            question_answer_id,\n",
    "            question_text,\n",
    "            document_tokens,\n",
    "            original_answer_text=None,\n",
    "            answer_start_position=None,\n",
    "            answer_end_position=None,\n",
    "            additional_answers=None,\n",
    "    ):\n",
    "    self.question_answer_id = question_answer_id\n",
    "    self.question_text = question_text\n",
    "    self.document_tokens = document_tokens\n",
    "    self.original_answer_text = original_answer_text\n",
    "    self.answer_start_position = answer_start_position\n",
    "    self.answer_end_position = answer_end_position\n",
    "    self.additional_answers = additional_answers\n",
    "\n",
    "# Class to store features, input ids, input mask, segment ids, start positions, end positions, etc\n",
    "class DataFeatures(object):\n",
    "  def __init__(self,\n",
    "                 unique_id,\n",
    "                 example_index,\n",
    "                 document_span_index,\n",
    "                 tokens,\n",
    "                 token_to_origin_mapping,\n",
    "                 token_max_context,\n",
    "                 input_ids,\n",
    "                 input_mask,\n",
    "                 segments,\n",
    "                 start_position=None,\n",
    "                 end_position=None,\n",
    "                 class_index=None):            \n",
    "    self.unique_id = unique_id\n",
    "    self.example_index = example_index\n",
    "    self.document_span_index = document_span_index\n",
    "    self.tokens = tokens\n",
    "    self.token_to_origin_mapping = token_to_origin_mapping\n",
    "    self.token_max_context = token_max_context\n",
    "    self.input_ids = input_ids\n",
    "    self.input_mask = input_mask\n",
    "    self.segments = segments\n",
    "    self.start_position = start_position\n",
    "    self.end_position = end_position\n",
    "    self.class_index = class_index\n",
    "\n",
    "\n",
    "# Function to read COQA datasets and process the data\n",
    "def get_data_from_coqa(isTrain, input_file, history_len=2, add_QA_tag=False):\n",
    "\n",
    "# Check if the character is a white space\n",
    "  def check_whitespace(char):\n",
    "        if char == \" \" or char == \"\\t\" or char == \"\\r\" or char == \"\\n\" or ord(char) == 0x202F:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "# Token conversion\n",
    "  def tokenize_string(str):\n",
    "        if (str.lower() == '-lrb-'):\n",
    "            str = '('\n",
    "        elif (str.lower() == '-rrb-'):\n",
    "            str = ')'\n",
    "        elif (str.lower() == '-lsb-'):\n",
    "            str = '['\n",
    "        elif (str.lower() == '-rsb-'):\n",
    "            str = ']'\n",
    "        elif (str.lower() == '-lcb-'):\n",
    "            str = '{'\n",
    "        elif (str.lower() == '-rcb-'):\n",
    "            str = '}'\n",
    "        return str\n",
    "\n",
    "  def space_extension(matchobject):\n",
    "    return ' ' + matchobject.group(0) + ' '  \n",
    "\n",
    " # Preprocessing \n",
    "  def pre_processing(word):\n",
    "    word = re.sub(\n",
    "        u'-|\\u2010|\\u2011|\\u2012|\\u2013|\\u2014|\\u2015|%|\\[|\\]|:|\\(|\\)|/|\\t',\n",
    "        space_extension, word)\n",
    "    word = word.strip(' \\n')\n",
    "    word = re.sub('\\s+', ' ', word)\n",
    "    return word\n",
    "\n",
    "  # Process text to return output as words with their indexes in the sentences \n",
    "  def processing(main_text):\n",
    "          processed_text = {'word': [], 'offsets': [], 'sentences': []}\n",
    " \n",
    "          for token in main_text:\n",
    "              processed_text['word'].append(tokenize_string(token.text))\n",
    "              processed_text['offsets'].append((token.idx, token.idx + len(token.text)))\n",
    "          #print(\"=======offset is\", (token.idx, token.idx + len(token.text)), \"=====word is\", tokenize_string(token.text))\n",
    "\n",
    "          word_index = 0\n",
    "          for sentence in main_text.sents:\n",
    "              processed_text['sentences'].append((word_index, word_index + len(sentence)))\n",
    "              word_index += len(sentence)\n",
    "          #print(\"=======sentence is\",  word_index, word_index + len(sentence) )\n",
    "\n",
    "          assert word_index == len(processed_text['word'])\n",
    "           \n",
    "          return processed_text\n",
    "\n",
    "  # Get the context offsets\n",
    "  def context_offsets(words, raw_text):\n",
    "    #print(words, raw_text)\n",
    "    raw_text_context_offsets = []\n",
    "    r = 0\n",
    "    for token in words:\n",
    "        while r < len(raw_text) and re.match('\\s', raw_text[r]):\n",
    "            r += 1\n",
    "        if raw_text[r:r + len(token)] != token:\n",
    "            print('Error', token, 'Raw Text:', raw_text)\n",
    "\n",
    "        raw_text_context_offsets.append((r, r + len(token)))\n",
    "        r += len(token)\n",
    " \n",
    "    return raw_text_context_offsets\n",
    "\n",
    "  # Function to find span with start and end index provided\n",
    "  def define_span_indices(offsets, start_pos, end_pos):\n",
    "     \n",
    "    span_start_index = -1\n",
    "    span_end_index = -1\n",
    "    #print(\"offset is\", offsets)\n",
    "    for i, offset in enumerate(offsets):\n",
    "        if (span_start_index < 0) or (start_pos >= offset[0]):\n",
    "            span_start_index = i\n",
    "        if (span_end_index < 0) and (end_pos <= offset[1]):\n",
    "            span_end_index = i\n",
    "    #print(\"span\",span_start_index, span_end_index)\n",
    "    return (span_start_index, span_end_index)\n",
    "\n",
    "# Removing punctuations, lowering texts and removing extra white spaces\n",
    "  def pre_process_answer(s):\n",
    "    \n",
    "    \n",
    "    # Remove articles from the text\n",
    "    def remove_articles(text):\n",
    "        reg_expression = re.compile(r'\\b(a|an|the)\\b', re.UNICODE)\n",
    "        return re.sub(reg_expression, ' ', text)\n",
    "    \n",
    "    # Remove white spaces from the text\n",
    "    def adjust_white_space(text):\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    # Remove punctuations from the text\n",
    "    def remove_punctuations(text):\n",
    "        rem = set(string.punctuation)\n",
    "        return ''.join(c for c in text if c not in rem)\n",
    "\n",
    "    # Lower the text characters\n",
    "    def lowering_text(text):\n",
    "        return text.lower()\n",
    "     \n",
    "\n",
    "    return adjust_white_space(remove_articles(remove_punctuations(lowering_text(s))))  \n",
    "\n",
    "# Find the span providing the context and offsets  \n",
    "  def span_with_ground_truth(context, offsets, ground_truth):\n",
    "\n",
    "    best_F1 = 0.0\n",
    "    best_span = (len(offsets) - 1, len(offsets) - 1)\n",
    "    ground_truth_temp = pre_process_answer(pre_processing(ground_truth)).split()\n",
    "\n",
    "    ls = [\n",
    "        i for i in range(len(offsets))\n",
    "        if context[offsets[i][0]:offsets[i][1]].lower() in ground_truth\n",
    "    ]\n",
    "\n",
    "    for i in range(len(ls)):\n",
    "        for j in range(i, len(ls)):\n",
    "            prediction = pre_process_answer(\n",
    "                pre_processing(\n",
    "                    context[offsets[ls[i]][0]:offsets[ls[j]][1]])).split()\n",
    "            common = Counter(prediction) & Counter(ground_truth_temp)\n",
    "            num_same = sum(common.values())\n",
    "\n",
    "            #print(num_same, \"common span\")\n",
    "            if num_same > 0:\n",
    "                precision = 1.0 * num_same / len(prediction)\n",
    "                recall = 1.0 * num_same / len(ground_truth_temp)\n",
    "                F1 = (2 * precision * recall) / (precision + recall)\n",
    "                if F1 > best_F1:\n",
    "                    best_F1 = F1\n",
    "                    best_span = (ls[i], ls[j])\n",
    "     \n",
    "    #print(best_span, \"best span\")\n",
    "\n",
    "    return best_span\n",
    "\n",
    "  nlp = spacy.load(\"en_core_web_sm\")\n",
    "  # Read training file\n",
    "  with open(input_file, \"r\", encoding='utf-8') as reader:\n",
    "        input_text_file = json.load(reader)[\"data\"]\n",
    "  print(len(input_text_file))\n",
    "  samples = []\n",
    "  input_text_file = input_text_file \n",
    "############################################################\n",
    "############################################################\n",
    "############################################################\n",
    "  if isTrain:\n",
    "    data_len =  len(input_text_file) # Restricted training data due to hardware limitations\n",
    "  else:\n",
    "    data_len =  len(input_text_file) # Entire Development Data is loaded \n",
    "  number_yes = 0\n",
    "  number_no = 0\n",
    "  number_unknown = 0\n",
    "  number_span = 0 \n",
    "  number_nonespan = 0 \n",
    "  # Fetch and store story, questions and answers after processing the text\n",
    "  for data_index in tqdm(range(data_len), desc='Generating examples'):\n",
    "    input_data = input_text_file[data_index]\n",
    "    context_string = input_data['story']\n",
    "    input_data_object = {\n",
    "        'context': context_string,\n",
    "        'source': input_data['source'],\n",
    "        'id': input_data['id'],\n",
    "        'filename': input_data['filename']\n",
    "    }\n",
    "     \n",
    "     \n",
    "    nlp_context = nlp(pre_processing(context_string)) \n",
    "     \n",
    "    input_data_object['annotated_context'] = processing(nlp_context)\n",
    "     \n",
    "    input_data_object['raw_context_offsets'] = context_offsets(\n",
    "          input_data_object['annotated_context']['word'], context_string)\n",
    "      \n",
    "    assert len(input_data['questions']) == len(input_data['answers'])\n",
    "    additional_answers = {} \n",
    "    if 'additional_answers' in input_data:\n",
    "      for k, answer in input_data['additional_answers'].items():\n",
    "        if len(answer) == len(input_data['answers']):\n",
    "          for example in answer:\n",
    "            index = example['turn_id']\n",
    "            if index not in additional_answers:\n",
    "              additional_answers[index] = []\n",
    "            additional_answers[index].append(example['input_text'])\n",
    "    \n",
    "    \n",
    "    for i in range(len(input_data['questions'])):\n",
    "      question, answer = input_data['questions'][i], input_data['answers'][i]\n",
    "      assert question['turn_id'] == answer['turn_id']\n",
    "       \n",
    "\n",
    "      index = question['turn_id']\n",
    "      _qas = {\n",
    "          'turn_id': index,\n",
    "          'question': question['input_text'],\n",
    "          'answer': answer['input_text']\n",
    "      }\n",
    "       \n",
    "      if index in additional_answers:\n",
    "        _qas['additional_answers'] = additional_answers[index]\n",
    "      _qas['raw_answer'] = answer['input_text']\n",
    "      \n",
    "      if _qas['raw_answer'].lower() in ['yes', 'yes.']:\n",
    "        _qas['raw_answer'] = 'yes'\n",
    "        number_yes =  number_yes+1\n",
    "      if _qas['raw_answer'].lower() in ['no', 'no.']:\n",
    "        \n",
    "        _qas['raw_answer'] = 'no'\n",
    "        number_no =number_no +1\n",
    "      if _qas['raw_answer'].lower() in ['unknown', 'unknown.']:\n",
    "        \n",
    "        _qas['raw_answer'] = 'unknown'  \n",
    "        number_unknown = number_unknown+1     \n",
    "      _qas['answer_span_start'] = answer['span_start']\n",
    "      _qas['answer_span_end'] = answer['span_end']\n",
    "       \n",
    "      start = answer['span_start']\n",
    "      end = answer['span_end']\n",
    "      chosen_text = input_data_object['context'][start:end].lower()\n",
    "      \n",
    "      while len(chosen_text) > 0 and check_whitespace(chosen_text[0]):\n",
    "        chosen_text = chosen_text[1:]\n",
    "        start += 1\n",
    "       \n",
    "      while len(chosen_text) > 0 and check_whitespace(chosen_text[-1]):\n",
    "        chosen_text = chosen_text[:-1]\n",
    "        end -= 1\n",
    "        \n",
    "      \n",
    "      input_text = _qas['answer'].strip().lower() \n",
    "      if input_text in chosen_text:\n",
    "        #print(input_text, \"=====\",chosen_text)\n",
    "        input_text = _qas['answer'].strip().lower() \n",
    "        number_span = number_span+1\n",
    "        p = chosen_text.find(input_text)\n",
    "         \n",
    "        #get the answer span from raw text when the answer can be extracted exactly\n",
    "        _qas['answer_span'] = define_span_indices(input_data_object['raw_context_offsets'],\n",
    "                                                start + p,\n",
    "                                                start + p + len(input_text))\n",
    "        #print(input_data_object['raw_context_offsets'])\n",
    "        #_qas['raw_long_question'] = question['input_text']\n",
    "###########################here remove the else to every question)\n",
    "      else:\n",
    "#########################################################################\n",
    "        input_text = answer['span_text'].strip().lower()\n",
    "        number_nonespan =number_nonespan +1 \n",
    "#########################################################################\n",
    "         \n",
    "        _qas['answer_span'] = span_with_ground_truth(\n",
    "                      input_data_object['context'], input_data_object['raw_context_offsets'],\n",
    "                      input_text)\n",
    "        #look back maximumly 2 questions \n",
    "     \n",
    "      long_question = ''\n",
    "      for j in range(i - history_len, i + 1):\n",
    "           if j < 0:\n",
    "             continue\n",
    "           long_question += (' <Q> ' if add_QA_tag else\n",
    "                                  ' ') + input_data['questions'][j]['input_text']\n",
    "           \n",
    "           if j < i:\n",
    "                    long_question += (' <A> ' if add_QA_tag else\n",
    "                                      ' ') + input_data['answers'][j]['input_text']\n",
    "           \n",
    "           long_question = long_question.strip()\n",
    "############################################################################################################################################################################           \n",
    "           #add question history \n",
    "           #print(long_question)\n",
    "           _qas['raw_long_question'] = long_question\n",
    "############################################################################################################################################################################\n",
    "           \n",
    "           _qas['annotated_long_question'] = processing(\n",
    "                nlp(pre_processing(long_question)))\n",
    "           #offset is the letter position of the word\n",
    "   # Store questions along with their answers     \n",
    "      sample = QA(\n",
    "                question_answer_id =input_data_object['id'] + ' ' + str(_qas['turn_id']),\n",
    "                question_text =_qas['raw_long_question'],\n",
    "                document_tokens =input_data_object['annotated_context']['word'],\n",
    "                original_answer_text =_qas['raw_answer'],\n",
    "                answer_start_position =_qas['answer_span'][0],\n",
    "                answer_end_position =_qas['answer_span'][1],\n",
    "                additional_answers=_qas['additional_answers'] if 'additional_answers' in _qas else None,\n",
    "                )\n",
    "      \n",
    "      samples.append(sample)\n",
    "  print(\"yes\",number_yes,\"no\", number_no, \"unknown\",number_unknown,\"span\",number_span, number_nonespan)\n",
    "      #print(sample.question_answer_id, sample.question_text, sample.original_answer_text,  sample.answer_start_position, sample.answer_end_position, sample.additional_answers )#, sample.question_text,sample.original_answer_text)\n",
    "  return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "foKc7xe3lkFu"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Function to improve the answer by modifying the start and end indexes to appropriate answer\n",
    "def modify_answer_span(document_tokens, input_start_index, input_end_index, tokenizer, original_answer_text):\n",
    "  #print(document_tokens, input_start_index, input_end_index, tokenizer, original_answer_text)\n",
    "  token_answer_text = \" \".join(bert_tokenizer.tokenize(original_answer_text))\n",
    "  \n",
    "  # loop to modify the indexes for appropriate answer\n",
    "  for new_start_index in range(input_start_index, input_end_index + 1):\n",
    "    for new_end_index in range(input_end_index, new_start_index - 1, -1):\n",
    "      text_span = \" \".join(document_tokens[new_start_index:(new_end_index + 1)])\n",
    "      if text_span == token_answer_text:\n",
    "        return (new_start_index, new_end_index)\n",
    "  \n",
    " \n",
    "  return (input_start_index, input_end_index)\n",
    "\n",
    "# Function to select the span with the maximum context for the token\n",
    "def check_max_context(document_spans, current_span_index, current_position):\n",
    "   \n",
    "  best_score = None\n",
    "  best_span_index = None\n",
    "  for (span_index, document_span) in enumerate(document_spans):\n",
    "    end = document_span.start + document_span.length - 1\n",
    "    if current_position < document_span.start:\n",
    "            continue\n",
    "    if current_position > end:\n",
    "            continue\n",
    "    left_context = current_position - document_span.start\n",
    "    right_context = end - current_position\n",
    "    score = min(left_context, right_context) + 0.01 * document_span.length # Score calculation for the current start and end index\n",
    "    if best_score is None or score > best_score:  # selection of indexes based on better score\n",
    "      best_score = score\n",
    "      best_span_index = span_index\n",
    "  \n",
    "  return current_span_index == best_span_index\n",
    "# Function to convert CoQA Data to features\n",
    "def converting_examples_into_features(examples, tokenizer, maximum_sequence_length, document_stride, maximum_query_length):\n",
    "   \n",
    "  unique_id = 1000000000\n",
    "  features = []\n",
    "  for (example_index, example) in enumerate(tqdm(examples, desc=\"Generating features for CoQA...\")):\n",
    "    query_token = tokenizer.tokenize(example.question_text)\n",
    "     \n",
    "    class_index = 3\n",
    "\n",
    "    # Check for the type of answer whether it is yes/no type otherwise set to unknown\n",
    "    if example.original_answer_text == 'yes':\n",
    "      class_index = 0  \n",
    "    elif example.original_answer_text == 'no':\n",
    "      class_index = 1 \n",
    "    elif example.original_answer_text == 'unknown':\n",
    "      class_index = 2  \n",
    "  \n",
    "    # Check for the length of the query and select the query until uth maximum query length set\n",
    "    if len(query_token) > maximum_query_length:\n",
    "      query_token = query_token[0:maximum_query_length]\n",
    "    \n",
    "    token_to_original_index = []\n",
    "    original_to_token_index = []\n",
    "    all_document_tokens = []\n",
    "\n",
    "    for (i, token) in enumerate(example.document_tokens):\n",
    "      original_to_token_index.append(len(all_document_tokens))\n",
    "      sub_tokens = tokenizer.tokenize(token)\n",
    "      for sub_token in sub_tokens:\n",
    "        token_to_original_index.append(i)\n",
    "        all_document_tokens.append(sub_token)\n",
    "     \n",
    "    token_start_position = None\n",
    "    token_end_position = None\n",
    "    if class_index < 3:\n",
    "      token_start_position, token_end_position = 0,0\n",
    "    else:\n",
    "      token_start_position = original_to_token_index[example.answer_start_position]\n",
    "      if example.answer_end_position < len(example.document_tokens) - 1:\n",
    "        token_end_position = original_to_token_index[example.answer_end_position + 1] - 1\n",
    "      else:\n",
    "        token_end_position = len(all_document_tokens) - 1\n",
    "      (token_start_position, token_end_position) = modify_answer_span(\n",
    "                all_document_tokens, token_start_position, token_end_position, tokenizer,\n",
    "                example.original_answer_text)\n",
    "    \n",
    "     \n",
    "    maximum_tokens_for_document = maximum_sequence_length - len(query_token) - 3\n",
    "     \n",
    "    _DocSpan = collections.namedtuple(  \n",
    "            \"DocSpan\", [\"start\", \"length\"])\n",
    " ######################################################################need to figure out this##################################    \n",
    "    document_spans = []\n",
    "    start_offset = 0\n",
    "    while start_offset < len(all_document_tokens):\n",
    "      length = len(all_document_tokens) - start_offset\n",
    "      if length > maximum_tokens_for_document:\n",
    "        length = maximum_tokens_for_document\n",
    "      document_spans.append(_DocSpan(start=start_offset, length=length))\n",
    "      if start_offset + length == len(all_document_tokens):\n",
    "        break\n",
    "      start_offset += min(length, document_stride)\n",
    "      \n",
    "    \n",
    "    # loop to add the seperator tokens in the input sequence\n",
    "    for (document_span_index, document_span) in enumerate(document_spans):   \n",
    "      slice_class_index = class_index\n",
    "      tokens = []\n",
    "      token_to_origin_mapping = {}\n",
    "      token_max_context = {}\n",
    "      segment_ids = []\n",
    "      tokens.append(\"[CLS]\")\n",
    "      segment_ids.append(0)\n",
    "      \n",
    "      for token in query_token:\n",
    "          \n",
    "          tokens.append(token)\n",
    "          segment_ids.append(0)\n",
    "      #this is for single query examples\n",
    "      tokens.append(\"[SEP]\")\n",
    "\n",
    "      segment_ids.append(0)\n",
    "       \n",
    "      for i in range(document_span.length):\n",
    "        split_token_index = document_span.start + i\n",
    "        token_to_origin_mapping[len(\n",
    "                    tokens)] = token_to_original_index[split_token_index]\n",
    "        is_max_context = check_max_context(document_spans,\n",
    "                                                       document_span_index,\n",
    "                                                       split_token_index)\n",
    "        token_max_context[len(tokens)] = is_max_context\n",
    "        tokens.append(all_document_tokens[split_token_index])\n",
    "        segment_ids.append(1)\n",
    "      #this merges the entire paragraph\n",
    "      tokens.append(\"[SEP]\")\n",
    "      segment_ids.append(1)\n",
    "       \n",
    "      input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "      \n",
    "      input_mask = [1] * len(input_ids)\n",
    "       \n",
    "\n",
    "      while len(input_ids) < maximum_sequence_length:\n",
    "        input_ids.append(0)\n",
    "        input_mask.append(0)\n",
    "        segment_ids.append(0)\n",
    "      #print(input_ids, input_mask,segment_ids )\n",
    "      assert len(input_ids) == maximum_sequence_length\n",
    "      assert len(input_mask) == maximum_sequence_length\n",
    "      assert len(segment_ids) == maximum_sequence_length\n",
    "\n",
    "      # Start and end position calculations\n",
    "      start_position = None\n",
    "      end_position = None\n",
    "      if class_index >= 3:\n",
    "        document_start = document_span.start\n",
    "        document_end = document_span.start + document_span.length - 1\n",
    "         \n",
    "        out_of_span = False\n",
    "  \n",
    "      \n",
    "        if not (token_start_position >= document_start\n",
    "                        and token_end_position <= document_end):\n",
    "           \n",
    "          out_of_span = True\n",
    "        if out_of_span:\n",
    "          start_position = 0\n",
    "          end_position = 0\n",
    "          slice_class_index = 2\n",
    "        else: #why use document offest\n",
    "          document_offset = len(query_token) + 2\n",
    "          start_position = token_start_position - document_start + document_offset\n",
    "          end_position = token_end_position - document_start + document_offset\n",
    "          \n",
    "      else:\n",
    "        start_position = 0\n",
    "        end_position = 0\n",
    "      \n",
    "      # add the current feature calculated to the features list   \n",
    "      features.append(                            \n",
    "          DataFeatures(unique_id=unique_id, #record id\n",
    "                        example_index=example_index, #question id\n",
    "                        document_span_index=document_span_index,#document id\n",
    "                        tokens=tokens,#tokens for the entire questions and context texts\n",
    "                        token_to_origin_mapping=token_to_origin_mapping, #mapping the sequence with original index\n",
    "                        token_max_context=token_max_context,#whether exceed the maximum length\n",
    "                        input_ids=input_ids,#the tokenizer word embedding\n",
    "                        input_mask=input_mask,#the mask for all tokens, with word is 1, the rest padded with 0, 450\n",
    "                        segments=segment_ids,#segments of historical question and answer 0, context 1 \n",
    "                        start_position=start_position,#start position of the answer span\n",
    "                        end_position=end_position,#end position of the answer span\n",
    "                        class_index=slice_class_index)) #whether yes, no, unknow, or usual\n",
    "     \n",
    "      unique_id += 1\n",
    "       \n",
    "  return features  # Return all the features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VlAAi3asm-Hc"
   },
   "source": [
    "#Load Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4026,
     "status": "ok",
     "timestamp": 1619447532242,
     "user": {
      "displayName": "Ning Zhang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhRCnmT8mmairzuShAibDId7kDGmrCXjp8yIjrCOw=s64",
      "userId": "12536691088853738123"
     },
     "user_tz": -480
    },
    "id": "n1eckNTFlkLF",
    "outputId": "a7a090c0-e432-477f-a10c-386fc8da6118"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Generating examples:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating examples: 100%|██████████| 1/1 [00:01<00:00,  1.84s/it]\n",
      "Generating features for CoQA...:  42%|████▏     | 5/12 [00:00<00:00, 44.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes 0 no 3 unknown 0 span 4 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating features for CoQA...: 100%|██████████| 12/12 [00:00<00:00, 43.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.DataFeatures object at 0x7f6cb412ab90>\n",
      "tensor([[   2,   98, 1665,  ...,    0,    0,    0],\n",
      "        [   2,   98, 1665,  ...,    0,    0,    0],\n",
      "        [   2,   98, 1665,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [   2,   98,  144,  ...,    7,   13,    3],\n",
      "        [   2,   98,  144,  ...,    0,    0,    0],\n",
      "        [   2,  113,  144,  ...,    0,    0,    0]])\n",
      "tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])\n",
      "<torch.utils.data.dataset.TensorDataset object at 0x7f6d000bb290>\n",
      "<torch.utils.data.sampler.SequentialSampler object at 0x7f6d000bec50>\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x7f6d000bb090>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Read COQA Dev file, File path needs to be provided where COQA dev file is stored\n",
    "testing_samples = get_data_from_coqa(False, input_file=\"data/coqa-dev-v1.0.json\",\n",
    "                                                history_len= 2,\n",
    "                                                add_QA_tag= False)\n",
    "# Converting the development examples to features\n",
    "testing_features = converting_examples_into_features(\n",
    "                examples=testing_samples,\n",
    "                tokenizer=bert_tokenizer,\n",
    "                maximum_sequence_length=450,\n",
    "                document_stride=128,\n",
    "                maximum_query_length=75,\n",
    "            )\n",
    "print(testing_features[0])\n",
    "#Tensor construction for input ids\n",
    "dataset_input_ids = torch.tensor([f.input_ids for f in testing_features], dtype=torch.long)\n",
    "print(dataset_input_ids)\n",
    "\n",
    "#Tensor construction for input masks\n",
    "dataset_input_masks = torch.tensor([f.input_mask for f in testing_features], dtype=torch.long)\n",
    "print(dataset_input_masks)\n",
    "\n",
    "#Tensor construction for segment ids\n",
    "dataset_segment_ids = torch.tensor([f.segments for f in testing_features], dtype=torch.long)\n",
    "print(dataset_segment_ids)\n",
    "\n",
    "dataset_example_index  = torch.arange(dataset_input_ids.size(0), dtype=torch.long)\n",
    "\n",
    "# Wrapping tensors in a tensor dataset\n",
    "testing_data = TensorDataset(dataset_input_ids, dataset_input_masks, dataset_segment_ids, dataset_example_index)\n",
    " \n",
    "torch.save(testing_data, 'outputs/test_tensor_albert_base_16.pt')\n",
    "testing_data_save = torch.load('outputs/test_tensor_albert_base_16.pt')\n",
    "print(testing_data_save)\n",
    "# Sampling the elements in the same order they are(sequentially)\n",
    "testing_data_sampler = SequentialSampler(testing_data_save)\n",
    "print(testing_data_sampler)\n",
    "\n",
    "# Creating python iterable over tensor dataset\n",
    "testing_dataloader = DataLoader(testing_data_save, sampler=testing_data_sampler, batch_size=8)\n",
    "print(testing_dataloader) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 907,
     "referenced_widgets": [
      "02befd9e4e1b4b7bab5b079cb5e8d1dd",
      "90a72d1cf56f45c9837a28e770a1df9f",
      "6a09d69f54ce419f85ee6af55f128b16",
      "5dd14753f10d48a6945226a34be98921",
      "f4cd828157aa434796d5694690a8b09a",
      "ea287411a7ee40fd89ef7c8041ff5347",
      "a57460bf01824fefae53fdc48fd9768a",
      "fa4f60dce7334709881750b5aff07b8e",
      "05b75e87009447eb82cc9fc288fd5749",
      "ee318d357b82442085188ffdb2460e86",
      "48c66d8ae5454384846f8b628b26b264",
      "7838971635474042b4948fb3902f18df",
      "82f857fc8e0943fb9d180a4a7a8c80d3",
      "842c450a8bce46b8909cabf6d5907337",
      "135e174a10da4e26a1e860f855db0a85",
      "813ecb446fa74295b3fad5eaf8d116ac"
     ]
    },
    "executionInfo": {
     "elapsed": 13177,
     "status": "ok",
     "timestamp": 1619447240991,
     "user": {
      "displayName": "Ning Zhang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhRCnmT8mmairzuShAibDId7kDGmrCXjp8yIjrCOw=s64",
      "userId": "12536691088853738123"
     },
     "user_tz": -480
    },
    "id": "ubsX67_VOXWN",
    "outputId": "598ebc97-0ac5-409f-e345-146214650a51"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02befd9e4e1b4b7bab5b079cb5e8d1dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=684.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05b75e87009447eb82cc9fc288fd5749",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=47376696.0, style=ProgressStyle(descrip…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at albert-base-v2 were not used when initializing CoQAwithAlbert: ['predictions.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.decoder.bias']\n",
      "- This IS expected if you are initializing CoQAwithAlbert from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CoQAwithAlbert from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CoQAwithAlbert were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['linear.weight', 'linear.bias', 'qa_outputs.weight', 'qa_outputs.bias', 'class_outputs.weight', 'class_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CoQAwithAlbert(\n",
       "  (albert): AlbertModel(\n",
       "    (embeddings): AlbertEmbeddings(\n",
       "      (word_embeddings): Embedding(30000, 128, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 128)\n",
       "      (token_type_embeddings): Embedding(2, 128)\n",
       "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0, inplace=False)\n",
       "    )\n",
       "    (encoder): AlbertTransformer(\n",
       "      (embedding_hidden_mapping_in): Linear(in_features=128, out_features=768, bias=True)\n",
       "      (albert_layer_groups): ModuleList(\n",
       "        (0): AlbertLayerGroup(\n",
       "          (albert_layers): ModuleList(\n",
       "            (0): AlbertLayer(\n",
       "              (full_layer_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (attention): AlbertAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (attention_dropout): Dropout(p=0, inplace=False)\n",
       "                (output_dropout): Dropout(p=0, inplace=False)\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              )\n",
       "              (ffn): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (ffn_output): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (pooler_activation): Tanh()\n",
       "  )\n",
       "  (linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (class_outputs): Linear(in_features=768, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import AlbertModel,AlbertConfig    \n",
    "from transformers import AlbertModel, AdamW, AlbertConfig\n",
    "AlbertPreTrainedModel = transformers.AlbertPreTrainedModel \n",
    "# Model creation using Bert Pretrained Model as a base\n",
    "class CoQAwithAlbert(AlbertPreTrainedModel):\n",
    "\n",
    "  # Configurations passed to BERT model\n",
    "  def __init__(\n",
    "            self,\n",
    "            config,\n",
    "            output_attentions=False,\n",
    "            keep_multihead_output=False,\n",
    "            class_alpha=1.0,\n",
    "            mask_p=0.0,\n",
    "    ):\n",
    "    super(CoQAwithAlbert, self).__init__(config)\n",
    "    self.class_alpha = class_alpha\n",
    "    self.mask_p = mask_p\n",
    "    self.albert = AlbertModel(\n",
    "    config,\n",
    "    )\n",
    "    self.linear = nn.Linear(config.hidden_size,config.hidden_size)\n",
    "    self.relu  = nn.ReLU()\n",
    "  \n",
    "    self.qa_outputs = nn.Linear(config.hidden_size, 2)\n",
    "    self.output_attentions = False\n",
    "    self.class_outputs = nn.Linear(config.hidden_size, 4)\n",
    "    model_config = AlbertConfig.from_pretrained('albert-base-v2', output_hidden_states=True)\n",
    "    self.albert = AlbertModel.from_pretrained('albert-base-v2', config=model_config)\n",
    "    #self.apply(self.init_bert_weights)\n",
    "    #self.apply(self.init_bert_weights)\n",
    "  \n",
    "  # Forward pass for the BERT model\n",
    "  def forward(\n",
    "            self,\n",
    "            input_ids,  # Input seq indices \n",
    "            token_type_ids=None,\n",
    "            attention_mask=None, # Masking to avoid attention\n",
    "            start_positions=None, # Starting position of the span\n",
    "            end_positions=None,  # End position of the span\n",
    "            class_index = None,\n",
    "    ):\n",
    "    outputs = self.albert(\n",
    "            input_ids,\n",
    "            token_type_ids=token_type_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            #head_mask=head_mask,\n",
    "        )\n",
    "     \n",
    "    # outputs consists of the elements based on the configurations provided to BERT\n",
    "    sequence_output= outputs[0]\n",
    "    class_outputs = outputs[1] \n",
    "    \n",
    "    span_logits_0 = self.linear(sequence_output)\n",
    "    span_logits_0 =self.relu(span_logits_0)  \n",
    "    span_logits_0 = self.linear(span_logits_0)\n",
    "    span_logits_0 =self.relu(span_logits_0)  \n",
    "    span_logits = self.qa_outputs(span_logits_0)\n",
    "    \n",
    "    class_logits_0 = self.linear(class_outputs)\n",
    "    class_logits_0 =self.relu(class_logits_0 )  \n",
    "    class_logits_0 = self.linear(class_logits_0 )\n",
    "    class_logits_0 =self.relu(class_logits_0 )  \n",
    "\n",
    "    class_logits = self.class_outputs(class_logits_0)\n",
    "    #print(span_logits_0.shape, class_logits_0.shape)\n",
    "    start_logits, end_logits = span_logits.split(1, dim=-1)\n",
    "    start_logits = start_logits.squeeze(-1)\n",
    "    end_logits = end_logits.squeeze(-1)\n",
    "    #print(class_logits.shape)\n",
    "\n",
    "    # Span extraction based on start positions and end positions\n",
    "    if start_positions is not None and end_positions is not None:\n",
    "      if len(start_positions.size()) > 1:\n",
    "        start_positions = start_positions.squeeze(-1)\n",
    "      if len(end_positions.size()) > 1:\n",
    "        end_positions = end_positions.squeeze(-1)\n",
    "      ignored_index = start_logits.size(1)\n",
    "       \n",
    "      start_positions.clamp_(0, ignored_index)\n",
    "      end_positions.clamp_(0, ignored_index)\n",
    "      #print(start_logits.shape, start_positions)\n",
    "      #print(end_logits, end_positions)\n",
    "      span_loss_factor = CrossEntropyLoss(ignore_index=ignored_index)\n",
    "      class_loss_factor = CrossEntropyLoss()\n",
    "      #here need to have the argmax for start_logits\n",
    "      #this loss is still based on the text span, there is no text generation component, need to see more how others do this piece, the model is not quite right\n",
    "      #data preprocessing done, but there is no ground truth answer therefore can't train, refer to other models, or otherwise this is still using squad method to do coqa\n",
    "      start_loss = span_loss_factor(start_logits, start_positions)\n",
    "      end_loss = span_loss_factor(end_logits, end_positions)\n",
    "      class_loss = class_loss_factor(class_logits, class_index)\n",
    "      #print(class_logits, class_index)\n",
    "      #########################################################\n",
    "      #########################################################\n",
    "      #index = class_logits.data.cpu().numpy().argmax()\n",
    "     #index_tensor = torch.argmax(class_logits)\n",
    "      loss=0\n",
    "      #add_loss =nn.L1Loss()\n",
    "     # for i, index in enumerate(class_index):\n",
    "     #   if index == torch.tensor(1).to(device):\n",
    "          #print(class_logits[i].argmax(),index, \"###############\")\n",
    "          #loss =  add_loss(class_logits[i].argmax()/index + 0.001,torch.tensor(1.0).to(device)+0.001).detach()\n",
    "     #     loss = abs((class_logits[i].argmax()/index + 0.001)-(torch.tensor(1.0).to(device)+0.001))\n",
    "          \n",
    "     #   else:\n",
    "     #     loss = 0\n",
    "       \n",
    "   \n",
    "       \n",
    "      ########################################################################################################\n",
    "      total_loss = (start_loss + end_loss) / 2 + self.class_alpha * class_loss + 0.05* loss \n",
    "      \n",
    "      return total_loss\n",
    "    return start_logits, end_logits, class_logits\n",
    "model1 = CoQAwithAlbert.from_pretrained('albert-base-v2', output_hidden_states=True)\n",
    " # Set model in training mode\n",
    "device=\"cuda\"\n",
    "model1.to(device)\n",
    "model1.train()\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "42UTrqnmoGyp"
   },
   "source": [
    "#Another model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4146,
     "status": "ok",
     "timestamp": 1619448057502,
     "user": {
      "displayName": "Ning Zhang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhRCnmT8mmairzuShAibDId7kDGmrCXjp8yIjrCOw=s64",
      "userId": "12536691088853738123"
     },
     "user_tz": -480
    },
    "id": "-V8ofeBjn-Qe",
    "outputId": "58e04527-3a4a-49f9-c7a6-298c2aa4365c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at albert-base-v2 were not used when initializing CoQAwithAlbert: ['predictions.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.decoder.bias']\n",
      "- This IS expected if you are initializing CoQAwithAlbert from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CoQAwithAlbert from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CoQAwithAlbert were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['linear.weight', 'linear.bias', 'qa_outputs.weight', 'qa_outputs.bias', 'class_outputs.weight', 'class_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CoQAwithAlbert(\n",
       "  (albert): AlbertModel(\n",
       "    (embeddings): AlbertEmbeddings(\n",
       "      (word_embeddings): Embedding(30000, 128, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 128)\n",
       "      (token_type_embeddings): Embedding(2, 128)\n",
       "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0, inplace=False)\n",
       "    )\n",
       "    (encoder): AlbertTransformer(\n",
       "      (embedding_hidden_mapping_in): Linear(in_features=128, out_features=768, bias=True)\n",
       "      (albert_layer_groups): ModuleList(\n",
       "        (0): AlbertLayerGroup(\n",
       "          (albert_layers): ModuleList(\n",
       "            (0): AlbertLayer(\n",
       "              (full_layer_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (attention): AlbertAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (attention_dropout): Dropout(p=0, inplace=False)\n",
       "                (output_dropout): Dropout(p=0, inplace=False)\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              )\n",
       "              (ffn): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (ffn_output): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (pooler_activation): Tanh()\n",
       "  )\n",
       "  (linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (class_outputs): Linear(in_features=768, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import AlbertModel,AlbertConfig    \n",
    "from transformers import AlbertModel, AdamW, AlbertConfig\n",
    "AlbertPreTrainedModel = transformers.AlbertPreTrainedModel \n",
    "# Model creation using Bert Pretrained Model as a base\n",
    "class CoQAwithAlbert_l(AlbertPreTrainedModel):\n",
    "\n",
    "  # Configurations passed to BERT model\n",
    "  def __init__(\n",
    "            self,\n",
    "            config,\n",
    "            output_attentions=False,\n",
    "            keep_multihead_output=False,\n",
    "            class_alpha=1.0,\n",
    "            mask_p=0.0,\n",
    "    ):\n",
    "    super(CoQAwithAlbert_l, self).__init__(config)\n",
    "    self.class_alpha = class_alpha\n",
    "    self.mask_p = mask_p\n",
    "    self.albert = AlbertModel(\n",
    "    config,\n",
    "    )\n",
    "    self.linear = nn.Linear(config.hidden_size,config.hidden_size)\n",
    "    self.relu  = nn.ReLU()\n",
    "  \n",
    "    self.qa_outputs = nn.Linear(config.hidden_size, 2)\n",
    "    self.output_attentions = False\n",
    "    self.class_outputs = nn.Linear(config.hidden_size, 4)\n",
    "    model_config = AlbertConfig.from_pretrained('albert-base-v2', output_hidden_states=True)\n",
    "    self.albert = AlbertModel.from_pretrained('albert-base-v2', config=model_config)\n",
    "    #self.apply(self.init_bert_weights)\n",
    "    #self.apply(self.init_bert_weights)\n",
    "  \n",
    "  # Forward pass for the BERT model\n",
    "  def forward(\n",
    "            self,\n",
    "            input_ids,  # Input seq indices \n",
    "            token_type_ids=None,\n",
    "            attention_mask=None, # Masking to avoid attention\n",
    "            start_positions=None, # Starting position of the span\n",
    "            end_positions=None,  # End position of the span\n",
    "            class_index = None,\n",
    "    ):\n",
    "    outputs = self.albert(\n",
    "            input_ids,\n",
    "            token_type_ids=token_type_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            #head_mask=head_mask,\n",
    "        )\n",
    "     \n",
    "    # outputs consists of the elements based on the configurations provided to BERT\n",
    "    sequence_output= outputs[0]\n",
    "    class_outputs = outputs[1] \n",
    "    \n",
    "    span_logits_0 = self.linear(sequence_output)\n",
    "    span_logits_0 =self.relu(span_logits_0)  \n",
    "    span_logits_0 = self.linear(span_logits_0)\n",
    "    span_logits_0 =self.relu(span_logits_0)  \n",
    "    span_logits = self.qa_outputs(span_logits_0)\n",
    "    \n",
    "    class_logits_0 = self.linear(class_outputs)\n",
    "    class_logits_0 =self.relu(class_logits_0 )  \n",
    "    class_logits_0 = self.linear(class_logits_0 )\n",
    "    class_logits_0 =self.relu(class_logits_0 )  \n",
    "\n",
    "    class_logits = self.class_outputs(class_logits_0)\n",
    "    #print(span_logits_0.shape, class_logits_0.shape)\n",
    "    start_logits, end_logits = span_logits.split(1, dim=-1)\n",
    "    start_logits = start_logits.squeeze(-1)\n",
    "    end_logits = end_logits.squeeze(-1)\n",
    "    #print(class_logits.shape)\n",
    "\n",
    "    # Span extraction based on start positions and end positions\n",
    "    if start_positions is not None and end_positions is not None:\n",
    "      if len(start_positions.size()) > 1:\n",
    "        start_positions = start_positions.squeeze(-1)\n",
    "      if len(end_positions.size()) > 1:\n",
    "        end_positions = end_positions.squeeze(-1)\n",
    "      ignored_index = start_logits.size(1)\n",
    "       \n",
    "      start_positions.clamp_(0, ignored_index)\n",
    "      end_positions.clamp_(0, ignored_index)\n",
    "      #print(start_logits.shape, start_positions)\n",
    "      #print(end_logits, end_positions)\n",
    "      span_loss_factor = CrossEntropyLoss(ignore_index=ignored_index)\n",
    "      class_loss_factor = CrossEntropyLoss()\n",
    "      #here need to have the argmax for start_logits\n",
    "      #this loss is still based on the text span, there is no text generation component, need to see more how others do this piece, the model is not quite right\n",
    "      #data preprocessing done, but there is no ground truth answer therefore can't train, refer to other models, or otherwise this is still using squad method to do coqa\n",
    "      start_loss = span_loss_factor(start_logits, start_positions)\n",
    "      end_loss = span_loss_factor(end_logits, end_positions)\n",
    "      class_loss = class_loss_factor(class_logits, class_index)\n",
    "      #print(class_logits, class_index)\n",
    "      #########################################################\n",
    "      #########################################################\n",
    "      #index = class_logits.data.cpu().numpy().argmax()\n",
    "     #index_tensor = torch.argmax(class_logits)\n",
    "      loss=0\n",
    "      add_loss =nn.L1Loss()\n",
    "      for i, index in enumerate(class_index):\n",
    "        if index == torch.tensor(1).to(device):\n",
    "          #print(class_logits[i].argmax(),index, \"###############\")\n",
    "          #loss =  add_loss(class_logits[i].argmax()/index + 0.001,torch.tensor(1.0).to(device)+0.001).detach()\n",
    "          loss = abs((class_logits[i].argmax()/index + 0.001)-(torch.tensor(1.0).to(device)+0.001))\n",
    "          \n",
    "        else:\n",
    "          loss = 0\n",
    "       \n",
    "   \n",
    "       \n",
    "      ########################################################################################################\n",
    "      total_loss = (start_loss + end_loss) / 2 + self.class_alpha * class_loss + 0.05* loss \n",
    "      \n",
    "      return total_loss\n",
    "    return start_logits, end_logits, class_logits\n",
    "model = CoQAwithAlbert_l.from_pretrained('albert-base-v2', output_hidden_states=True)\n",
    " # Set model in training mode\n",
    "device=\"cuda\"\n",
    "model.to(device)\n",
    "model.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FI_ykSgPnnQP"
   },
   "outputs": [],
   "source": [
    "#load another model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hzIV7vzPnMhT"
   },
   "source": [
    "#Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1968,
     "status": "ok",
     "timestamp": 1619448326585,
     "user": {
      "displayName": "Ning Zhang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhRCnmT8mmairzuShAibDId7kDGmrCXjp8yIjrCOw=s64",
      "userId": "12536691088853738123"
     },
     "user_tz": -480
    },
    "id": "w_cHAj-88Mue",
    "outputId": "76d64d8d-ec74-481d-b586-b7bd0a4bc07b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████| 2/2 [00:00<00:00,  3.83it/s]\n"
     ]
    }
   ],
   "source": [
    "model =torch.load('outputs/albert_base_8_2linear_l1.pth')\n",
    "model1 = torch.load('outputs/albert_base_8_2linear.pth')\n",
    "# Fetch the results for predictions using the trained model\n",
    "results_for_predictions = []\n",
    "RawResult = collections.namedtuple(\"RawResult\",\n",
    "                                   [\"unique_id\", \"start_logits\", \"end_logits\", \"cls_logits\"])\n",
    "for tqdm_input_ids, tqdm_input_mask, tqdm_segment_ids, tqdm_example_indices in tqdm(\n",
    "                testing_dataloader,\n",
    "                desc=\"Evaluation\",\n",
    "                disable=-1 not in [-1, 0]):\n",
    "   \n",
    "  tqdm_input_ids = tqdm_input_ids.to(device)\n",
    "  tqdm_input_mask = tqdm_input_mask.to(device)\n",
    "  tqdm_segment_ids = tqdm_segment_ids.to(device)\n",
    "\n",
    "  # Get all the results from the model\n",
    "  # Ensemble the results from Albert and BERT\n",
    "  with torch.no_grad():\n",
    "    model_batch_start_logits_0, model_batch_end_logits_0, model_batch_cls_logits_0 = model(tqdm_input_ids, tqdm_segment_ids, tqdm_input_mask) \n",
    "    model_batch_start_logits_1, model_batch_end_logits_1, model_batch_cls_logits_1 = model1(tqdm_input_ids, tqdm_segment_ids, tqdm_input_mask) \n",
    "    model_batch_start_logits = (model_batch_start_logits_0 + model_batch_start_logits_1)/2\n",
    "    model_batch_end_logits = (model_batch_end_logits_0 + model_batch_end_logits_1)/2\n",
    "    model_batch_cls_logits = (model_batch_cls_logits_0 + model_batch_cls_logits_1)/2\n",
    "    #model_batch_start_logits, model_batch_end_logits, model_batch_cls_logits = bert_i(tqdm_input_ids, tqdm_segment_ids, tqdm_input_mask)  \n",
    "    #model_batch_start_logits, model_batch_end_logits, model_batch_cls_logits = albert(tqdm_input_ids, tqdm_segment_ids, tqdm_input_mask)  \n",
    "    #model_batch_start_logits, model_batch_end_logits, model_batch_cls_logits = roberta(tqdm_input_ids, tqdm_segment_ids, tqdm_input_mask)    \n",
    "   # robertamodel_batch_start_logits, robertamodel_batch_end_logits, robertamodel_batch_cls_logits = roberta(tqdm_input_ids, tqdm_segment_ids, tqdm_input_mask)\n",
    "   # bertmodel_batch_start_logits, bertmodel_batch_end_logits, bertmodel_batch_cls_logits = bert(tqdm_input_ids, tqdm_segment_ids, tqdm_input_mask)\n",
    "   # albertmodel_batch_start_logits, albertmodel_batch_end_logits, albertmodel_batch_cls_logits = albert(tqdm_input_ids, tqdm_segment_ids, tqdm_input_mask)\n",
    "  #model_batch_start_logits = (bertmodel_batch_start_logits +albertmodel_batch_start_logits)/2\n",
    "  #model_batch_end_logits = (bertmodel_batch_end_logits+albertmodel_batch_end_logits)/2 \n",
    "  #model_batch_cls_logits =(bertmodel_batch_cls_logits+ albertmodel_batch_cls_logits)/2\n",
    " \n",
    "   \n",
    "  # Get the start end logists from the model and store it in results\n",
    "  for i, tqdm_example_index in enumerate(tqdm_example_indices):\n",
    "    this_start_logits = model_batch_start_logits[i].detach().cpu().tolist()\n",
    "    this_end_logits = model_batch_end_logits[i].detach().cpu().tolist()\n",
    "    this_cls_logits = model_batch_cls_logits[i].detach().cpu().tolist()\n",
    "    testing_feature = testing_features[tqdm_example_index.item()]\n",
    "    unique_id = int(testing_feature.unique_id)\n",
    " \n",
    "    # Store the prediction in the results list\n",
    "    results_for_predictions.append(\n",
    "                    RawResult(unique_id=unique_id,\n",
    "                    start_logits= this_start_logits,\n",
    "                    end_logits= this_end_logits,\n",
    "                    cls_logits= this_cls_logits))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WP8OdIiXnPeK"
   },
   "source": [
    "#Save output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a5-x49P1AHnb"
   },
   "outputs": [],
   "source": [
    "\n",
    "op_pred_file = os.path.join(output_directory, \"Output_Preds_albert_base_b8_ensemble.json\")\n",
    "output_nbest_file = os.path.join(output_directory, \"nbest_predictions_albert_base_b8_ensemble.json\") \n",
    "# Get the appropriate index for the answer text\n",
    "def compute_best_indices(logits, n):\n",
    "  index_with_score = sorted(enumerate(logits),\n",
    "                             key=lambda x: x[1],\n",
    "                             reverse=True)\n",
    "   \n",
    "  best_indices = []\n",
    "  for i in range(len(index_with_score)):\n",
    "    if i >= n:\n",
    "      break\n",
    "    best_indices.append(index_with_score[i][0])\n",
    "  return best_indices\n",
    "# Get the final text fetching it from the span using the predicted answer\n",
    "def output_final_answer_text(predicted_text, original_text, low_case, v_log=False):\n",
    "  def remove_spaces(text):\n",
    "    non_space_chars = []\n",
    "    non_space_char_to_space_char_map = collections.OrderedDict()\n",
    "    for (i, c) in enumerate(text):\n",
    "      if c == \" \":\n",
    "        continue\n",
    "      non_space_char_to_space_char_map[len(non_space_chars)] = i\n",
    "      non_space_chars.append(c)\n",
    "    non_space_text = \"\".join(non_space_chars)\n",
    "    return (non_space_text, non_space_char_to_space_char_map)\n",
    "\n",
    "  # Get the BERT tokenizer to tokenize the text\n",
    "  tokenizer = BasicTokenizer(do_lower_case=low_case)\n",
    "  tokenized_text = \" \".join(tokenizer.tokenize(original_text))\n",
    "\n",
    "  # Find predicted text in the tokenized text to get the start and end positions\n",
    "  start_position = tokenized_text.find(predicted_text)\n",
    "  if start_position == -1:\n",
    "      return original_text\n",
    "  end_position = start_position + len(predicted_text) - 1\n",
    "\n",
    "  # Remove spaces if any\n",
    "  (original_non_space_text, original_non_space_to_space_map) = remove_spaces(original_text)\n",
    "  (tokenized_non_space_text, tokenized_non_space_to_space_map) = remove_spaces(tokenized_text)\n",
    "\n",
    "  if len(original_non_space_text) != len(tokenized_non_space_text):\n",
    "    return original_text\n",
    "\n",
    "  tokenized_non_space_to_space_map = {}\n",
    "  for (i, _tokenized_index) in tokenized_non_space_to_space_map.items():\n",
    "    tokenized_non_space_to_space_map[_tokenized_index] = i\n",
    "\n",
    "  # Get the start position\n",
    "  original_start_position = None\n",
    "  if start_position in tokenized_non_space_to_space_map:\n",
    "    non_space_start_position = tokenized_non_space_to_space_map[start_position]\n",
    "    if non_space_start_position in original_non_space_to_space_map:\n",
    "      original_start_position = original_non_space_to_space_map[non_space_start_position]\n",
    "  \n",
    "  # Check if the start position is None\n",
    "  if original_start_position is None:\n",
    "    return original_text\n",
    "\n",
    "  # Get the End position\n",
    "  original_text_end_position = None\n",
    "  if end_position in tokenized_non_space_to_space_map:\n",
    "    non_space_end_position = tokenized_non_space_to_space_map[end_position]\n",
    "    if non_space_end_position in original_non_space_to_space_map:\n",
    "      original_text_end_position = original_non_space_to_space_map[non_space_end_position]\n",
    "\n",
    "  # Check if the end position is None\n",
    "  if original_text_end_position is None:\n",
    "    return original_text\n",
    "  \n",
    "  # Get the answer using start position and end position in the text\n",
    "  final_output_text = original_text[original_start_position:(original_text_end_position + 1)]\n",
    "  return final_output_text\n",
    "# Calculates the probability of the span found\n",
    "def compute_softmax_score(scores):\n",
    "  if not scores:\n",
    "    return []\n",
    "\n",
    "  maximum_softmax_score = None\n",
    "  for score in scores:\n",
    "    if maximum_softmax_score is None or score > maximum_softmax_score:\n",
    "      maximum_softmax_score = score\n",
    "\n",
    "  expected_scores = []\n",
    "  total_score_sum = 0.0\n",
    "  for score in scores:\n",
    "    x = math.exp(score - maximum_softmax_score)\n",
    "    expected_scores.append(x)\n",
    "    total_score_sum += x\n",
    "\n",
    "  probabilities = []\n",
    "  for score in expected_scores:\n",
    "    probabilities.append(score / total_score_sum) # Probability calculation using the softmax score\n",
    "  return probabilities\n",
    "\n",
    "# Removing punctuations, lowering texts and removing extra white spaces\n",
    "def normalize_answer1(s):\n",
    "    \n",
    "    # Remove articles from the text\n",
    "    def remove_articles(text):\n",
    "        regex = re.compile(r'\\b(a|an|the)\\b', re.UNICODE)\n",
    "        return re.sub(regex, ' ', text)\n",
    "    \n",
    "    # Remove white spaces from the text\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    # Remove punctuations from the text\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    # Lower the text characters\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "# Check if the predictions are numbers or boolean values then set those to string equivalent\n",
    "def confirm_predictions(json_best_predictions):\n",
    "  # Number strings that will be used to represent numbers in the answers instead of actual numbers\n",
    "  subs = ['one', 'two', 'three','four','five','six','seven','eight','nine','ten','eleven','twelve','true','false']\n",
    "  original = json_best_predictions[0]['text']\n",
    "  if len(original) < 2:\n",
    "    for e in json_best_predictions[1:]:\n",
    "      if normalize_answer1(e['text']) in subs:\n",
    "        return e['text']\n",
    "    return 'unknown'\n",
    "  return original\n",
    "# Function to Predict answers and write those predictions to predictions file \n",
    "def predict_answers(test_samples, test_sample_features, results_for_predictions, best_size,\n",
    "                  maximum_answer_length, low_case, op_pred_file, v_log,\n",
    "                  null_score_threshold):\n",
    "  \n",
    "  ex_index_to_feat_index = collections.defaultdict(list)\n",
    "  \n",
    "  # Create the dictionary of all the features in test features keeping feature index as key\n",
    "  for feature in test_sample_features:\n",
    "    ex_index_to_feat_index[feature.example_index].append(feature)\n",
    " \n",
    "  ids_for_results = {}\n",
    "  for result in results_for_predictions:\n",
    "    ids_for_results[result.unique_id] = result\n",
    "   \n",
    "  # Naming the tuples for predictions\n",
    "  Preliminary_Predictions = collections.namedtuple(\n",
    "      \"Preliminary_Predictions\", [\n",
    "                           \"feature_index\",\n",
    "                           \"start_index\",\n",
    "                           \"end_index\",\n",
    "                           \"start_logit\",\n",
    "                           \"end_logit\",\n",
    "                           \"class_logit\",\n",
    "                           \"class_index\",\n",
    "      ])\n",
    "  \n",
    "  complete_predictions = []\n",
    "  best_n_predictions_json = collections.OrderedDict() #in this case best 30 predictions\n",
    "  prediction_scores_json = collections.OrderedDict()\n",
    "  \n",
    "  for (example_index, example) in enumerate(\n",
    "      tqdm(test_samples, desc=\"Predicting...\")):\n",
    "    features = ex_index_to_feat_index[example_index]\n",
    "     \n",
    "    preliminary_predictions = []\n",
    "\n",
    "    part_preliminary_predictions = []\n",
    "\n",
    "    # Indices initialization\n",
    "    score_of_answer_yes, score_of_answer_no, score_span, score_of_no_answer = -float('INF'), -float('INF'), -float('INF'), float('INF')\n",
    "\n",
    "    \n",
    "    minimum_no_answer_feature_index, maximum_yes_feature_index, maximum_no_feature_index, maximum_span_feature_index = 0, 0, 0, 0\n",
    "    max_span_start_indexes, max_span_end_indexes = [], []\n",
    "     \n",
    "    # get the best start and end indices\n",
    "    for (feature_index, feature) in enumerate(features):\n",
    "       \n",
    "      result = ids_for_results[feature.unique_id]\n",
    "      # check the score for each class and determine the output yes, no, or span, or unknown \n",
    "      feature_yes_score, feature_no_score, feature_noanswer_score, feature_span_score = result.cls_logits \n",
    "       \n",
    "      if feature_noanswer_score < score_of_no_answer:\n",
    "        score_of_no_answer = feature_noanswer_score\n",
    "        minimum_no_answer_feature_index = feature_index\n",
    "      if feature_yes_score > score_of_answer_yes:\n",
    "        score_of_answer_yes = feature_yes_score\n",
    "        maximum_yes_feature_index = feature_index\n",
    "      if feature_no_score > score_of_answer_no:\n",
    "        score_of_answer_no = feature_no_score\n",
    "        maximum_no_feature_index = feature_index\n",
    "      # Here hasn't assign the correct class yet\n",
    "      if feature_span_score > score_span:\n",
    "        score_span = feature_span_score\n",
    "        maximum_span_feature_index = feature_index\n",
    "        start_indices = compute_best_indices(result.start_logits,\n",
    "                                                  best_size)\n",
    "        end_indices = compute_best_indices(result.end_logits, best_size)\n",
    "        maximum_span_start_indices, maximum_span_end_indices = start_indices, end_indices\n",
    "      \n",
    "         \n",
    "    \n",
    "    preliminary_predictions.append(\n",
    "        Preliminary_Predictions(feature_index=minimum_no_answer_feature_index,\n",
    "                                start_index=0,\n",
    "                                end_index=0,\n",
    "                                start_logit=-float('INF'),\n",
    "                                end_logit=-float('INF'),\n",
    "                                class_logit=score_of_no_answer,\n",
    "                                class_index=2))\n",
    "    preliminary_predictions.append(\n",
    "              Preliminary_Predictions(feature_index=maximum_yes_feature_index,\n",
    "                                start_index=0,\n",
    "                                end_index=0,\n",
    "                                start_logit=-float('INF'),\n",
    "                                end_logit=-float('INF'),\n",
    "                                class_logit=score_of_answer_yes,\n",
    "                                class_index=0))\n",
    "    preliminary_predictions.append(\n",
    "              Preliminary_Predictions(feature_index=maximum_no_feature_index,\n",
    "                                start_index=0,\n",
    "                                end_index=0,\n",
    "                                start_logit=-float('INF'),\n",
    "                                end_logit=-float('INF'),\n",
    "                                class_logit=score_of_answer_no,\n",
    "                                class_index=1))\n",
    "    preliminary_predictions.append(\n",
    "              Preliminary_Predictions(feature_index=maximum_span_feature_index,\n",
    "                                start_index=0,\n",
    "                                end_index=0,\n",
    "                                start_logit=-float('INF'),\n",
    "                                end_logit=-float('INF'),\n",
    "                                class_logit=score_span,\n",
    "                                class_index=3))\n",
    "   \n",
    "    feature = features[maximum_span_feature_index]\n",
    "    for start_index in maximum_span_start_indices:\n",
    "      for end_index in maximum_span_end_indices:\n",
    "        \n",
    "        if start_index >= len(feature.tokens):\n",
    "          continue\n",
    "        if end_index >= len(feature.tokens):\n",
    "          continue\n",
    "        if start_index not in feature.token_to_origin_mapping:\n",
    "          continue\n",
    "        if end_index not in feature.token_to_origin_mapping:\n",
    "          continue\n",
    "        if not feature.token_max_context.get(start_index, False):\n",
    "          continue\n",
    "        if end_index < start_index:\n",
    "          continue\n",
    "        length = end_index - start_index + 1\n",
    "        if length > maximum_answer_length:\n",
    "          continue\n",
    "        \n",
    "        part_preliminary_predictions.append(\n",
    "                      Preliminary_Predictions(\n",
    "                          feature_index=maximum_span_feature_index,\n",
    "                          start_index=start_index,\n",
    "                          end_index=end_index,\n",
    "                          start_logit=ids_for_results[\n",
    "                              feature.unique_id].start_logits[start_index],\n",
    "                          end_logit=ids_for_results[\n",
    "                              feature.unique_id].end_logits[end_index],\n",
    "                          class_logit=score_span,\n",
    "                          class_index=3))\n",
    "    ##this is to sort the largest score value for start and end pair    \n",
    "    part_preliminary_predictions = sorted(\n",
    "              part_preliminary_predictions,\n",
    "              key=lambda p: p.start_logit + p.end_logit,\n",
    "              reverse=True)\n",
    "    ##this is to sort the largest score value for class \n",
    "    preliminary_predictions = sorted(preliminary_predictions,\n",
    "                                      key=lambda p: p.class_logit,\n",
    "                                      reverse=True)\n",
    "    \n",
    "    Best_Predictions = collections.namedtuple(  \n",
    "              \"Best_Predictions\",\n",
    "              [\"text\", \"start_logit\", \"end_logit\", \"class_logit\", \"class_index\"])\n",
    "    \n",
    "    known_predictions = {}\n",
    "    best = []\n",
    "    class_rank = []\n",
    "    for prediction in part_preliminary_predictions:\n",
    "      if len(best) >= best_size:\n",
    "        break\n",
    "      feature = features[prediction.feature_index]\n",
    "      if prediction.class_index == 3:\n",
    "        tokenized_tokens = feature.tokens[prediction.start_index:(prediction.end_index + 1)]\n",
    "        original_document_start = feature.token_to_origin_mapping[prediction.start_index]\n",
    "        original_document_end = feature.token_to_origin_mapping[prediction.end_index]\n",
    "        original_tokens = example.document_tokens[original_document_start:(original_document_end + 1)]\n",
    "        \n",
    "        tokenized_text = \" \".join(tokenized_tokens)\n",
    "\n",
    "        tokenized_text = tokenized_text.replace(\" ##\", \"\")\n",
    "        tokenized_text = tokenized_text.replace(\"##\", \"\")\n",
    "\n",
    "        tokenized_text = tokenized_text.strip()\n",
    "        tokenized_text = \" \".join(tokenized_text.split())\n",
    "        original_text = \" \".join(original_tokens)\n",
    "\n",
    "        final_output_text = output_final_answer_text(tokenized_text, original_text, low_case, v_log)\n",
    "         \n",
    "        if final_output_text in known_predictions:\n",
    "          continue\n",
    "\n",
    "        known_predictions[final_output_text] = True\n",
    "        best.append(\n",
    "                      Best_Predictions(text=final_output_text,\n",
    "                                      start_logit=prediction.start_logit,\n",
    "                                      end_logit=prediction.end_logit,\n",
    "                                      class_logit=prediction.class_logit,\n",
    "                                      class_index=prediction.class_index))\n",
    "    \n",
    "    # Writing the approriate answers in predictions which will be written to json file\n",
    "    if not best or len(best) < 1: \n",
    "      best.append(\n",
    "                  Best_Predictions(text=\"unknown\",\n",
    "                                  start_logit=-float('INF'),\n",
    "                                  end_logit=-float('INF'),\n",
    "                                  class_logit=score_span,\n",
    "                                  class_index=3))\n",
    "    for prediction in preliminary_predictions:\n",
    "      if prediction.class_index == 3:\n",
    "        final_output_text = best[0].text\n",
    "        class_rank.append(\n",
    "                      Best_Predictions(text=final_output_text,\n",
    "                                      start_logit=best[0].start_logit,\n",
    "                                      end_logit=best[0].end_logit,\n",
    "                                      class_logit=prediction.class_logit,\n",
    "                                      class_index=prediction.class_index))\n",
    "      elif prediction.class_index == 0:\n",
    "        final_output_text = \"yes\"\n",
    "        class_rank.append(\n",
    "                      Best_Predictions(text=final_output_text,\n",
    "                                      start_logit=-float('INF'),\n",
    "                                      end_logit=-float('INF'),\n",
    "                                      class_logit=prediction.class_logit,\n",
    "                                      class_index=prediction.class_index))\n",
    "      elif prediction.class_index == 1:\n",
    "                  final_output_text = \"no\"\n",
    "                  class_rank.append(\n",
    "                      Best_Predictions(text=final_output_text,\n",
    "                                      start_logit=-float('INF'),\n",
    "                                      end_logit=-float('INF'),\n",
    "                                      class_logit=prediction.class_logit,\n",
    "                                      class_index=prediction.class_index))\n",
    "      elif prediction.class_index == 2:\n",
    "                  final_output_text = \"unknown\"\n",
    "                  class_rank.append(\n",
    "                      Best_Predictions(text=final_output_text,\n",
    "                                      start_logit=-float('INF'),\n",
    "                                      end_logit=-float('INF'),\n",
    "                                      class_logit=prediction.class_logit,\n",
    "                                      class_index=prediction.class_index))\n",
    "                  \n",
    "    assert len(best) >= 1\n",
    "\n",
    "    total_scores = []\n",
    "    class_scores = []\n",
    "    for item in best:\n",
    "      total_scores.append(item.start_logit + item.end_logit)\n",
    "    for rank in class_rank:\n",
    "      class_scores.append(rank.class_logit)\n",
    "    \n",
    "    # calculate Softmax\n",
    "    span_probabilities = compute_softmax_score(total_scores)\n",
    "    class_probabilities = compute_softmax_score(class_scores)\n",
    "    best_predictions_json = []\n",
    "\n",
    "    current_rank, current_probabilities, current_scores = (\n",
    "        best, span_probabilities,\n",
    "        total_scores) if class_rank[0].class_index == 3 and len(best) > 1 else (\n",
    "            class_rank, class_probabilities, class_scores)\n",
    "    \n",
    "    \n",
    "    # Store the answer text, probability and score for each entry\n",
    "    for i, entry in enumerate(current_rank): \n",
    "      predicted_outputs = collections.OrderedDict()\n",
    "      predicted_outputs[\"text\"] = entry.text\n",
    "      predicted_outputs[\"probability\"] = current_probabilities[i]\n",
    "      predicted_outputs[\"score\"] = current_scores[i]\n",
    "      best_predictions_json.append(predicted_outputs)\n",
    "\n",
    "    assert len(best_predictions_json) >= 1\n",
    "\n",
    "    _id, _turn_id = example.question_answer_id.split()\n",
    "    complete_predictions.append({'id': _id, 'turn_id': int(_turn_id), 'answer': confirm_predictions(best_predictions_json)})\n",
    "    best_n_predictions_json[example.question_answer_id] = best_predictions_json\n",
    "\n",
    "  # Write the prediction files\n",
    "  with open(op_pred_file, \"w\") as writer:\n",
    "        writer.write(json.dumps(complete_predictions, indent=4) + \"\\n\")\n",
    "  \n",
    "  with open(output_nbest_file, \"w\") as writer:\n",
    "        writer.write(json.dumps(best_n_predictions_json, indent=4) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1154,
     "status": "ok",
     "timestamp": 1619448332231,
     "user": {
      "displayName": "Ning Zhang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhRCnmT8mmairzuShAibDId7kDGmrCXjp8yIjrCOw=s64",
      "userId": "12536691088853738123"
     },
     "user_tz": -480
    },
    "id": "HxhzcNKEAw1Z",
    "outputId": "7294074d-aaf9-4fb1-c7ee-2d2868a45b9f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting...: 100%|██████████| 12/12 [00:00<00:00, 239.52it/s]\n"
     ]
    }
   ],
   "source": [
    "# Call to predict answers functions to write them to predictions file\n",
    "predict_answers(testing_samples, testing_features, results_for_predictions, 20, 30, True, \n",
    "                  op_pred_file, True, 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lAQvfzgUnTuG"
   },
   "source": [
    "#Calculate Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1762,
     "status": "ok",
     "timestamp": 1619448369529,
     "user": {
      "displayName": "Ning Zhang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhRCnmT8mmairzuShAibDId7kDGmrCXjp8yIjrCOw=s64",
      "userId": "12536691088853738123"
     },
     "user_tz": -480
    },
    "id": "LDWdmf71f8sI",
    "outputId": "4244d7f4-8467-438d-eb74-2b221c27aab0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing prediction for 3i02618ya06g9pi2dcnttyux9nopu3 and turn_id: 1\n",
      "Missing prediction for 3i02618ya06g9pi2dcnttyux9nopu3 and turn_id: 2\n",
      "Missing prediction for 3i02618ya06g9pi2dcnttyux9nopu3 and turn_id: 3\n",
      "Missing prediction for 3i02618ya06g9pi2dcnttyux9nopu3 and turn_id: 4\n",
      "Missing prediction for 3i02618ya06g9pi2dcnttyux9nopu3 and turn_id: 5\n",
      "Missing prediction for 3i02618ya06g9pi2dcnttyux9nopu3 and turn_id: 6\n",
      "Missing prediction for 3i02618ya06g9pi2dcnttyux9nopu3 and turn_id: 7\n",
      "Missing prediction for 3i02618ya06g9pi2dcnttyux9nopu3 and turn_id: 8\n",
      "Missing prediction for 3i02618ya06g9pi2dcnttyux9nopu3 and turn_id: 9\n",
      "Missing prediction for 3i02618ya06g9pi2dcnttyux9nopu3 and turn_id: 10\n",
      "Missing prediction for 3i02618ya06g9pi2dcnttyux9nopu3 and turn_id: 11\n",
      "Missing prediction for 3i02618ya06g9pi2dcnttyux9nopu3 and turn_id: 12\n",
      "Missing prediction for 3i02618ya06g9pi2dcnttyux9nopu3 and turn_id: 13\n",
      "Missing prediction for 3i02618ya06g9pi2dcnttyux9nopu3 and turn_id: 14\n",
      "Missing prediction for 3i02618ya06g9pi2dcnttyux9nopu3 and turn_id: 15\n",
      "Missing prediction for 3i02618ya06g9pi2dcnttyux9nopu3 and turn_id: 16\n",
      "Missing prediction for 3i02618ya06g9pi2dcnttyux9nopu3 and turn_id: 17\n",
      "Missing prediction for 3i02618ya06g9pi2dcnttyux9nopu3 and turn_id: 18\n",
      "Missing prediction for 3i02618ya06g9pi2dcnttyux9nopu3 and turn_id: 19\n",
      "Missing prediction for 3ixqg4fa2tygl3tpwwa12i2uf58b90 and turn_id: 1\n",
      "Missing prediction for 3ixqg4fa2tygl3tpwwa12i2uf58b90 and turn_id: 2\n",
      "Missing prediction for 3ixqg4fa2tygl3tpwwa12i2uf58b90 and turn_id: 3\n",
      "Missing prediction for 3ixqg4fa2tygl3tpwwa12i2uf58b90 and turn_id: 4\n",
      "Missing prediction for 3ixqg4fa2tygl3tpwwa12i2uf58b90 and turn_id: 5\n",
      "Missing prediction for 3ixqg4fa2tygl3tpwwa12i2uf58b90 and turn_id: 6\n",
      "Missing prediction for 3ixqg4fa2tygl3tpwwa12i2uf58b90 and turn_id: 7\n",
      "Missing prediction for 3ixqg4fa2tygl3tpwwa12i2uf58b90 and turn_id: 8\n",
      "Missing prediction for 3ixqg4fa2tygl3tpwwa12i2uf58b90 and turn_id: 9\n",
      "Missing prediction for 3ixqg4fa2tygl3tpwwa12i2uf58b90 and turn_id: 10\n",
      "Missing prediction for 3ixqg4fa2tygl3tpwwa12i2uf58b90 and turn_id: 11\n",
      "Missing prediction for 3ixqg4fa2tygl3tpwwa12i2uf58b90 and turn_id: 12\n",
      "{\n",
      "  \"children_stories\": {\n",
      "    \"em\": 0.0,\n",
      "    \"f1\": 0.0,\n",
      "    \"turns\": 12\n",
      "  },\n",
      "  \"literature\": {\n",
      "    \"em\": 0.0,\n",
      "    \"f1\": 0.0,\n",
      "    \"turns\": 0\n",
      "  },\n",
      "  \"mid-high_school\": {\n",
      "    \"em\": 0.0,\n",
      "    \"f1\": 0.0,\n",
      "    \"turns\": 19\n",
      "  },\n",
      "  \"news\": {\n",
      "    \"em\": 0.0,\n",
      "    \"f1\": 0.0,\n",
      "    \"turns\": 0\n",
      "  },\n",
      "  \"wikipedia\": {\n",
      "    \"em\": 0.0,\n",
      "    \"f1\": 0.0,\n",
      "    \"turns\": 12\n",
      "  },\n",
      "  \"reddit\": {\n",
      "    \"em\": 0.0,\n",
      "    \"f1\": 0.0,\n",
      "    \"turns\": 0\n",
      "  },\n",
      "  \"science\": {\n",
      "    \"em\": 0.0,\n",
      "    \"f1\": 0.0,\n",
      "    \"turns\": 0\n",
      "  },\n",
      "  \"in_domain\": {\n",
      "    \"em\": 0.0,\n",
      "    \"f1\": 0.0,\n",
      "    \"turns\": 43\n",
      "  },\n",
      "  \"out_domain\": {\n",
      "    \"em\": 0.0,\n",
      "    \"f1\": 0.0,\n",
      "    \"turns\": 0\n",
      "  },\n",
      "  \"overall\": {\n",
      "    \"em\": 0.0,\n",
      "    \"f1\": 0.0,\n",
      "    \"turns\": 43\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!python3 evaluate.py --data-file data/coqa-dev-v1.0.json --pred-file outputs/Output_Preds_albert_base_b8_ensemble.json\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n_d2tP70IKT4"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyO9Vp3SmW49MdCmCsme0nF6",
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "albert_ensembe_b8.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "02befd9e4e1b4b7bab5b079cb5e8d1dd": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6a09d69f54ce419f85ee6af55f128b16",
       "IPY_MODEL_5dd14753f10d48a6945226a34be98921"
      ],
      "layout": "IPY_MODEL_90a72d1cf56f45c9837a28e770a1df9f"
     }
    },
    "04b2d77fb0f9418c86dda5073c790d00": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "05b75e87009447eb82cc9fc288fd5749": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_48c66d8ae5454384846f8b628b26b264",
       "IPY_MODEL_7838971635474042b4948fb3902f18df"
      ],
      "layout": "IPY_MODEL_ee318d357b82442085188ffdb2460e86"
     }
    },
    "10338a2cbc69478c85dfd02f0cba11e8": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "135e174a10da4e26a1e860f855db0a85": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2f7815e80abc4729b6bc674cedec8c8e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fae6c8b6835042eba391d763c73e828f",
      "max": 760289,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_baebbdd4b0564673b44b062deb887cfb",
      "value": 760289
     }
    },
    "30bc2c43965c44379ded9bb74b067767": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "48c66d8ae5454384846f8b628b26b264": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_842c450a8bce46b8909cabf6d5907337",
      "max": 47376696,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_82f857fc8e0943fb9d180a4a7a8c80d3",
      "value": 47376696
     }
    },
    "4ce9c85e72624c34a5090f09663d470d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f351803bc8ea402ba7ca255933908197",
      "placeholder": "​",
      "style": "IPY_MODEL_84dc144fd3aa430fb420750ce8f1591c",
      "value": " 760k/760k [02:05&lt;00:00, 6.07kB/s]"
     }
    },
    "52cdac639c934378b29de9a43ea866e5": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2f7815e80abc4729b6bc674cedec8c8e",
       "IPY_MODEL_4ce9c85e72624c34a5090f09663d470d"
      ],
      "layout": "IPY_MODEL_10338a2cbc69478c85dfd02f0cba11e8"
     }
    },
    "5dd14753f10d48a6945226a34be98921": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fa4f60dce7334709881750b5aff07b8e",
      "placeholder": "​",
      "style": "IPY_MODEL_a57460bf01824fefae53fdc48fd9768a",
      "value": " 684/684 [00:01&lt;00:00, 510B/s]"
     }
    },
    "663195f9a5e44822b6044593bfa95e63": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6a09d69f54ce419f85ee6af55f128b16": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ea287411a7ee40fd89ef7c8041ff5347",
      "max": 684,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f4cd828157aa434796d5694690a8b09a",
      "value": 684
     }
    },
    "6e8d7fdf087c4211816840a4a3dddfad": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7838971635474042b4948fb3902f18df": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_813ecb446fa74295b3fad5eaf8d116ac",
      "placeholder": "​",
      "style": "IPY_MODEL_135e174a10da4e26a1e860f855db0a85",
      "value": " 47.4M/47.4M [01:07&lt;00:00, 706kB/s]"
     }
    },
    "788bae25caf74591864f3f4f2b32e5bb": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e250780d9101411b84ea7fb8f107cd21",
       "IPY_MODEL_a61a9d18df7d41f284f91cc33909bd37"
      ],
      "layout": "IPY_MODEL_30bc2c43965c44379ded9bb74b067767"
     }
    },
    "813ecb446fa74295b3fad5eaf8d116ac": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "82f857fc8e0943fb9d180a4a7a8c80d3": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "842c450a8bce46b8909cabf6d5907337": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "84dc144fd3aa430fb420750ce8f1591c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "90a72d1cf56f45c9837a28e770a1df9f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a57460bf01824fefae53fdc48fd9768a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a61a9d18df7d41f284f91cc33909bd37": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d8e4e790891d4f478edec319a58b173d",
      "placeholder": "​",
      "style": "IPY_MODEL_663195f9a5e44822b6044593bfa95e63",
      "value": " 1.31M/1.31M [00:04&lt;00:00, 266kB/s]"
     }
    },
    "baebbdd4b0564673b44b062deb887cfb": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "d8e4e790891d4f478edec319a58b173d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e250780d9101411b84ea7fb8f107cd21": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6e8d7fdf087c4211816840a4a3dddfad",
      "max": 1312669,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_04b2d77fb0f9418c86dda5073c790d00",
      "value": 1312669
     }
    },
    "ea287411a7ee40fd89ef7c8041ff5347": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ee318d357b82442085188ffdb2460e86": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f351803bc8ea402ba7ca255933908197": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f4cd828157aa434796d5694690a8b09a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "fa4f60dce7334709881750b5aff07b8e": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fae6c8b6835042eba391d763c73e828f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
