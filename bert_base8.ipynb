{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NB54koJNRUmB"
   },
   "source": [
    "https://huggingface.co/transformers/_modules/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26983,
     "status": "ok",
     "timestamp": 1619398780668,
     "user": {
      "displayName": "Ning Zhang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhRCnmT8mmairzuShAibDId7kDGmrCXjp8yIjrCOw=s64",
      "userId": "12536691088853738123"
     },
     "user_tz": -480
    },
    "id": "MyFzK7mFiW-6",
    "outputId": "4975c749-925e-43c3-fc71-3a7e889914d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n",
      "/content/gdrive/My Drive/AI Sem II/NLP/Project Test/transformers-coqa-master\n",
      "/content/gdrive/My Drive/AI Sem II/NLP/Project Test/transformers-coqa-master\n"
     ]
    }
   ],
   "source": [
    "# For Google Colaboratory\n",
    "import sys, os\n",
    "if 'google.colab' in sys.modules:\n",
    "    # mount google drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    " \n",
    "    path_to_file = '/content/gdrive/My Drive/AI Sem II/NLP/Project Test/'\n",
    "    print(path_to_file)\n",
    "    # change current path to the folder containing \"file_name\"\n",
    "    os.chdir(path_to_file)\n",
    "    !pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21328,
     "status": "ok",
     "timestamp": 1619398803053,
     "user": {
      "displayName": "Ning Zhang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhRCnmT8mmairzuShAibDId7kDGmrCXjp8yIjrCOw=s64",
      "userId": "12536691088853738123"
     },
     "user_tz": -480
    },
    "id": "MYFgNG28jXnW",
    "outputId": "b0c05d7d-0f02-4af4-d3ab-84032526a81a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/huggingface/transformers\n",
      "  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-guk62kum\n",
      "  Running command git clone -q https://github.com/huggingface/transformers /tmp/pip-req-build-guk62kum\n",
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0.dev0) (4.41.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0.dev0) (1.19.5)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0.dev0) (2019.12.20)\n",
      "Collecting huggingface-hub>=0.0.8\n",
      "  Downloading https://files.pythonhosted.org/packages/a1/88/7b1e45720ecf59c6c6737ff332f41c955963090a18e72acbcbeac6b25e86/huggingface_hub-0.0.8-py3-none-any.whl\n",
      "Collecting tokenizers<0.11,>=0.10.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n",
      "\u001b[K     |████████████████████████████████| 3.3MB 6.4MB/s \n",
      "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0.dev0) (20.9)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0.dev0) (3.10.1)\n",
      "Collecting sacremoses\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
      "\u001b[K     |████████████████████████████████| 901kB 29.0MB/s \n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0.dev0) (2.23.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0.dev0) (3.0.12)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.6.0.dev0) (2.4.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.6.0.dev0) (3.4.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.6.0.dev0) (3.7.4.3)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.6.0.dev0) (1.0.1)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.6.0.dev0) (7.1.2)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.6.0.dev0) (1.15.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.6.0.dev0) (2020.12.5)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.6.0.dev0) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.6.0.dev0) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.6.0.dev0) (1.24.3)\n",
      "Building wheels for collected packages: transformers\n",
      "  Building wheel for transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for transformers: filename=transformers-4.6.0.dev0-cp37-none-any.whl size=2120912 sha256=46c6ed2679cfd71988c07dcf34a0b608d44304f9cccc11f581d1ffc81ea5b78b\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-jlb_9x8b/wheels/70/d3/52/b3fa4f8b8ef04167ac62e5bb2accb62ae764db2a378247490e\n",
      "Successfully built transformers\n",
      "Installing collected packages: huggingface-hub, tokenizers, sacremoses, transformers\n",
      "Successfully installed huggingface-hub-0.0.8 sacremoses-0.0.45 tokenizers-0.10.2 transformers-4.6.0.dev0\n"
     ]
    }
   ],
   "source": [
    "pip install git+https://github.com/huggingface/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2XfyPtO2jZlO",
    "outputId": "c85da46b-06e2-481d-f57a-d774f4d22559"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1b/d8/0361bbaf7a1ff56b44dca04dace54c82d63dad7475b7d25ea1baefafafb2/spacy-3.0.6-cp37-cp37m-manylinux2014_x86_64.whl (12.8MB)\n",
      "\u001b[K     |████████████████████████████████| 12.8MB 5.5MB/s \n",
      "\u001b[?25hCollecting thinc<8.1.0,>=8.0.3\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/61/87/decceba68a0c6ca356ddcb6aea8b2500e71d9bc187f148aae19b747b7d3c/thinc-8.0.3-cp37-cp37m-manylinux2014_x86_64.whl (1.1MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1MB 48.8MB/s \n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (56.0.0)\n",
      "Collecting pydantic<1.8.0,>=1.7.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b3/0a/52ae1c659fc08f13dd7c0ae07b88e4f807ad83fb9954a59b0b0a3d1a8ab6/pydantic-1.7.3-cp37-cp37m-manylinux2014_x86_64.whl (9.1MB)\n",
      "\u001b[K     |████████████████████████████████| 9.1MB 49.2MB/s \n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.5)\n",
      "Requirement already satisfied, skipping upgrade: typing-extensions<4.0.0.0,>=3.7.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from spacy) (3.7.4.3)\n",
      "Collecting srsly<3.0.0,>=2.4.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c3/84/dfdfc9f6f04f6b88207d96d9520b911e5fec0c67ff47a0dea31ab5429a1e/srsly-2.4.1-cp37-cp37m-manylinux2014_x86_64.whl (456kB)\n",
      "\u001b[K     |████████████████████████████████| 460kB 40.1MB/s \n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.11.3)\n",
      "Requirement already satisfied, skipping upgrade: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.8.2)\n",
      "Requirement already satisfied, skipping upgrade: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
      "Requirement already satisfied, skipping upgrade: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
      "Collecting typer<0.4.0,>=0.3.0\n",
      "  Downloading https://files.pythonhosted.org/packages/90/34/d138832f6945432c638f32137e6c79a3b682f06a63c488dcfaca6b166c64/typer-0.3.2-py3-none-any.whl\n",
      "Requirement already satisfied, skipping upgrade: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.41.1)\n",
      "Requirement already satisfied, skipping upgrade: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.5)\n",
      "Requirement already satisfied, skipping upgrade: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (20.9)\n",
      "Requirement already satisfied, skipping upgrade: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
      "Collecting catalogue<2.1.0,>=2.0.3\n",
      "  Downloading https://files.pythonhosted.org/packages/82/a5/b5021c74c04cac35a27d34cbf3146d86eb8e173b4491888bc4908c4c8b3b/catalogue-2.0.3-py3-none-any.whl\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.19.5)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.4\n",
      "  Downloading https://files.pythonhosted.org/packages/8d/67/d4002a18e26bf29b17ab563ddb55232b445ab6a02f97bf17d1345ff34d3f/spacy_legacy-3.0.5-py2.py3-none-any.whl\n",
      "Collecting pathy>=0.3.5\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/13/87/5991d87be8ed60beb172b4062dbafef18b32fa559635a8e2b633c2974f85/pathy-0.5.2-py3-none-any.whl (42kB)\n",
      "\u001b[K     |████████████████████████████████| 51kB 6.7MB/s \n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy) (1.1.1)\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.12.5)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
      "Requirement already satisfied, skipping upgrade: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.0->spacy) (7.1.2)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy) (2.4.7)\n",
      "Requirement already satisfied, skipping upgrade: zipp>=0.5; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.3->spacy) (3.4.1)\n",
      "Collecting smart-open<4.0.0,>=2.2.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/11/9a/ba2d5f67f25e8d5bbf2fcec7a99b1e38428e83cb715f64dd179ca43a11bb/smart_open-3.0.0.tar.gz (113kB)\n",
      "\u001b[K     |████████████████████████████████| 122kB 50.1MB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: smart-open\n",
      "  Building wheel for smart-open (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for smart-open: filename=smart_open-3.0.0-cp37-none-any.whl size=107098 sha256=e28ee9aea3a6e7eabff55e2add68aa2bc39f854a8df256b5638eee3f5f6977dc\n",
      "  Stored in directory: /root/.cache/pip/wheels/18/88/7c/f06dabd5e9cabe02d2269167bcacbbf9b47d0c0ff7d6ebcb78\n",
      "Successfully built smart-open\n",
      "Installing collected packages: catalogue, srsly, pydantic, thinc, typer, spacy-legacy, smart-open, pathy, spacy\n",
      "  Found existing installation: catalogue 1.0.0\n",
      "    Uninstalling catalogue-1.0.0:\n",
      "      Successfully uninstalled catalogue-1.0.0\n",
      "  Found existing installation: srsly 1.0.5\n",
      "    Uninstalling srsly-1.0.5:\n",
      "      Successfully uninstalled srsly-1.0.5\n",
      "  Found existing installation: thinc 7.4.0\n",
      "    Uninstalling thinc-7.4.0:\n",
      "      Successfully uninstalled thinc-7.4.0\n",
      "  Found existing installation: smart-open 5.0.0\n",
      "    Uninstalling smart-open-5.0.0:\n",
      "      Successfully uninstalled smart-open-5.0.0\n",
      "  Found existing installation: spacy 2.2.4\n",
      "    Uninstalling spacy-2.2.4:\n"
     ]
    }
   ],
   "source": [
    "!pip install -U spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W86G1qWTjwdc"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import string\n",
    "import transformers\n",
    "import re\n",
    "from tqdm import tqdm, trange\n",
    "import spacy \n",
    "import spacy.cli\n",
    "import logging\n",
    "import collections\n",
    "from collections import Counter\n",
    "import math\n",
    "import torch\n",
    "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler, TensorDataset)\n",
    "from transformers import BertModel, BertTokenizer, AdamW\n",
    "#from pytorch_pretrained_bert.modeling import BertModel, BertPretrainedModel\n",
    "#from pytorch_pretrained_bert.tokenization import BertTokenizer\n",
    "#from pytorch_pretrained_bert.optimization import BertAdam, WarmupLinearSchedule\n",
    "#from pytorch_ptrtrained_bert.tokenization import BasicTokenizer, whitespace_tokenize\n",
    "from transformers.models.bert.tokenization_bert import BasicTokenizer\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import json\n",
    "spacy.cli.download('en')\n",
    "spacy.load('en_core_web_sm')\n",
    "import random\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import matplotlib as pplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FleUeEhpj01T"
   },
   "outputs": [],
   "source": [
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "#model =BertModel.from_pretrained('bert-base-uncased')\n",
    "output_directory = 'outputs'\n",
    "\n",
    "# If output directory doesn't exist create one\n",
    "if not os.path.exists(output_directory):\n",
    "  os.makedirs(output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d4WXOe-3j4SE"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#from transformers import BartModel\n",
    "#BartPreTrainedModel = transformers.PretrainedBartModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U6rFZIJNHlU3"
   },
   "outputs": [],
   "source": [
    "#Class to store questions, their answers along with the starting index of the answer and end index of answer and tokens in the story\n",
    "class QA(object):\n",
    "  def __init__(\n",
    "            self,\n",
    "            question_answer_id,\n",
    "            question_text,\n",
    "            document_tokens,\n",
    "            original_answer_text=None,\n",
    "            answer_start_position=None,\n",
    "            answer_end_position=None,\n",
    "            additional_answers=None,\n",
    "    ):\n",
    "    self.question_answer_id = question_answer_id\n",
    "    self.question_text = question_text\n",
    "    self.document_tokens = document_tokens\n",
    "    self.original_answer_text = original_answer_text\n",
    "    self.answer_start_position = answer_start_position\n",
    "    self.answer_end_position = answer_end_position\n",
    "    self.additional_answers = additional_answers\n",
    "\n",
    "# Class to store features, input ids, input mask, segment ids, start positions, end positions, etc\n",
    "class DataFeatures(object):\n",
    "  def __init__(self,\n",
    "                 unique_id,\n",
    "                 example_index,\n",
    "                 document_span_index,\n",
    "                 tokens,\n",
    "                 token_to_origin_mapping,\n",
    "                 token_max_context,\n",
    "                 input_ids,\n",
    "                 input_mask,\n",
    "                 segments,\n",
    "                 start_position=None,\n",
    "                 end_position=None,\n",
    "                 class_index=None):            \n",
    "    self.unique_id = unique_id\n",
    "    self.example_index = example_index\n",
    "    self.document_span_index = document_span_index\n",
    "    self.tokens = tokens\n",
    "    self.token_to_origin_mapping = token_to_origin_mapping\n",
    "    self.token_max_context = token_max_context\n",
    "    self.input_ids = input_ids\n",
    "    self.input_mask = input_mask\n",
    "    self.segments = segments\n",
    "    self.start_position = start_position\n",
    "    self.end_position = end_position\n",
    "    self.class_index = class_index\n",
    "\n",
    "\n",
    "# Function to read COQA datasets and process the data\n",
    "def get_data_from_coqa(isTrain, input_file, history_len=2, add_QA_tag=False):\n",
    "\n",
    "# Check if the character is a white space\n",
    "  def check_whitespace(char):\n",
    "        if char == \" \" or char == \"\\t\" or char == \"\\r\" or char == \"\\n\" or ord(char) == 0x202F:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "# Token conversion\n",
    "  def tokenize_string(str):\n",
    "        if (str.lower() == '-lrb-'):\n",
    "            str = '('\n",
    "        elif (str.lower() == '-rrb-'):\n",
    "            str = ')'\n",
    "        elif (str.lower() == '-lsb-'):\n",
    "            str = '['\n",
    "        elif (str.lower() == '-rsb-'):\n",
    "            str = ']'\n",
    "        elif (str.lower() == '-lcb-'):\n",
    "            str = '{'\n",
    "        elif (str.lower() == '-rcb-'):\n",
    "            str = '}'\n",
    "        return str\n",
    "\n",
    "  def space_extension(matchobject):\n",
    "    return ' ' + matchobject.group(0) + ' '  \n",
    "\n",
    " # Preprocessing \n",
    "  def pre_processing(word):\n",
    "    word = re.sub(\n",
    "        u'-|\\u2010|\\u2011|\\u2012|\\u2013|\\u2014|\\u2015|%|\\[|\\]|:|\\(|\\)|/|\\t',\n",
    "        space_extension, word)\n",
    "    word = word.strip(' \\n')\n",
    "    word = re.sub('\\s+', ' ', word)\n",
    "    return word\n",
    "\n",
    "  # Process text to return output as words with their indexes in the sentences \n",
    "  def processing(main_text):\n",
    "          processed_text = {'word': [], 'offsets': [], 'sentences': []}\n",
    " \n",
    "          for token in main_text:\n",
    "              processed_text['word'].append(tokenize_string(token.text))\n",
    "              processed_text['offsets'].append((token.idx, token.idx + len(token.text)))\n",
    "          #print(\"=======offset is\", (token.idx, token.idx + len(token.text)), \"=====word is\", tokenize_string(token.text))\n",
    "\n",
    "          word_index = 0\n",
    "          for sentence in main_text.sents:\n",
    "              processed_text['sentences'].append((word_index, word_index + len(sentence)))\n",
    "              word_index += len(sentence)\n",
    "          #print(\"=======sentence is\",  word_index, word_index + len(sentence) )\n",
    "\n",
    "          assert word_index == len(processed_text['word'])\n",
    "           \n",
    "          return processed_text\n",
    "\n",
    "  # Get the context offsets\n",
    "  def context_offsets(words, raw_text):\n",
    "    #print(words, raw_text)\n",
    "    raw_text_context_offsets = []\n",
    "    r = 0\n",
    "    for token in words:\n",
    "        while r < len(raw_text) and re.match('\\s', raw_text[r]):\n",
    "            r += 1\n",
    "        if raw_text[r:r + len(token)] != token:\n",
    "            print('Error', token, 'Raw Text:', raw_text)\n",
    "\n",
    "        raw_text_context_offsets.append((r, r + len(token)))\n",
    "        r += len(token)\n",
    " \n",
    "    return raw_text_context_offsets\n",
    "\n",
    "  # Function to find span with start and end index provided\n",
    "  def define_span_indices(offsets, start_pos, end_pos):\n",
    "     \n",
    "    span_start_index = -1\n",
    "    span_end_index = -1\n",
    "    #print(\"offset is\", offsets)\n",
    "    for i, offset in enumerate(offsets):\n",
    "        if (span_start_index < 0) or (start_pos >= offset[0]):\n",
    "            span_start_index = i\n",
    "        if (span_end_index < 0) and (end_pos <= offset[1]):\n",
    "            span_end_index = i\n",
    "    #print(\"span\",span_start_index, span_end_index)\n",
    "    return (span_start_index, span_end_index)\n",
    "\n",
    "# Removing punctuations, lowering texts and removing extra white spaces\n",
    "  def pre_process_answer(s):\n",
    "    \n",
    "    \n",
    "    # Remove articles from the text\n",
    "    def remove_articles(text):\n",
    "        reg_expression = re.compile(r'\\b(a|an|the)\\b', re.UNICODE)\n",
    "        return re.sub(reg_expression, ' ', text)\n",
    "    \n",
    "    # Remove white spaces from the text\n",
    "    def adjust_white_space(text):\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    # Remove punctuations from the text\n",
    "    def remove_punctuations(text):\n",
    "        rem = set(string.punctuation)\n",
    "        return ''.join(c for c in text if c not in rem)\n",
    "\n",
    "    # Lower the text characters\n",
    "    def lowering_text(text):\n",
    "        return text.lower()\n",
    "     \n",
    "\n",
    "    return adjust_white_space(remove_articles(remove_punctuations(lowering_text(s))))  \n",
    "\n",
    "# Find the span providing the context and offsets  \n",
    "  def span_with_ground_truth(context, offsets, ground_truth):\n",
    "\n",
    "    best_F1 = 0.0\n",
    "    best_span = (len(offsets) - 1, len(offsets) - 1)\n",
    "    ground_truth_temp = pre_process_answer(pre_processing(ground_truth)).split()\n",
    "\n",
    "    ls = [\n",
    "        i for i in range(len(offsets))\n",
    "        if context[offsets[i][0]:offsets[i][1]].lower() in ground_truth\n",
    "    ]\n",
    "\n",
    "    for i in range(len(ls)):\n",
    "        for j in range(i, len(ls)):\n",
    "            prediction = pre_process_answer(\n",
    "                pre_processing(\n",
    "                    context[offsets[ls[i]][0]:offsets[ls[j]][1]])).split()\n",
    "            common = Counter(prediction) & Counter(ground_truth_temp)\n",
    "            num_same = sum(common.values())\n",
    "\n",
    "            #print(num_same, \"common span\")\n",
    "            if num_same > 0:\n",
    "                precision = 1.0 * num_same / len(prediction)\n",
    "                recall = 1.0 * num_same / len(ground_truth_temp)\n",
    "                F1 = (2 * precision * recall) / (precision + recall)\n",
    "                if F1 > best_F1:\n",
    "                    best_F1 = F1\n",
    "                    best_span = (ls[i], ls[j])\n",
    "     \n",
    "    #print(best_span, \"best span\")\n",
    "\n",
    "    return best_span\n",
    "\n",
    "  nlp = spacy.load(\"en_core_web_sm\")\n",
    "  # Read training file\n",
    "  with open(input_file, \"r\", encoding='utf-8') as reader:\n",
    "        input_text_file = json.load(reader)[\"data\"]\n",
    "  print(len(input_text_file))\n",
    "  samples = []\n",
    "  input_text_file = input_text_file \n",
    "############################################################\n",
    "############################################################\n",
    "############################################################\n",
    "  if isTrain:\n",
    "    data_len =  1#len(input_text_file) # Restricted training data due to hardware limitations\n",
    "  else:\n",
    "    data_len =  1#len(input_text_file) # Entire Development Data is loaded \n",
    "  number_yes = 0\n",
    "  number_no = 0\n",
    "  number_unknown = 0\n",
    "  number_span = 0 \n",
    "  number_nonespan = 0 \n",
    "  # Fetch and store story, questions and answers after processing the text\n",
    "  for data_index in tqdm(range(data_len), desc='Generating examples'):\n",
    "    input_data = input_text_file[data_index]\n",
    "    context_string = input_data['story']\n",
    "    input_data_object = {\n",
    "        'context': context_string,\n",
    "        'source': input_data['source'],\n",
    "        'id': input_data['id'],\n",
    "        'filename': input_data['filename']\n",
    "    }\n",
    "     \n",
    "     \n",
    "    nlp_context = nlp(pre_processing(context_string)) \n",
    "     \n",
    "    input_data_object['annotated_context'] = processing(nlp_context)\n",
    "     \n",
    "    input_data_object['raw_context_offsets'] = context_offsets(\n",
    "          input_data_object['annotated_context']['word'], context_string)\n",
    "      \n",
    "    assert len(input_data['questions']) == len(input_data['answers'])\n",
    "    additional_answers = {} \n",
    "    if 'additional_answers' in input_data:\n",
    "      for k, answer in input_data['additional_answers'].items():\n",
    "        if len(answer) == len(input_data['answers']):\n",
    "          for example in answer:\n",
    "            index = example['turn_id']\n",
    "            if index not in additional_answers:\n",
    "              additional_answers[index] = []\n",
    "            additional_answers[index].append(example['input_text'])\n",
    "    \n",
    "    \n",
    "    for i in range(len(input_data['questions'])):\n",
    "      question, answer = input_data['questions'][i], input_data['answers'][i]\n",
    "      assert question['turn_id'] == answer['turn_id']\n",
    "       \n",
    "\n",
    "      index = question['turn_id']\n",
    "      _qas = {\n",
    "          'turn_id': index,\n",
    "          'question': question['input_text'],\n",
    "          'answer': answer['input_text']\n",
    "      }\n",
    "       \n",
    "      if index in additional_answers:\n",
    "        _qas['additional_answers'] = additional_answers[index]\n",
    "      _qas['raw_answer'] = answer['input_text']\n",
    "      \n",
    "      if _qas['raw_answer'].lower() in ['yes', 'yes.']:\n",
    "        _qas['raw_answer'] = 'yes'\n",
    "        number_yes =  number_yes+1\n",
    "      if _qas['raw_answer'].lower() in ['no', 'no.']:\n",
    "        \n",
    "        _qas['raw_answer'] = 'no'\n",
    "        number_no =number_no +1\n",
    "      if _qas['raw_answer'].lower() in ['unknown', 'unknown.']:\n",
    "        \n",
    "        _qas['raw_answer'] = 'unknown'  \n",
    "        number_unknown = number_unknown+1     \n",
    "      _qas['answer_span_start'] = answer['span_start']\n",
    "      _qas['answer_span_end'] = answer['span_end']\n",
    "       \n",
    "      start = answer['span_start']\n",
    "      end = answer['span_end']\n",
    "      chosen_text = input_data_object['context'][start:end].lower()\n",
    "      \n",
    "      while len(chosen_text) > 0 and check_whitespace(chosen_text[0]):\n",
    "        chosen_text = chosen_text[1:]\n",
    "        start += 1\n",
    "       \n",
    "      while len(chosen_text) > 0 and check_whitespace(chosen_text[-1]):\n",
    "        chosen_text = chosen_text[:-1]\n",
    "        end -= 1\n",
    "        \n",
    "      \n",
    "      input_text = _qas['answer'].strip().lower() \n",
    "      if input_text in chosen_text:\n",
    "        #print(input_text, \"=====\",chosen_text)\n",
    "        input_text = _qas['answer'].strip().lower() \n",
    "        number_span = number_span+1\n",
    "        p = chosen_text.find(input_text)\n",
    "         \n",
    "        #get the answer span from raw text when the answer can be extracted exactly\n",
    "        _qas['answer_span'] = define_span_indices(input_data_object['raw_context_offsets'],\n",
    "                                                start + p,\n",
    "                                                start + p + len(input_text))\n",
    "        #print(input_data_object['raw_context_offsets'])\n",
    "        #_qas['raw_long_question'] = question['input_text']\n",
    "###########################here remove the else to every question)\n",
    "      else:\n",
    "#########################################################################\n",
    "        input_text = answer['span_text'].strip().lower()\n",
    "        number_nonespan =number_nonespan +1 \n",
    "#########################################################################\n",
    "         \n",
    "        _qas['answer_span'] = span_with_ground_truth(\n",
    "                      input_data_object['context'], input_data_object['raw_context_offsets'],\n",
    "                      input_text)\n",
    "        #look back maximumly 2 questions \n",
    "     \n",
    "      long_question = ''\n",
    "      for j in range(i - history_len, i + 1):\n",
    "           if j < 0:\n",
    "             continue\n",
    "           long_question += (' <Q> ' if add_QA_tag else\n",
    "                                  ' ') + input_data['questions'][j]['input_text']\n",
    "           \n",
    "           if j < i:\n",
    "                    long_question += (' <A> ' if add_QA_tag else\n",
    "                                      ' ') + input_data['answers'][j]['input_text']\n",
    "           \n",
    "           long_question = long_question.strip()\n",
    "############################################################################################################################################################################           \n",
    "           #add question history \n",
    "           #print(long_question)\n",
    "           _qas['raw_long_question'] = long_question\n",
    "############################################################################################################################################################################\n",
    "           \n",
    "           _qas['annotated_long_question'] = processing(\n",
    "                nlp(pre_processing(long_question)))\n",
    "           #offset is the letter position of the word\n",
    "   # Store questions along with their answers     \n",
    "      sample = QA(\n",
    "                question_answer_id =input_data_object['id'] + ' ' + str(_qas['turn_id']),\n",
    "                question_text =_qas['raw_long_question'],\n",
    "                document_tokens =input_data_object['annotated_context']['word'],\n",
    "                original_answer_text =_qas['raw_answer'],\n",
    "                answer_start_position =_qas['answer_span'][0],\n",
    "                answer_end_position =_qas['answer_span'][1],\n",
    "                additional_answers=_qas['additional_answers'] if 'additional_answers' in _qas else None,\n",
    "                )\n",
    "      \n",
    "      samples.append(sample)\n",
    "  print(\"yes\",number_yes,\"no\", number_no, \"unknown\",number_unknown,\"span\",number_span, number_nonespan)\n",
    "      #print(sample.question_answer_id, sample.question_text, sample.original_answer_text,  sample.answer_start_position, sample.answer_end_position, sample.additional_answers )#, sample.question_text,sample.original_answer_text)\n",
    "  return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nouT83wYIUyQ"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Read CoQA training file, Needs to provide the training file path here\n",
    "training_samples = get_data_from_coqa(True, input_file=\"data/coqa-train-v1.0.json\", history_len= 2, add_QA_tag= False)\n",
    "#print(training_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DImDVoDhIkK_"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Function to improve the answer by modifying the start and end indexes to appropriate answer\n",
    "def modify_answer_span(document_tokens, input_start_index, input_end_index, tokenizer, original_answer_text):\n",
    "  #print(document_tokens, input_start_index, input_end_index, tokenizer, original_answer_text)\n",
    "  token_answer_text = \" \".join(bert_tokenizer.tokenize(original_answer_text))\n",
    "  \n",
    "  # loop to modify the indexes for appropriate answer\n",
    "  for new_start_index in range(input_start_index, input_end_index + 1):\n",
    "    for new_end_index in range(input_end_index, new_start_index - 1, -1):\n",
    "      text_span = \" \".join(document_tokens[new_start_index:(new_end_index + 1)])\n",
    "      if text_span == token_answer_text:\n",
    "        return (new_start_index, new_end_index)\n",
    "  \n",
    " \n",
    "  return (input_start_index, input_end_index)\n",
    "\n",
    "# Function to select the span with the maximum context for the token\n",
    "def check_max_context(document_spans, current_span_index, current_position):\n",
    "   \n",
    "  best_score = None\n",
    "  best_span_index = None\n",
    "  for (span_index, document_span) in enumerate(document_spans):\n",
    "    end = document_span.start + document_span.length - 1\n",
    "    if current_position < document_span.start:\n",
    "            continue\n",
    "    if current_position > end:\n",
    "            continue\n",
    "    left_context = current_position - document_span.start\n",
    "    right_context = end - current_position\n",
    "    score = min(left_context, right_context) + 0.01 * document_span.length # Score calculation for the current start and end index\n",
    "    if best_score is None or score > best_score:  # selection of indexes based on better score\n",
    "      best_score = score\n",
    "      best_span_index = span_index\n",
    "  \n",
    "  return current_span_index == best_span_index\n",
    "# Function to convert CoQA Data to features\n",
    "def converting_examples_into_features(examples, tokenizer, maximum_sequence_length, document_stride, maximum_query_length):\n",
    "   \n",
    "  unique_id = 1000000000\n",
    "  features = []\n",
    "  for (example_index, example) in enumerate(tqdm(examples, desc=\"Generating features for CoQA...\")):\n",
    "    query_token = tokenizer.tokenize(example.question_text)\n",
    "     \n",
    "    class_index = 3\n",
    "\n",
    "    # Check for the type of answer whether it is yes/no type otherwise set to unknown\n",
    "    if example.original_answer_text == 'yes':\n",
    "      class_index = 0  \n",
    "    elif example.original_answer_text == 'no':\n",
    "      class_index = 1 \n",
    "    elif example.original_answer_text == 'unknown':\n",
    "      class_index = 2  \n",
    "  \n",
    "    # Check for the length of the query and select the query until uth maximum query length set\n",
    "    if len(query_token) > maximum_query_length:\n",
    "      query_token = query_token[0:maximum_query_length]\n",
    "    \n",
    "    token_to_original_index = []\n",
    "    original_to_token_index = []\n",
    "    all_document_tokens = []\n",
    "\n",
    "    for (i, token) in enumerate(example.document_tokens):\n",
    "      original_to_token_index.append(len(all_document_tokens))\n",
    "      sub_tokens = tokenizer.tokenize(token)\n",
    "      for sub_token in sub_tokens:\n",
    "        token_to_original_index.append(i)\n",
    "        all_document_tokens.append(sub_token)\n",
    "     \n",
    "    token_start_position = None\n",
    "    token_end_position = None\n",
    "    if class_index < 3:\n",
    "      token_start_position, token_end_position = 0,0\n",
    "    else:\n",
    "      token_start_position = original_to_token_index[example.answer_start_position]\n",
    "      if example.answer_end_position < len(example.document_tokens) - 1:\n",
    "        token_end_position = original_to_token_index[example.answer_end_position + 1] - 1\n",
    "      else:\n",
    "        token_end_position = len(all_document_tokens) - 1\n",
    "      (token_start_position, token_end_position) = modify_answer_span(\n",
    "                all_document_tokens, token_start_position, token_end_position, tokenizer,\n",
    "                example.original_answer_text)\n",
    "    \n",
    "     \n",
    "    maximum_tokens_for_document = maximum_sequence_length - len(query_token) - 3\n",
    "     \n",
    "    _DocSpan = collections.namedtuple(  \n",
    "            \"DocSpan\", [\"start\", \"length\"])\n",
    " ######################################################################need to figure out this##################################    \n",
    "    document_spans = []\n",
    "    start_offset = 0\n",
    "    while start_offset < len(all_document_tokens):\n",
    "      length = len(all_document_tokens) - start_offset\n",
    "      if length > maximum_tokens_for_document:\n",
    "        length = maximum_tokens_for_document\n",
    "      document_spans.append(_DocSpan(start=start_offset, length=length))\n",
    "      if start_offset + length == len(all_document_tokens):\n",
    "        break\n",
    "      start_offset += min(length, document_stride)\n",
    "      \n",
    "    \n",
    "    # loop to add the seperator tokens in the input sequence\n",
    "    for (document_span_index, document_span) in enumerate(document_spans):   \n",
    "      slice_class_index = class_index\n",
    "      tokens = []\n",
    "      token_to_origin_mapping = {}\n",
    "      token_max_context = {}\n",
    "      segment_ids = []\n",
    "      tokens.append(\"[CLS]\")\n",
    "      segment_ids.append(0)\n",
    "      \n",
    "      for token in query_token:\n",
    "          \n",
    "          tokens.append(token)\n",
    "          segment_ids.append(0)\n",
    "      #this is for single query examples\n",
    "      tokens.append(\"[SEP]\")\n",
    "\n",
    "      segment_ids.append(0)\n",
    "       \n",
    "      for i in range(document_span.length):\n",
    "        split_token_index = document_span.start + i\n",
    "        token_to_origin_mapping[len(\n",
    "                    tokens)] = token_to_original_index[split_token_index]\n",
    "        is_max_context = check_max_context(document_spans,\n",
    "                                                       document_span_index,\n",
    "                                                       split_token_index)\n",
    "        token_max_context[len(tokens)] = is_max_context\n",
    "        tokens.append(all_document_tokens[split_token_index])\n",
    "        segment_ids.append(1)\n",
    "      #this merges the entire paragraph\n",
    "      tokens.append(\"[SEP]\")\n",
    "      segment_ids.append(1)\n",
    "       \n",
    "      input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "      \n",
    "      input_mask = [1] * len(input_ids)\n",
    "       \n",
    "\n",
    "      while len(input_ids) < maximum_sequence_length:\n",
    "        input_ids.append(0)\n",
    "        input_mask.append(0)\n",
    "        segment_ids.append(0)\n",
    "      #print(input_ids, input_mask,segment_ids )\n",
    "      assert len(input_ids) == maximum_sequence_length\n",
    "      assert len(input_mask) == maximum_sequence_length\n",
    "      assert len(segment_ids) == maximum_sequence_length\n",
    "\n",
    "      # Start and end position calculations\n",
    "      start_position = None\n",
    "      end_position = None\n",
    "      if class_index >= 3:\n",
    "        document_start = document_span.start\n",
    "        document_end = document_span.start + document_span.length - 1\n",
    "         \n",
    "        out_of_span = False\n",
    "  \n",
    "      \n",
    "        if not (token_start_position >= document_start\n",
    "                        and token_end_position <= document_end):\n",
    "           \n",
    "          out_of_span = True\n",
    "        if out_of_span:\n",
    "          start_position = 0\n",
    "          end_position = 0\n",
    "          slice_class_index = 2\n",
    "        else: #why use document offest\n",
    "          document_offset = len(query_token) + 2\n",
    "          start_position = token_start_position - document_start + document_offset\n",
    "          end_position = token_end_position - document_start + document_offset\n",
    "          \n",
    "      else:\n",
    "        start_position = 0\n",
    "        end_position = 0\n",
    "      \n",
    "      # add the current feature calculated to the features list   \n",
    "      features.append(                            \n",
    "          DataFeatures(unique_id=unique_id, #record id\n",
    "                        example_index=example_index, #question id\n",
    "                        document_span_index=document_span_index,#document id\n",
    "                        tokens=tokens,#tokens for the entire questions and context texts\n",
    "                        token_to_origin_mapping=token_to_origin_mapping, #mapping the sequence with original index\n",
    "                        token_max_context=token_max_context,#whether exceed the maximum length\n",
    "                        input_ids=input_ids,#the tokenizer word embedding\n",
    "                        input_mask=input_mask,#the mask for all tokens, with word is 1, the rest padded with 0, 450\n",
    "                        segments=segment_ids,#segments of historical question and answer 0, context 1 \n",
    "                        start_position=start_position,#start position of the answer span\n",
    "                        end_position=end_position,#end position of the answer span\n",
    "                        class_index=slice_class_index)) #whether yes, no, unknow, or usual\n",
    "     \n",
    "      unique_id += 1\n",
    "       \n",
    "  return features  # Return all the features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lYCklFhiIrpv"
   },
   "outputs": [],
   "source": [
    "\n",
    "training_data_features = converting_examples_into_features(\n",
    "                examples=training_samples,\n",
    "                tokenizer=bert_tokenizer,\n",
    "                maximum_sequence_length=450,\n",
    "                document_stride=128,\n",
    "                maximum_query_length=75,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yKSK94-iItqo"
   },
   "outputs": [],
   "source": [
    "# Tensor construction for input ids\n",
    "dataset_input_ids = torch.tensor([f.input_ids for f in training_data_features], dtype=torch.long)\n",
    "print(dataset_input_ids)\n",
    "\n",
    "# Tensor construction for input masks\n",
    "dataset_input_masks = torch.tensor([f.input_mask for f in training_data_features], dtype=torch.long)\n",
    "print(dataset_input_masks)\n",
    "\n",
    "# Tensor construction for segment ids\n",
    "dataset_segment_ids = torch.tensor([f.segments for f in training_data_features], dtype=torch.long)\n",
    "print(dataset_segment_ids)\n",
    "\n",
    "#Tensor construction for start positions\n",
    "dataset_start_positions = torch.tensor([f.start_position for f in training_data_features], dtype=torch.long)\n",
    "print(dataset_start_positions)\n",
    "\n",
    "#Tensor construction for end positions\n",
    "dataset_end_positions = torch.tensor([f.end_position for f in training_data_features], dtype=torch.long)\n",
    "print(dataset_end_positions)\n",
    "\n",
    "dataset_class_index = torch.tensor([f.class_index for f in training_data_features], dtype=torch.long)\n",
    "\n",
    "# Wrapping tensors in a tensor dataset\n",
    "training_data = TensorDataset(dataset_input_ids, dataset_input_masks, dataset_segment_ids, dataset_start_positions, dataset_end_positions,dataset_class_index)\n",
    "print(training_data)\n",
    "\n",
    "# Train sampler to return random indices\n",
    "training_data_sampler = RandomSampler(training_data)\n",
    "print(training_data_sampler)\n",
    "\n",
    "# Creating python iterable over tensor datasets\n",
    "training_dataloader = DataLoader(training_data, sampler=training_data_sampler, batch_size=2) # has to be little else the server will have runtime out error\n",
    "print(training_dataloader)\n",
    "from math import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 555,
     "referenced_widgets": [
      "c6b3dd693f894d9d924513f08e21896a",
      "9c5748f0f33e4c818518a3f06434ccb2",
      "fca2bde460324be8b25ae98999bc7715",
      "d2e7ce21328e4f45bebbff374ff415bd",
      "012bfb60b6354e998255a90ac1245da6",
      "4a6b6d231d3f4f29be2ecaee4317d1f0",
      "8271c524d80b4dd6b7778e759caabd1e",
      "0451796170b84a01ae7b4b2b4ce1bca5",
      "4253bdad9b134f3faf5878d234584fb9",
      "fcf54d9ca73a4120933721e2c138ac46",
      "93d1b88786314b8982d7b615a9e2865c",
      "eb2654efa0c2496391376984a7b5353f",
      "fa814dad9b8e410cab720286bfefef3b",
      "81f20aa1b080446ca846d09981ee2316",
      "9bf99f2552be4b5c95bd4d521cdb469c",
      "3f26a28ee2464eba8af648daecdd86a5"
     ]
    },
    "executionInfo": {
     "elapsed": 15650,
     "status": "error",
     "timestamp": 1619398436596,
     "user": {
      "displayName": "Ning Zhang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhRCnmT8mmairzuShAibDId7kDGmrCXjp8yIjrCOw=s64",
      "userId": "12536691088853738123"
     },
     "user_tz": -480
    },
    "id": "tim-F9Hqrsma",
    "outputId": "7738c0f2-b306-462a-96d9-bf6769e8342a"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6b3dd693f894d9d924513f08e21896a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=570.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4253bdad9b134f3faf5878d234584fb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing CoQAwithBert: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing CoQAwithBert from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CoQAwithBert from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CoQAwithBert were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias', 'class_outputs.weight', 'class_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-4531c5864e26>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     86\u001b[0m  \u001b[0;31m# Set model in training mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;31m#original\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    671\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m     def register_backward_hook(\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    385\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    385\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    385\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    407\u001b[0m                 \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m                     \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m                 \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    669\u001b[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001b[1;32m    670\u001b[0m                             non_blocking, memory_format=convert_to_format)\n\u001b[0;32m--> 671\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    672\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import BertModel,BertConfig    \n",
    "from transformers import BertModel, AdamW, BertConfig\n",
    "BertPreTrainedModel = transformers.BertPreTrainedModel \n",
    "# Model creation using Bert Pretrained Model as a base\n",
    "class CoQAwithBert(BertPreTrainedModel):\n",
    "\n",
    "  # Configurations passed to BERT model\n",
    "  def __init__(\n",
    "            self,\n",
    "            config,\n",
    "            output_attentions=False,\n",
    "            keep_multihead_output=False,\n",
    "            class_alpha=1.0,\n",
    "            mask_p=0.0,\n",
    "    ):\n",
    "    super(CoQAwithBert, self).__init__(config)\n",
    "    self.class_alpha = class_alpha\n",
    "    self.mask_p = mask_p\n",
    "    self.bert = BertModel(\n",
    "    config,\n",
    "    )\n",
    "    self.qa_outputs = nn.Linear(config.hidden_size, 2)\n",
    "    self.output_attentions = False\n",
    "    self.class_outputs = nn.Linear(config.hidden_size, 4)\n",
    "    model_config = BertConfig.from_pretrained('bert-base-uncased', output_hidden_states=True)\n",
    "    self.bert = BertModel.from_pretrained('bert-base-uncased', config=model_config)\n",
    "    #self.apply(self.init_bert_weights)\n",
    "    #self.apply(self.init_bert_weights)\n",
    "  \n",
    "  # Forward pass for the BERT model\n",
    "  def forward(\n",
    "            self,\n",
    "            input_ids,  # Input seq indices \n",
    "            token_type_ids=None,\n",
    "            attention_mask=None, # Masking to avoid attention\n",
    "            start_positions=None, # Starting position of the span\n",
    "            end_positions=None,  # End position of the span\n",
    "            class_index = None,\n",
    "    ):\n",
    "    outputs = self.bert(\n",
    "            input_ids,\n",
    "            token_type_ids=token_type_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            #head_mask=head_mask,\n",
    "        )\n",
    "     \n",
    "    # outputs consists of the elements based on the configurations provided to BERT\n",
    "    sequence_output= outputs[0]\n",
    "    class_outputs = outputs[1] \n",
    "    span_logits = self.qa_outputs(sequence_output)\n",
    "    \n",
    "    class_logits = self.class_outputs(class_outputs)\n",
    "    \n",
    "    start_logits, end_logits = span_logits.split(1, dim=-1)\n",
    "    start_logits = start_logits.squeeze(-1)\n",
    "    end_logits = end_logits.squeeze(-1)\n",
    "    #print(class_logits.shape)\n",
    "\n",
    "    # Span extraction based on start positions and end positions\n",
    "    if start_positions is not None and end_positions is not None:\n",
    "      if len(start_positions.size()) > 1:\n",
    "        start_positions = start_positions.squeeze(-1)\n",
    "      if len(end_positions.size()) > 1:\n",
    "        end_positions = end_positions.squeeze(-1)\n",
    "      ignored_index = start_logits.size(1)\n",
    "       \n",
    "      start_positions.clamp_(0, ignored_index)\n",
    "      end_positions.clamp_(0, ignored_index)\n",
    "      #print(start_logits.shape, start_positions)\n",
    "      #print(end_logits, end_positions)\n",
    "      span_loss_factor = CrossEntropyLoss(ignore_index=ignored_index)\n",
    "      class_loss_factor = CrossEntropyLoss()\n",
    "      #here need to have the argmax for start_logits\n",
    "      #this loss is still based on the text span, there is no text generation component, need to see more how others do this piece, the model is not quite right\n",
    "      #data preprocessing done, but there is no ground truth answer therefore can't train, refer to other models, or otherwise this is still using squad method to do coqa\n",
    "      start_loss = span_loss_factor(start_logits, start_positions)\n",
    "      end_loss = span_loss_factor(end_logits, end_positions)\n",
    "      class_loss = class_loss_factor(class_logits, class_index)\n",
    "      total_loss = (start_loss + end_loss) / 2 + self.class_alpha * class_loss\n",
    "      return total_loss\n",
    "    return start_logits, end_logits, class_logits\n",
    "model = CoQAwithBert.from_pretrained('bert-base-uncased', output_hidden_states=True)\n",
    " # Set model in training mode\n",
    "device=\"cuda\"\n",
    "model.to(device)\n",
    "#original "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 8872743,
     "status": "ok",
     "timestamp": 1619343512982,
     "user": {
      "displayName": "Ning Zhang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhRCnmT8mmairzuShAibDId7kDGmrCXjp8yIjrCOw=s64",
      "userId": "12536691088853738123"
     },
     "user_tz": -480
    },
    "id": "x9frxf9Lrmik",
    "outputId": "4f0df42e-57e6-4082-f4fe-65986ebebfc6"
   },
   "outputs": [],
   "source": [
    "# Assigning names to the elements of tuples\n",
    "RawResult = collections.namedtuple(\"RawResult\",\n",
    "                                   [\"unique_id\", \"start_logits\", \"end_logits\", \"cls_logits\"])\n",
    "\n",
    "# Calculation for optimization steps accordingly training length, which will be used for learning rate for BERT model\n",
    "train_optimization_steps = len(\n",
    "            training_dataloader\n",
    "        )\n",
    "\n",
    "# Fetch the training hyperparameters\n",
    "parameter_optimizer = list(model.named_parameters())\n",
    "parameter_optimizer = [n for n in parameter_optimizer if 'pooler' not in n[0]]\n",
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in parameter_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in parameter_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}]\n",
    "\n",
    "# Load apex optimizers\n",
    "optimizer = AdamW(optimizer_grouped_parameters,\n",
    "                                  lr=5e-5)\n",
    "#optimizer = FP16_Optimizer(optimizer, dynamic_loss_scale=True)\n",
    "\n",
    "\n",
    "n_gpu = torch.cuda.device_count()\n",
    "# Accuracy and loss for plotting\n",
    "plot_data = []\n",
    "for epoch in trange(1, desc=\"Epoch\"): # Epoch provided: 3, restricted for now to avoid the memory issue but can be increased to improve the model\n",
    "  for step, batch in enumerate(\n",
    "          tqdm(training_dataloader,\n",
    "                desc=\"Iteration\",\n",
    "                disable=-1 not in [-1, 0])):\n",
    "    if n_gpu == 1: # check for gpu count \n",
    "          batch = tuple(\n",
    "              t.to(device)\n",
    "              for t in batch)\n",
    "    # Get the batch data to be provided to the model\n",
    "    dataset_input_ids,dataset_input_masks,dataset_segment_ids, dataset_start_positions, dataset_end_positions, dataset_class_index = batch\n",
    "    loss = model(dataset_input_ids, dataset_segment_ids, dataset_input_masks,\n",
    "                    dataset_start_positions, dataset_end_positions, dataset_class_index)\n",
    "     # Adding the loss to plot data to plot the graph in the end \n",
    "    loss.backward()\n",
    "    plot_data.append(loss.detach().item())\n",
    "    print(loss.detach().item())\n",
    "    if step < 2000:\n",
    "          lr_this_step = np.linspace(0, 5e-5, 2001)[1:]\n",
    "          for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr_this_step[step]\n",
    "    else:\n",
    "          param_group['lr']= (1+ cos(step*pi/len(training_dataloader)))*1/2*(5e-5)\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    #global_step += 1\n",
    "torch.save(model, 'outputs/bert_test1.pth')\n",
    "import matplotlib.pyplot as pplot\n",
    "\n",
    "# Graph plotting code X axis is Batch and Y axis is loss\n",
    "pplot.figure(figsize= (15, 10))\n",
    "pplot.title(\"Training Loss\")\n",
    "pplot.xlabel(\"Batch\")\n",
    "pplot.ylabel(\"Loss\")\n",
    "pplot.plot(plot_data)\n",
    "pplot.savefig('outputs/bert_test1.png', dpi=300, bbox_inches='tight')\n",
    "pplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sFKOempeI64w"
   },
   "outputs": [],
   "source": [
    "output_directory = 'outputs'\n",
    "\n",
    "# If output directory doesn't exist create one\n",
    "if not os.path.exists(output_directory):\n",
    "  os.makedirs(output_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1464266,
     "status": "ok",
     "timestamp": 1619344864860,
     "user": {
      "displayName": "Ning Zhang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhRCnmT8mmairzuShAibDId7kDGmrCXjp8yIjrCOw=s64",
      "userId": "12536691088853738123"
     },
     "user_tz": -480
    },
    "id": "lrF9uAXK0-4n",
    "outputId": "722d99c4-0260-4f0e-c0dd-7dd5ba8d11b6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Generating examples:   0%|          | 0/500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating examples: 100%|██████████| 500/500 [24:21<00:00,  2.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes 871 no 742 unknown 67 span 4971 3012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Read COQA Dev file, File path needs to be provided where COQA dev file is stored\n",
    "testing_samples = get_data_from_coqa(False, input_file=\"data/coqa-dev-v1.0.json\",\n",
    "                                                history_len= 2,\n",
    "                                                add_QA_tag= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1634856,
     "status": "ok",
     "timestamp": 1619345035466,
     "user": {
      "displayName": "Ning Zhang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhRCnmT8mmairzuShAibDId7kDGmrCXjp8yIjrCOw=s64",
      "userId": "12536691088853738123"
     },
     "user_tz": -480
    },
    "id": "G0R8XuNK2x07",
    "outputId": "c6b9de04-10f5-478c-89df-c8756ec697b2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating features for CoQA...: 100%|██████████| 7983/7983 [02:50<00:00, 46.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.DataFeatures object at 0x7f4a33727110>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Converting the development examples to features\n",
    "testing_features = converting_examples_into_features(\n",
    "                examples=testing_samples,\n",
    "                tokenizer=bert_tokenizer,\n",
    "                maximum_sequence_length=450,\n",
    "                document_stride=128,\n",
    "                maximum_query_length=75,\n",
    "            )\n",
    "print(testing_features[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1635997,
     "status": "ok",
     "timestamp": 1619345036622,
     "user": {
      "displayName": "Ning Zhang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhRCnmT8mmairzuShAibDId7kDGmrCXjp8yIjrCOw=s64",
      "userId": "12536691088853738123"
     },
     "user_tz": -480
    },
    "id": "GNiuz1PW4Iy7",
    "outputId": "e6a160c7-7404-4919-c863-1216a3d5d7ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 101, 2054, 3609,  ...,    0,    0,    0],\n",
      "        [ 101, 2054, 3609,  ...,    0,    0,    0],\n",
      "        [ 101, 2054, 3609,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 101, 2073, 2515,  ...,    0,    0,    0],\n",
      "        [ 101, 2029, 2110,  ...,    0,    0,    0],\n",
      "        [ 101, 2003, 2009,  ...,    0,    0,    0]])\n",
      "tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])\n",
      "<torch.utils.data.dataset.TensorDataset object at 0x7f4a33995610>\n",
      "<torch.utils.data.sampler.SequentialSampler object at 0x7f4a3157ff10>\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x7f4a339b6bd0>\n"
     ]
    }
   ],
   "source": [
    "#Tensor construction for input ids\n",
    "dataset_input_ids = torch.tensor([f.input_ids for f in testing_features], dtype=torch.long)\n",
    "print(dataset_input_ids)\n",
    "\n",
    "#Tensor construction for input masks\n",
    "dataset_input_masks = torch.tensor([f.input_mask for f in testing_features], dtype=torch.long)\n",
    "print(dataset_input_masks)\n",
    "\n",
    "#Tensor construction for segment ids\n",
    "dataset_segment_ids = torch.tensor([f.segments for f in testing_features], dtype=torch.long)\n",
    "print(dataset_segment_ids)\n",
    "\n",
    "dataset_example_index  = torch.arange(dataset_input_ids.size(0), dtype=torch.long)\n",
    "\n",
    "# Wrapping tensors in a tensor dataset\n",
    "testing_data = TensorDataset(dataset_input_ids, dataset_input_masks, dataset_segment_ids, dataset_example_index)\n",
    "print(testing_data)\n",
    "\n",
    "# Sampling the elements in the same order they are(sequentially)\n",
    "testing_data_sampler = SequentialSampler(testing_data)\n",
    "print(testing_data_sampler)\n",
    "\n",
    "# Creating python iterable over tensor dataset\n",
    "testing_dataloader = DataLoader(testing_data, sampler=testing_data_sampler, batch_size=8)\n",
    "print(testing_dataloader) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 172692,
     "status": "ok",
     "timestamp": 1619353305092,
     "user": {
      "displayName": "Ning Zhang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhRCnmT8mmairzuShAibDId7kDGmrCXjp8yIjrCOw=s64",
      "userId": "12536691088853738123"
     },
     "user_tz": -480
    },
    "id": "w_cHAj-88Mue",
    "outputId": "38b34b34-0cb0-470a-9231-453e8577e588"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████| 1157/1157 [02:51<00:00,  6.74it/s]\n"
     ]
    }
   ],
   "source": [
    "# Fetch the results for predictions using the trained model\n",
    "results_for_predictions = []\n",
    "for tqdm_input_ids, tqdm_input_mask, tqdm_segment_ids, tqdm_example_indices in tqdm(\n",
    "                testing_dataloader,\n",
    "                desc=\"Evaluation\",\n",
    "                disable=-1 not in [-1, 0]):\n",
    "   \n",
    "  tqdm_input_ids = tqdm_input_ids.to(device)\n",
    "  tqdm_input_mask = tqdm_input_mask.to(device)\n",
    "  tqdm_segment_ids = tqdm_segment_ids.to(device)\n",
    "\n",
    "  # Get all the results from the model\n",
    "  # Ensemble the results from Albert and BERT\n",
    "  with torch.no_grad():\n",
    "    model_batch_start_logits, model_batch_end_logits, model_batch_cls_logits = model(tqdm_input_ids, tqdm_segment_ids, tqdm_input_mask)  \n",
    "    #model_batch_start_logits, model_batch_end_logits, model_batch_cls_logits = bert_i(tqdm_input_ids, tqdm_segment_ids, tqdm_input_mask)  \n",
    "    #model_batch_start_logits, model_batch_end_logits, model_batch_cls_logits = albert(tqdm_input_ids, tqdm_segment_ids, tqdm_input_mask)  \n",
    "    #model_batch_start_logits, model_batch_end_logits, model_batch_cls_logits = roberta(tqdm_input_ids, tqdm_segment_ids, tqdm_input_mask)    \n",
    "   # robertamodel_batch_start_logits, robertamodel_batch_end_logits, robertamodel_batch_cls_logits = roberta(tqdm_input_ids, tqdm_segment_ids, tqdm_input_mask)\n",
    "   # bertmodel_batch_start_logits, bertmodel_batch_end_logits, bertmodel_batch_cls_logits = bert(tqdm_input_ids, tqdm_segment_ids, tqdm_input_mask)\n",
    "   # albertmodel_batch_start_logits, albertmodel_batch_end_logits, albertmodel_batch_cls_logits = albert(tqdm_input_ids, tqdm_segment_ids, tqdm_input_mask)\n",
    "  #model_batch_start_logits = (bertmodel_batch_start_logits +albertmodel_batch_start_logits)/2\n",
    "  #model_batch_end_logits = (bertmodel_batch_end_logits+albertmodel_batch_end_logits)/2 \n",
    "  #model_batch_cls_logits =(bertmodel_batch_cls_logits+ albertmodel_batch_cls_logits)/2\n",
    " \n",
    "   \n",
    "  # Get the start end logists from the model and store it in results\n",
    "  for i, tqdm_example_index in enumerate(tqdm_example_indices):\n",
    "    this_start_logits = model_batch_start_logits[i].detach().cpu().tolist()\n",
    "    this_end_logits = model_batch_end_logits[i].detach().cpu().tolist()\n",
    "    this_cls_logits = model_batch_cls_logits[i].detach().cpu().tolist()\n",
    "    testing_feature = testing_features[tqdm_example_index.item()]\n",
    "    unique_id = int(testing_feature.unique_id)\n",
    " \n",
    "    # Store the prediction in the results list\n",
    "    results_for_predictions.append(\n",
    "                    RawResult(unique_id=unique_id,\n",
    "                    start_logits= this_start_logits,\n",
    "                    end_logits= this_end_logits,\n",
    "                    cls_logits= this_cls_logits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wZ99u1kI-oNZ"
   },
   "outputs": [],
   "source": [
    "op_pred_file = os.path.join(output_directory, \"Output_Preds_bert.json\")\n",
    "output_nbest_file = os.path.join(output_directory, \"nbest_predictions_bert.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a5-x49P1AHnb"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Get the appropriate index for the answer text\n",
    "def compute_best_indices(logits, n):\n",
    "  index_with_score = sorted(enumerate(logits),\n",
    "                             key=lambda x: x[1],\n",
    "                             reverse=True)\n",
    "   \n",
    "  best_indices = []\n",
    "  for i in range(len(index_with_score)):\n",
    "    if i >= n:\n",
    "      break\n",
    "    best_indices.append(index_with_score[i][0])\n",
    "  return best_indices\n",
    "# Get the final text fetching it from the span using the predicted answer\n",
    "def output_final_answer_text(predicted_text, original_text, low_case, v_log=False):\n",
    "  def remove_spaces(text):\n",
    "    non_space_chars = []\n",
    "    non_space_char_to_space_char_map = collections.OrderedDict()\n",
    "    for (i, c) in enumerate(text):\n",
    "      if c == \" \":\n",
    "        continue\n",
    "      non_space_char_to_space_char_map[len(non_space_chars)] = i\n",
    "      non_space_chars.append(c)\n",
    "    non_space_text = \"\".join(non_space_chars)\n",
    "    return (non_space_text, non_space_char_to_space_char_map)\n",
    "\n",
    "  # Get the BERT tokenizer to tokenize the text\n",
    "  tokenizer = BasicTokenizer(do_lower_case=low_case)\n",
    "  tokenized_text = \" \".join(tokenizer.tokenize(original_text))\n",
    "\n",
    "  # Find predicted text in the tokenized text to get the start and end positions\n",
    "  start_position = tokenized_text.find(predicted_text)\n",
    "  if start_position == -1:\n",
    "      return original_text\n",
    "  end_position = start_position + len(predicted_text) - 1\n",
    "\n",
    "  # Remove spaces if any\n",
    "  (original_non_space_text, original_non_space_to_space_map) = remove_spaces(original_text)\n",
    "  (tokenized_non_space_text, tokenized_non_space_to_space_map) = remove_spaces(tokenized_text)\n",
    "\n",
    "  if len(original_non_space_text) != len(tokenized_non_space_text):\n",
    "    return original_text\n",
    "\n",
    "  tokenized_non_space_to_space_map = {}\n",
    "  for (i, _tokenized_index) in tokenized_non_space_to_space_map.items():\n",
    "    tokenized_non_space_to_space_map[_tokenized_index] = i\n",
    "\n",
    "  # Get the start position\n",
    "  original_start_position = None\n",
    "  if start_position in tokenized_non_space_to_space_map:\n",
    "    non_space_start_position = tokenized_non_space_to_space_map[start_position]\n",
    "    if non_space_start_position in original_non_space_to_space_map:\n",
    "      original_start_position = original_non_space_to_space_map[non_space_start_position]\n",
    "  \n",
    "  # Check if the start position is None\n",
    "  if original_start_position is None:\n",
    "    return original_text\n",
    "\n",
    "  # Get the End position\n",
    "  original_text_end_position = None\n",
    "  if end_position in tokenized_non_space_to_space_map:\n",
    "    non_space_end_position = tokenized_non_space_to_space_map[end_position]\n",
    "    if non_space_end_position in original_non_space_to_space_map:\n",
    "      original_text_end_position = original_non_space_to_space_map[non_space_end_position]\n",
    "\n",
    "  # Check if the end position is None\n",
    "  if original_text_end_position is None:\n",
    "    return original_text\n",
    "  \n",
    "  # Get the answer using start position and end position in the text\n",
    "  final_output_text = original_text[original_start_position:(original_text_end_position + 1)]\n",
    "  return final_output_text\n",
    "# Calculates the probability of the span found\n",
    "def compute_softmax_score(scores):\n",
    "  if not scores:\n",
    "    return []\n",
    "\n",
    "  maximum_softmax_score = None\n",
    "  for score in scores:\n",
    "    if maximum_softmax_score is None or score > maximum_softmax_score:\n",
    "      maximum_softmax_score = score\n",
    "\n",
    "  expected_scores = []\n",
    "  total_score_sum = 0.0\n",
    "  for score in scores:\n",
    "    x = math.exp(score - maximum_softmax_score)\n",
    "    expected_scores.append(x)\n",
    "    total_score_sum += x\n",
    "\n",
    "  probabilities = []\n",
    "  for score in expected_scores:\n",
    "    probabilities.append(score / total_score_sum) # Probability calculation using the softmax score\n",
    "  return probabilities\n",
    "\n",
    "# Removing punctuations, lowering texts and removing extra white spaces\n",
    "def normalize_answer1(s):\n",
    "    \n",
    "    # Remove articles from the text\n",
    "    def remove_articles(text):\n",
    "        regex = re.compile(r'\\b(a|an|the)\\b', re.UNICODE)\n",
    "        return re.sub(regex, ' ', text)\n",
    "    \n",
    "    # Remove white spaces from the text\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    # Remove punctuations from the text\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    # Lower the text characters\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "# Check if the predictions are numbers or boolean values then set those to string equivalent\n",
    "def confirm_predictions(json_best_predictions):\n",
    "  # Number strings that will be used to represent numbers in the answers instead of actual numbers\n",
    "  subs = ['one', 'two', 'three','four','five','six','seven','eight','nine','ten','eleven','twelve','true','false']\n",
    "  original = json_best_predictions[0]['text']\n",
    "  if len(original) < 2:\n",
    "    for e in json_best_predictions[1:]:\n",
    "      if normalize_answer1(e['text']) in subs:\n",
    "        return e['text']\n",
    "    return 'unknown'\n",
    "  return original\n",
    "# Function to Predict answers and write those predictions to predictions file \n",
    "def predict_answers(test_samples, test_sample_features, results_for_predictions, best_size,\n",
    "                  maximum_answer_length, low_case, op_pred_file, v_log,\n",
    "                  null_score_threshold):\n",
    "  \n",
    "  ex_index_to_feat_index = collections.defaultdict(list)\n",
    "  \n",
    "  # Create the dictionary of all the features in test features keeping feature index as key\n",
    "  for feature in test_sample_features:\n",
    "    ex_index_to_feat_index[feature.example_index].append(feature)\n",
    " \n",
    "  ids_for_results = {}\n",
    "  for result in results_for_predictions:\n",
    "    ids_for_results[result.unique_id] = result\n",
    "   \n",
    "  # Naming the tuples for predictions\n",
    "  Preliminary_Predictions = collections.namedtuple(\n",
    "      \"Preliminary_Predictions\", [\n",
    "                           \"feature_index\",\n",
    "                           \"start_index\",\n",
    "                           \"end_index\",\n",
    "                           \"start_logit\",\n",
    "                           \"end_logit\",\n",
    "                           \"class_logit\",\n",
    "                           \"class_index\",\n",
    "      ])\n",
    "  \n",
    "  complete_predictions = []\n",
    "  best_n_predictions_json = collections.OrderedDict() #in this case best 30 predictions\n",
    "  prediction_scores_json = collections.OrderedDict()\n",
    "  \n",
    "  for (example_index, example) in enumerate(\n",
    "      tqdm(test_samples, desc=\"Predicting...\")):\n",
    "    features = ex_index_to_feat_index[example_index]\n",
    "     \n",
    "    preliminary_predictions = []\n",
    "\n",
    "    part_preliminary_predictions = []\n",
    "\n",
    "    # Indices initialization\n",
    "    score_of_answer_yes, score_of_answer_no, score_span, score_of_no_answer = -float('INF'), -float('INF'), -float('INF'), float('INF')\n",
    "\n",
    "    \n",
    "    minimum_no_answer_feature_index, maximum_yes_feature_index, maximum_no_feature_index, maximum_span_feature_index = 0, 0, 0, 0\n",
    "    max_span_start_indexes, max_span_end_indexes = [], []\n",
    "     \n",
    "    # get the best start and end indices\n",
    "    for (feature_index, feature) in enumerate(features):\n",
    "       \n",
    "      result = ids_for_results[feature.unique_id]\n",
    "      # check the score for each class and determine the output yes, no, or span, or unknown \n",
    "      feature_yes_score, feature_no_score, feature_noanswer_score, feature_span_score = result.cls_logits \n",
    "       \n",
    "      if feature_noanswer_score < score_of_no_answer:\n",
    "        score_of_no_answer = feature_noanswer_score\n",
    "        minimum_no_answer_feature_index = feature_index\n",
    "      if feature_yes_score > score_of_answer_yes:\n",
    "        score_of_answer_yes = feature_yes_score\n",
    "        maximum_yes_feature_index = feature_index\n",
    "      if feature_no_score > score_of_answer_no:\n",
    "        score_of_answer_no = feature_no_score\n",
    "        maximum_no_feature_index = feature_index\n",
    "      # Here hasn't assign the correct class yet\n",
    "      if feature_span_score > score_span:\n",
    "        score_span = feature_span_score\n",
    "        maximum_span_feature_index = feature_index\n",
    "        start_indices = compute_best_indices(result.start_logits,\n",
    "                                                  best_size)\n",
    "        end_indices = compute_best_indices(result.end_logits, best_size)\n",
    "        maximum_span_start_indices, maximum_span_end_indices = start_indices, end_indices\n",
    "      \n",
    "         \n",
    "    \n",
    "    preliminary_predictions.append(\n",
    "        Preliminary_Predictions(feature_index=minimum_no_answer_feature_index,\n",
    "                                start_index=0,\n",
    "                                end_index=0,\n",
    "                                start_logit=-float('INF'),\n",
    "                                end_logit=-float('INF'),\n",
    "                                class_logit=score_of_no_answer,\n",
    "                                class_index=2))\n",
    "    preliminary_predictions.append(\n",
    "              Preliminary_Predictions(feature_index=maximum_yes_feature_index,\n",
    "                                start_index=0,\n",
    "                                end_index=0,\n",
    "                                start_logit=-float('INF'),\n",
    "                                end_logit=-float('INF'),\n",
    "                                class_logit=score_of_answer_yes,\n",
    "                                class_index=0))\n",
    "    preliminary_predictions.append(\n",
    "              Preliminary_Predictions(feature_index=maximum_no_feature_index,\n",
    "                                start_index=0,\n",
    "                                end_index=0,\n",
    "                                start_logit=-float('INF'),\n",
    "                                end_logit=-float('INF'),\n",
    "                                class_logit=score_of_answer_no,\n",
    "                                class_index=1))\n",
    "    preliminary_predictions.append(\n",
    "              Preliminary_Predictions(feature_index=maximum_span_feature_index,\n",
    "                                start_index=0,\n",
    "                                end_index=0,\n",
    "                                start_logit=-float('INF'),\n",
    "                                end_logit=-float('INF'),\n",
    "                                class_logit=score_span,\n",
    "                                class_index=3))\n",
    "   \n",
    "    feature = features[maximum_span_feature_index]\n",
    "    for start_index in maximum_span_start_indices:\n",
    "      for end_index in maximum_span_end_indices:\n",
    "        \n",
    "        if start_index >= len(feature.tokens):\n",
    "          continue\n",
    "        if end_index >= len(feature.tokens):\n",
    "          continue\n",
    "        if start_index not in feature.token_to_origin_mapping:\n",
    "          continue\n",
    "        if end_index not in feature.token_to_origin_mapping:\n",
    "          continue\n",
    "        if not feature.token_max_context.get(start_index, False):\n",
    "          continue\n",
    "        if end_index < start_index:\n",
    "          continue\n",
    "        length = end_index - start_index + 1\n",
    "        if length > maximum_answer_length:\n",
    "          continue\n",
    "        \n",
    "        part_preliminary_predictions.append(\n",
    "                      Preliminary_Predictions(\n",
    "                          feature_index=maximum_span_feature_index,\n",
    "                          start_index=start_index,\n",
    "                          end_index=end_index,\n",
    "                          start_logit=ids_for_results[\n",
    "                              feature.unique_id].start_logits[start_index],\n",
    "                          end_logit=ids_for_results[\n",
    "                              feature.unique_id].end_logits[end_index],\n",
    "                          class_logit=score_span,\n",
    "                          class_index=3))\n",
    "    ##this is to sort the largest score value for start and end pair    \n",
    "    part_preliminary_predictions = sorted(\n",
    "              part_preliminary_predictions,\n",
    "              key=lambda p: p.start_logit + p.end_logit,\n",
    "              reverse=True)\n",
    "    ##this is to sort the largest score value for class \n",
    "    preliminary_predictions = sorted(preliminary_predictions,\n",
    "                                      key=lambda p: p.class_logit,\n",
    "                                      reverse=True)\n",
    "    \n",
    "    Best_Predictions = collections.namedtuple(  \n",
    "              \"Best_Predictions\",\n",
    "              [\"text\", \"start_logit\", \"end_logit\", \"class_logit\", \"class_index\"])\n",
    "    \n",
    "    known_predictions = {}\n",
    "    best = []\n",
    "    class_rank = []\n",
    "    for prediction in part_preliminary_predictions:\n",
    "      if len(best) >= best_size:\n",
    "        break\n",
    "      feature = features[prediction.feature_index]\n",
    "      if prediction.class_index == 3:\n",
    "        tokenized_tokens = feature.tokens[prediction.start_index:(prediction.end_index + 1)]\n",
    "        original_document_start = feature.token_to_origin_mapping[prediction.start_index]\n",
    "        original_document_end = feature.token_to_origin_mapping[prediction.end_index]\n",
    "        original_tokens = example.document_tokens[original_document_start:(original_document_end + 1)]\n",
    "        \n",
    "        tokenized_text = \" \".join(tokenized_tokens)\n",
    "\n",
    "        tokenized_text = tokenized_text.replace(\" ##\", \"\")\n",
    "        tokenized_text = tokenized_text.replace(\"##\", \"\")\n",
    "\n",
    "        tokenized_text = tokenized_text.strip()\n",
    "        tokenized_text = \" \".join(tokenized_text.split())\n",
    "        original_text = \" \".join(original_tokens)\n",
    "\n",
    "        final_output_text = output_final_answer_text(tokenized_text, original_text, low_case, v_log)\n",
    "         \n",
    "        if final_output_text in known_predictions:\n",
    "          continue\n",
    "\n",
    "        known_predictions[final_output_text] = True\n",
    "        best.append(\n",
    "                      Best_Predictions(text=final_output_text,\n",
    "                                      start_logit=prediction.start_logit,\n",
    "                                      end_logit=prediction.end_logit,\n",
    "                                      class_logit=prediction.class_logit,\n",
    "                                      class_index=prediction.class_index))\n",
    "    \n",
    "    # Writing the approriate answers in predictions which will be written to json file\n",
    "    if not best or len(best) < 1: \n",
    "      best.append(\n",
    "                  Best_Predictions(text=\"unknown\",\n",
    "                                  start_logit=-float('INF'),\n",
    "                                  end_logit=-float('INF'),\n",
    "                                  class_logit=score_span,\n",
    "                                  class_index=3))\n",
    "    for prediction in preliminary_predictions:\n",
    "      if prediction.class_index == 3:\n",
    "        final_output_text = best[0].text\n",
    "        class_rank.append(\n",
    "                      Best_Predictions(text=final_output_text,\n",
    "                                      start_logit=best[0].start_logit,\n",
    "                                      end_logit=best[0].end_logit,\n",
    "                                      class_logit=prediction.class_logit,\n",
    "                                      class_index=prediction.class_index))\n",
    "      elif prediction.class_index == 0:\n",
    "        final_output_text = \"yes\"\n",
    "        class_rank.append(\n",
    "                      Best_Predictions(text=final_output_text,\n",
    "                                      start_logit=-float('INF'),\n",
    "                                      end_logit=-float('INF'),\n",
    "                                      class_logit=prediction.class_logit,\n",
    "                                      class_index=prediction.class_index))\n",
    "      elif prediction.class_index == 1:\n",
    "                  final_output_text = \"no\"\n",
    "                  class_rank.append(\n",
    "                      Best_Predictions(text=final_output_text,\n",
    "                                      start_logit=-float('INF'),\n",
    "                                      end_logit=-float('INF'),\n",
    "                                      class_logit=prediction.class_logit,\n",
    "                                      class_index=prediction.class_index))\n",
    "      elif prediction.class_index == 2:\n",
    "                  final_output_text = \"unknown\"\n",
    "                  class_rank.append(\n",
    "                      Best_Predictions(text=final_output_text,\n",
    "                                      start_logit=-float('INF'),\n",
    "                                      end_logit=-float('INF'),\n",
    "                                      class_logit=prediction.class_logit,\n",
    "                                      class_index=prediction.class_index))\n",
    "                  \n",
    "    assert len(best) >= 1\n",
    "\n",
    "    total_scores = []\n",
    "    class_scores = []\n",
    "    for item in best:\n",
    "      total_scores.append(item.start_logit + item.end_logit)\n",
    "    for rank in class_rank:\n",
    "      class_scores.append(rank.class_logit)\n",
    "    \n",
    "    # calculate Softmax\n",
    "    span_probabilities = compute_softmax_score(total_scores)\n",
    "    class_probabilities = compute_softmax_score(class_scores)\n",
    "    best_predictions_json = []\n",
    "\n",
    "    current_rank, current_probabilities, current_scores = (\n",
    "        best, span_probabilities,\n",
    "        total_scores) if class_rank[0].class_index == 3 and len(best) > 1 else (\n",
    "            class_rank, class_probabilities, class_scores)\n",
    "    \n",
    "    \n",
    "    # Store the answer text, probability and score for each entry\n",
    "    for i, entry in enumerate(current_rank): \n",
    "      predicted_outputs = collections.OrderedDict()\n",
    "      predicted_outputs[\"text\"] = entry.text\n",
    "      predicted_outputs[\"probability\"] = current_probabilities[i]\n",
    "      predicted_outputs[\"score\"] = current_scores[i]\n",
    "      best_predictions_json.append(predicted_outputs)\n",
    "\n",
    "    assert len(best_predictions_json) >= 1\n",
    "\n",
    "    _id, _turn_id = example.question_answer_id.split()\n",
    "    complete_predictions.append({'id': _id, 'turn_id': int(_turn_id), 'answer': confirm_predictions(best_predictions_json)})\n",
    "    best_n_predictions_json[example.question_answer_id] = best_predictions_json\n",
    "\n",
    "  # Write the prediction files\n",
    "  with open(op_pred_file, \"w\") as writer:\n",
    "        writer.write(json.dumps(complete_predictions, indent=4) + \"\\n\")\n",
    "  \n",
    "  with open(output_nbest_file, \"w\") as writer:\n",
    "        writer.write(json.dumps(best_n_predictions_json, indent=4) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 166371,
     "status": "ok",
     "timestamp": 1619353342522,
     "user": {
      "displayName": "Ning Zhang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhRCnmT8mmairzuShAibDId7kDGmrCXjp8yIjrCOw=s64",
      "userId": "12536691088853738123"
     },
     "user_tz": -480
    },
    "id": "HxhzcNKEAw1Z",
    "outputId": "96e9eedc-70d9-4cc3-8d14-5c2c2af07f5a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting...: 100%|██████████| 7983/7983 [00:35<00:00, 225.82it/s]\n"
     ]
    }
   ],
   "source": [
    "# Call to predict answers functions to write them to predictions file\n",
    "predict_answers(testing_samples, testing_features, results_for_predictions, 20, 30, True, \n",
    "                  op_pred_file, True, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4871,
     "status": "ok",
     "timestamp": 1619355289134,
     "user": {
      "displayName": "Ning Zhang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhRCnmT8mmairzuShAibDId7kDGmrCXjp8yIjrCOw=s64",
      "userId": "12536691088853738123"
     },
     "user_tz": -480
    },
    "id": "lvXM82xOAx-i",
    "outputId": "722d7edc-56b1-4f61-f093-3cdd8f5d41a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"children_stories\": {\n",
      "    \"em\": 54.3,\n",
      "    \"f1\": 66.3,\n",
      "    \"turns\": 1425\n",
      "  },\n",
      "  \"literature\": {\n",
      "    \"em\": 53.4,\n",
      "    \"f1\": 62.8,\n",
      "    \"turns\": 1630\n",
      "  },\n",
      "  \"mid-high_school\": {\n",
      "    \"em\": 51.2,\n",
      "    \"f1\": 62.0,\n",
      "    \"turns\": 1653\n",
      "  },\n",
      "  \"news\": {\n",
      "    \"em\": 56.9,\n",
      "    \"f1\": 68.5,\n",
      "    \"turns\": 1649\n",
      "  },\n",
      "  \"wikipedia\": {\n",
      "    \"em\": 59.8,\n",
      "    \"f1\": 71.1,\n",
      "    \"turns\": 1626\n",
      "  },\n",
      "  \"reddit\": {\n",
      "    \"em\": 0.0,\n",
      "    \"f1\": 0.0,\n",
      "    \"turns\": 0\n",
      "  },\n",
      "  \"science\": {\n",
      "    \"em\": 0.0,\n",
      "    \"f1\": 0.0,\n",
      "    \"turns\": 0\n",
      "  },\n",
      "  \"in_domain\": {\n",
      "    \"em\": 55.1,\n",
      "    \"f1\": 66.1,\n",
      "    \"turns\": 7983\n",
      "  },\n",
      "  \"out_domain\": {\n",
      "    \"em\": 0.0,\n",
      "    \"f1\": 0.0,\n",
      "    \"turns\": 0\n",
      "  },\n",
      "  \"overall\": {\n",
      "    \"em\": 55.1,\n",
      "    \"f1\": 66.1,\n",
      "    \"turns\": 7983\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!python3 evaluate.py --data-file data/coqa-dev-v1.0.json --pred-file outputs/Output_Preds_bert.json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xxuOCB3gA0Qi"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPb/sELkZr0/APSS54qFm2u",
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "StepbyStep_ensamble_training_mode.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "012bfb60b6354e998255a90ac1245da6": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "019435d2992c40de8caae0023063810a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0451796170b84a01ae7b4b2b4ce1bca5": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "18a5354ed50042d190f9b2d73eb89f38": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "196b03c9c76d4f7dbb12fb9c32e3e822": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1aae7470fdcf47029f2cc7230bee715f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "214b5b9fb2cd4ed0bf4ae55cb8b42ded": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "2341b7e5f1c4401d8523ed28c1b9d80e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_df3ce102e5ff4d748bb7c90a646d7bdb",
       "IPY_MODEL_38518ff413d84b70a7801f663278fab7"
      ],
      "layout": "IPY_MODEL_508e6ad29c0244968719ae15ac39638c"
     }
    },
    "27375f9e2a64429ba383a10781641063": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "2982d6361d2047be88020e8ea22525af": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "30adb987ff9745d1811bb0499b711353": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_495f10aa99534aa69b0522b270552746",
       "IPY_MODEL_8add70da1f8244d1821c39a041a5fc2c"
      ],
      "layout": "IPY_MODEL_f33d5a0c4f3946c0bb544ab2fd68a75f"
     }
    },
    "38518ff413d84b70a7801f663278fab7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_521d32fbd2ab434e86323b29668bd64e",
      "placeholder": "​",
      "style": "IPY_MODEL_c14439f2dfc64222a0cd44e4c1d1b7bb",
      "value": " 684/684 [00:01&lt;00:00, 383B/s]"
     }
    },
    "3a2ba4318e634c6cbff2fd05b5f119d2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4a39fe79f6304faaa2311f1709a5ec0a",
       "IPY_MODEL_b2a4ba8da6a34d30bb5c0c07ec4360eb"
      ],
      "layout": "IPY_MODEL_196b03c9c76d4f7dbb12fb9c32e3e822"
     }
    },
    "3f26a28ee2464eba8af648daecdd86a5": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4253bdad9b134f3faf5878d234584fb9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_93d1b88786314b8982d7b615a9e2865c",
       "IPY_MODEL_eb2654efa0c2496391376984a7b5353f"
      ],
      "layout": "IPY_MODEL_fcf54d9ca73a4120933721e2c138ac46"
     }
    },
    "495f10aa99534aa69b0522b270552746": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1aae7470fdcf47029f2cc7230bee715f",
      "max": 481,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_bd15258e8ddf4cc2b26280f2e403a842",
      "value": 481
     }
    },
    "4a39fe79f6304faaa2311f1709a5ec0a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_99c027a01ed847a0bd256becc58734bd",
      "max": 47376696,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_861c928ff9a74e0887cde5fb704f3d64",
      "value": 47376696
     }
    },
    "4a6b6d231d3f4f29be2ecaee4317d1f0": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "508e6ad29c0244968719ae15ac39638c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "521d32fbd2ab434e86323b29668bd64e": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "70d7ce2b754d4493a757c846376fb32b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f0d271e541c3451e8de97fd5ad8b59d3",
      "placeholder": "​",
      "style": "IPY_MODEL_019435d2992c40de8caae0023063810a",
      "value": " 501M/501M [00:09&lt;00:00, 55.1MB/s]"
     }
    },
    "81f20aa1b080446ca846d09981ee2316": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8271c524d80b4dd6b7778e759caabd1e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "861c928ff9a74e0887cde5fb704f3d64": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "8add70da1f8244d1821c39a041a5fc2c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d93a31a307cf4eb3857a4794b25b87c7",
      "placeholder": "​",
      "style": "IPY_MODEL_f31d157211f34853997c52c751c2e7c4",
      "value": " 481/481 [00:00&lt;00:00, 1.60kB/s]"
     }
    },
    "93d1b88786314b8982d7b615a9e2865c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_81f20aa1b080446ca846d09981ee2316",
      "max": 440473133,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_fa814dad9b8e410cab720286bfefef3b",
      "value": 440473133
     }
    },
    "99c027a01ed847a0bd256becc58734bd": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9bf99f2552be4b5c95bd4d521cdb469c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9c5748f0f33e4c818518a3f06434ccb2": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a43b7615992444049a481fd86279d294": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f7b0b88144464d98aa95de9fd9c79dfd",
       "IPY_MODEL_70d7ce2b754d4493a757c846376fb32b"
      ],
      "layout": "IPY_MODEL_de0a3ddda0174aa5be1c040a47af1218"
     }
    },
    "b2a4ba8da6a34d30bb5c0c07ec4360eb": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b44940443b2448ec9d12b381346ebc8c",
      "placeholder": "​",
      "style": "IPY_MODEL_dac4d69a93cc4dd599c2d3c4c61686e3",
      "value": " 47.4M/47.4M [00:00&lt;00:00, 53.4MB/s]"
     }
    },
    "b44940443b2448ec9d12b381346ebc8c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bd15258e8ddf4cc2b26280f2e403a842": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "c14439f2dfc64222a0cd44e4c1d1b7bb": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c6b3dd693f894d9d924513f08e21896a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_fca2bde460324be8b25ae98999bc7715",
       "IPY_MODEL_d2e7ce21328e4f45bebbff374ff415bd"
      ],
      "layout": "IPY_MODEL_9c5748f0f33e4c818518a3f06434ccb2"
     }
    },
    "d2e7ce21328e4f45bebbff374ff415bd": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0451796170b84a01ae7b4b2b4ce1bca5",
      "placeholder": "​",
      "style": "IPY_MODEL_8271c524d80b4dd6b7778e759caabd1e",
      "value": " 570/570 [00:01&lt;00:00, 432B/s]"
     }
    },
    "d93a31a307cf4eb3857a4794b25b87c7": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dac4d69a93cc4dd599c2d3c4c61686e3": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "de0a3ddda0174aa5be1c040a47af1218": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "df3ce102e5ff4d748bb7c90a646d7bdb": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2982d6361d2047be88020e8ea22525af",
      "max": 684,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_214b5b9fb2cd4ed0bf4ae55cb8b42ded",
      "value": 684
     }
    },
    "eb2654efa0c2496391376984a7b5353f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3f26a28ee2464eba8af648daecdd86a5",
      "placeholder": "​",
      "style": "IPY_MODEL_9bf99f2552be4b5c95bd4d521cdb469c",
      "value": " 440M/440M [00:11&lt;00:00, 39.5MB/s]"
     }
    },
    "f0d271e541c3451e8de97fd5ad8b59d3": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f31d157211f34853997c52c751c2e7c4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f33d5a0c4f3946c0bb544ab2fd68a75f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f7b0b88144464d98aa95de9fd9c79dfd": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_18a5354ed50042d190f9b2d73eb89f38",
      "max": 501200538,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_27375f9e2a64429ba383a10781641063",
      "value": 501200538
     }
    },
    "fa814dad9b8e410cab720286bfefef3b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "fca2bde460324be8b25ae98999bc7715": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4a6b6d231d3f4f29be2ecaee4317d1f0",
      "max": 570,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_012bfb60b6354e998255a90ac1245da6",
      "value": 570
     }
    },
    "fcf54d9ca73a4120933721e2c138ac46": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
