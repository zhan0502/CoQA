{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NB54koJNRUmB"
   },
   "source": [
    "https://huggingface.co/transformers/_modules/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20991,
     "status": "ok",
     "timestamp": 1619515676631,
     "user": {
      "displayName": "chao wang",
      "photoUrl": "",
      "userId": "16929231235898515362"
     },
     "user_tz": -480
    },
    "id": "MyFzK7mFiW-6",
    "outputId": "36909174-f689-4c89-91eb-9aca0e92fd51"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n",
      "/content/gdrive/My Drive/AI Sem II/NLP/Project Test/\n",
      "/content/gdrive/My Drive/AI Sem II/NLP/Project Test\n"
     ]
    }
   ],
   "source": [
    "# For Google Colaboratory\n",
    "\n",
    "import sys, os\n",
    "if 'google.colab' in sys.modules:\n",
    "    # mount google drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    " \n",
    "    path_to_file = '/content/gdrive/My Drive/AI Sem II/NLP/Project Test/'\n",
    "    print(path_to_file)\n",
    "    # change current path to the folder containing \"file_name\"\n",
    "    os.chdir(path_to_file)\n",
    "    !pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 34503,
     "status": "ok",
     "timestamp": 1619515708546,
     "user": {
      "displayName": "chao wang",
      "photoUrl": "",
      "userId": "16929231235898515362"
     },
     "user_tz": -480
    },
    "id": "2XfyPtO2jZlO",
    "outputId": "72a49c7c-2a80-4f9a-c953-33127d9bd284"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/huggingface/transformers\n",
      "  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-zi2eauo8\n",
      "  Running command git clone -q https://github.com/huggingface/transformers /tmp/pip-req-build-zi2eauo8\n",
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0.dev0) (4.41.1)\n",
      "Collecting huggingface-hub>=0.0.8\n",
      "  Downloading https://files.pythonhosted.org/packages/a1/88/7b1e45720ecf59c6c6737ff332f41c955963090a18e72acbcbeac6b25e86/huggingface_hub-0.0.8-py3-none-any.whl\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0.dev0) (20.9)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0.dev0) (1.19.5)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0.dev0) (2019.12.20)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0.dev0) (2.23.0)\n",
      "Collecting sacremoses\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
      "\u001b[K     |████████████████████████████████| 901kB 21.9MB/s \n",
      "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n",
      "\u001b[K     |████████████████████████████████| 3.3MB 65.3MB/s \n",
      "\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0.dev0) (3.10.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0.dev0) (3.0.12)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.6.0.dev0) (2.4.7)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.6.0.dev0) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.6.0.dev0) (2020.12.5)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.6.0.dev0) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.6.0.dev0) (3.0.4)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.6.0.dev0) (1.0.1)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.6.0.dev0) (7.1.2)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.6.0.dev0) (1.15.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.6.0.dev0) (3.7.4.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.6.0.dev0) (3.4.1)\n",
      "Building wheels for collected packages: transformers\n",
      "  Building wheel for transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for transformers: filename=transformers-4.6.0.dev0-cp37-none-any.whl size=2122782 sha256=5907b6e26dd4a009fad5dc78bf9ca22c5565b0f144192a4d5ccf1a0239064de2\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-9kj_0oet/wheels/70/d3/52/b3fa4f8b8ef04167ac62e5bb2accb62ae764db2a378247490e\n",
      "Successfully built transformers\n",
      "Installing collected packages: huggingface-hub, sacremoses, tokenizers, transformers\n",
      "Successfully installed huggingface-hub-0.0.8 sacremoses-0.0.45 tokenizers-0.10.2 transformers-4.6.0.dev0\n",
      "Collecting spacy\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1b/d8/0361bbaf7a1ff56b44dca04dace54c82d63dad7475b7d25ea1baefafafb2/spacy-3.0.6-cp37-cp37m-manylinux2014_x86_64.whl (12.8MB)\n",
      "\u001b[K     |████████████████████████████████| 12.8MB 13.5MB/s \n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.8.2)\n",
      "Requirement already satisfied, skipping upgrade: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.5)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.19.5)\n",
      "Requirement already satisfied, skipping upgrade: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.11.3)\n",
      "Collecting pathy>=0.3.5\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/13/87/5991d87be8ed60beb172b4062dbafef18b32fa559635a8e2b633c2974f85/pathy-0.5.2-py3-none-any.whl (42kB)\n",
      "\u001b[K     |████████████████████████████████| 51kB 8.4MB/s \n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.41.1)\n",
      "Collecting thinc<8.1.0,>=8.0.3\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/61/87/decceba68a0c6ca356ddcb6aea8b2500e71d9bc187f148aae19b747b7d3c/thinc-8.0.3-cp37-cp37m-manylinux2014_x86_64.whl (1.1MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1MB 73.7MB/s \n",
      "\u001b[?25hCollecting typer<0.4.0,>=0.3.0\n",
      "  Downloading https://files.pythonhosted.org/packages/90/34/d138832f6945432c638f32137e6c79a3b682f06a63c488dcfaca6b166c64/typer-0.3.2-py3-none-any.whl\n",
      "Requirement already satisfied, skipping upgrade: typing-extensions<4.0.0.0,>=3.7.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from spacy) (3.7.4.3)\n",
      "Requirement already satisfied, skipping upgrade: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.5)\n",
      "Requirement already satisfied, skipping upgrade: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied, skipping upgrade: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
      "Collecting catalogue<2.1.0,>=2.0.3\n",
      "  Downloading https://files.pythonhosted.org/packages/82/a5/b5021c74c04cac35a27d34cbf3146d86eb8e173b4491888bc4908c4c8b3b/catalogue-2.0.3-py3-none-any.whl\n",
      "Collecting srsly<3.0.0,>=2.4.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c3/84/dfdfc9f6f04f6b88207d96d9520b911e5fec0c67ff47a0dea31ab5429a1e/srsly-2.4.1-cp37-cp37m-manylinux2014_x86_64.whl (456kB)\n",
      "\u001b[K     |████████████████████████████████| 460kB 68.1MB/s \n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (56.0.0)\n",
      "Requirement already satisfied, skipping upgrade: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (20.9)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.4\n",
      "  Downloading https://files.pythonhosted.org/packages/8d/67/d4002a18e26bf29b17ab563ddb55232b445ab6a02f97bf17d1345ff34d3f/spacy_legacy-3.0.5-py2.py3-none-any.whl\n",
      "Collecting pydantic<1.8.0,>=1.7.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b3/0a/52ae1c659fc08f13dd7c0ae07b88e4f807ad83fb9954a59b0b0a3d1a8ab6/pydantic-1.7.3-cp37-cp37m-manylinux2014_x86_64.whl (9.1MB)\n",
      "\u001b[K     |████████████████████████████████| 9.1MB 73.2MB/s \n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy) (1.1.1)\n",
      "Collecting smart-open<4.0.0,>=2.2.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/11/9a/ba2d5f67f25e8d5bbf2fcec7a99b1e38428e83cb715f64dd179ca43a11bb/smart_open-3.0.0.tar.gz (113kB)\n",
      "\u001b[K     |████████████████████████████████| 122kB 53.4MB/s \n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.0->spacy) (7.1.2)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.12.5)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
      "Requirement already satisfied, skipping upgrade: zipp>=0.5; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.3->spacy) (3.4.1)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy) (2.4.7)\n",
      "Building wheels for collected packages: smart-open\n",
      "  Building wheel for smart-open (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for smart-open: filename=smart_open-3.0.0-cp37-none-any.whl size=107098 sha256=a07823492ae06a428370fcb83e8f3905f72f2a7b6abd854e19efef43f2becc8c\n",
      "  Stored in directory: /root/.cache/pip/wheels/18/88/7c/f06dabd5e9cabe02d2269167bcacbbf9b47d0c0ff7d6ebcb78\n",
      "Successfully built smart-open\n",
      "Installing collected packages: typer, smart-open, pathy, catalogue, srsly, pydantic, thinc, spacy-legacy, spacy\n",
      "  Found existing installation: smart-open 5.0.0\n",
      "    Uninstalling smart-open-5.0.0:\n",
      "      Successfully uninstalled smart-open-5.0.0\n",
      "  Found existing installation: catalogue 1.0.0\n",
      "    Uninstalling catalogue-1.0.0:\n",
      "      Successfully uninstalled catalogue-1.0.0\n",
      "  Found existing installation: srsly 1.0.5\n",
      "    Uninstalling srsly-1.0.5:\n",
      "      Successfully uninstalled srsly-1.0.5\n",
      "  Found existing installation: thinc 7.4.0\n",
      "    Uninstalling thinc-7.4.0:\n",
      "      Successfully uninstalled thinc-7.4.0\n",
      "  Found existing installation: spacy 2.2.4\n",
      "    Uninstalling spacy-2.2.4:\n",
      "      Successfully uninstalled spacy-2.2.4\n",
      "Successfully installed catalogue-2.0.3 pathy-0.5.2 pydantic-1.7.3 smart-open-3.0.0 spacy-3.0.6 spacy-legacy-3.0.5 srsly-2.4.1 thinc-8.0.3 typer-0.3.2\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/huggingface/transformers\n",
    "!pip install -U spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 43186,
     "status": "ok",
     "timestamp": 1619515719566,
     "user": {
      "displayName": "chao wang",
      "photoUrl": "",
      "userId": "16929231235898515362"
     },
     "user_tz": -480
    },
    "id": "W86G1qWTjwdc",
    "outputId": "38f9c126-b43f-41b0-c7af-af8ce816ea76"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;3m⚠ As of spaCy v3.0, shortcuts like 'en' are deprecated. Please use the\n",
      "full pipeline package name 'en_core_web_sm' instead.\u001b[0m\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import string\n",
    "import transformers\n",
    "import re\n",
    "from tqdm import tqdm, trange\n",
    "import spacy \n",
    "import spacy.cli\n",
    "import logging\n",
    "import collections\n",
    "from collections import Counter\n",
    "import math\n",
    "import torch\n",
    "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler, TensorDataset)\n",
    "from transformers import BertModel, BertTokenizer, AlbertTokenizer, AdamW\n",
    "#from pytorch_pretrained_bert.modeling import BertModel, BertPretrainedModel\n",
    "#from pytorch_pretrained_bert.tokenization import BertTokenizer\n",
    "#from pytorch_pretrained_bert.optimization import BertAdam, WarmupLinearSchedule\n",
    "#from pytorch_ptrtrained_bert.tokenization import BasicTokenizer, whitespace_tokenize\n",
    "from transformers.models.bert.tokenization_bert import BasicTokenizer\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import json\n",
    "spacy.cli.download('en')\n",
    "spacy.load('en_core_web_sm')\n",
    "import random\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import matplotlib as pplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 43148,
     "status": "ok",
     "timestamp": 1619515722170,
     "user": {
      "displayName": "chao wang",
      "photoUrl": "",
      "userId": "16929231235898515362"
     },
     "user_tz": -480
    },
    "id": "wT3ORk94Z5ZZ",
    "outputId": "d459e11b-b009-4036-9f04-3cd66869a5cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentencepiece\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
      "\r",
      "\u001b[K     |▎                               | 10kB 25.0MB/s eta 0:00:01\r",
      "\u001b[K     |▌                               | 20kB 33.9MB/s eta 0:00:01\r",
      "\u001b[K     |▉                               | 30kB 25.9MB/s eta 0:00:01\r",
      "\u001b[K     |█                               | 40kB 19.8MB/s eta 0:00:01\r",
      "\u001b[K     |█▍                              | 51kB 14.5MB/s eta 0:00:01\r",
      "\u001b[K     |█▋                              | 61kB 13.8MB/s eta 0:00:01\r",
      "\u001b[K     |██                              | 71kB 14.0MB/s eta 0:00:01\r",
      "\u001b[K     |██▏                             | 81kB 15.3MB/s eta 0:00:01\r",
      "\u001b[K     |██▌                             | 92kB 15.4MB/s eta 0:00:01\r",
      "\u001b[K     |██▊                             | 102kB 16.3MB/s eta 0:00:01\r",
      "\u001b[K     |███                             | 112kB 16.3MB/s eta 0:00:01\r",
      "\u001b[K     |███▎                            | 122kB 16.3MB/s eta 0:00:01\r",
      "\u001b[K     |███▌                            | 133kB 16.3MB/s eta 0:00:01\r",
      "\u001b[K     |███▉                            | 143kB 16.3MB/s eta 0:00:01\r",
      "\u001b[K     |████                            | 153kB 16.3MB/s eta 0:00:01\r",
      "\u001b[K     |████▍                           | 163kB 16.3MB/s eta 0:00:01\r",
      "\u001b[K     |████▋                           | 174kB 16.3MB/s eta 0:00:01\r",
      "\u001b[K     |█████                           | 184kB 16.3MB/s eta 0:00:01\r",
      "\u001b[K     |█████▏                          | 194kB 16.3MB/s eta 0:00:01\r",
      "\u001b[K     |█████▌                          | 204kB 16.3MB/s eta 0:00:01\r",
      "\u001b[K     |█████▊                          | 215kB 16.3MB/s eta 0:00:01\r",
      "\u001b[K     |██████                          | 225kB 16.3MB/s eta 0:00:01\r",
      "\u001b[K     |██████▎                         | 235kB 16.3MB/s eta 0:00:01\r",
      "\u001b[K     |██████▌                         | 245kB 16.3MB/s eta 0:00:01\r",
      "\u001b[K     |██████▉                         | 256kB 16.3MB/s eta 0:00:01\r",
      "\u001b[K     |███████                         | 266kB 16.3MB/s eta 0:00:01\r",
      "\u001b[K     |███████▍                        | 276kB 16.3MB/s eta 0:00:01\r",
      "\u001b[K     |███████▋                        | 286kB 16.3MB/s eta 0:00:01\r",
      "\u001b[K     |████████                        | 296kB 16.3MB/s eta 0:00:01\r",
      "\u001b[K     |████████▏                       | 307kB 16.3MB/s eta 0:00:01\r",
      "\u001b[K     |████████▍                       | 317kB 16.3MB/s eta 0:00:01\r",
      "\u001b[K     |████████▊                       | 327kB 16.3MB/s eta 0:00:01\r",
      "\u001b[K     |█████████                       | 337kB 16.3MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▎                      | 348kB 16.3MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▌                      | 358kB 16.3MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▉                      | 368kB 16.3MB/s eta 0:00:01\r",
      "\u001b[K     |██████████                      | 378kB 16.3MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▍                     | 389kB 16.3MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▋                     | 399kB 16.3MB/s eta 0:00:01\r",
      "\u001b[K     |███████████                     | 409kB 16.3MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▏                    | 419kB 16.3MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▍                    | 430kB 16.3MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▊                    | 440kB 16.3MB/s eta 0:00:01\r",
      "\u001b[K     |████████████                    | 450kB 16.3MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▎                   | 460kB 16.3MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▌                   | 471kB 16.3MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▉                   | 481kB 16.3MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████                   | 491kB 16.3MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▍                  | 501kB 16.3MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▋                  | 512kB 16.3MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▉                  | 522kB 16.3MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▏                 | 532kB 16.3MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▍                 | 542kB 16.3MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▊                 | 552kB 16.3MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████                 | 563kB 16.3MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▎                | 573kB 16.3MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▌                | 583kB 16.3MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▉                | 593kB 16.3MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████                | 604kB 16.3MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▍               | 614kB 16.3MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▋               | 624kB 16.3MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▉               | 634kB 16.3MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▏              | 645kB 16.3MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▍              | 655kB 16.3MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▊              | 665kB 16.3MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████              | 675kB 16.3MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▎             | 686kB 16.3MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▌             | 696kB 16.3MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▉             | 706kB 16.3MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████             | 716kB 16.3MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▎            | 727kB 16.3MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▋            | 737kB 16.3MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▉            | 747kB 16.3MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▏           | 757kB 16.3MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▍           | 768kB 16.3MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▊           | 778kB 16.3MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████           | 788kB 16.3MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▎          | 798kB 16.3MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▌          | 808kB 16.3MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▉          | 819kB 16.3MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████          | 829kB 16.3MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▎         | 839kB 16.3MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▋         | 849kB 16.3MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▉         | 860kB 16.3MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▏        | 870kB 16.3MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▍        | 880kB 16.3MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▊        | 890kB 16.3MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████        | 901kB 16.3MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▎       | 911kB 16.3MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▌       | 921kB 16.3MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▊       | 931kB 16.3MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████       | 942kB 16.3MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▎      | 952kB 16.3MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▋      | 962kB 16.3MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▉      | 972kB 16.3MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▏     | 983kB 16.3MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▍     | 993kB 16.3MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▊     | 1.0MB 16.3MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████     | 1.0MB 16.3MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▎    | 1.0MB 16.3MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▌    | 1.0MB 16.3MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▊    | 1.0MB 16.3MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████    | 1.1MB 16.3MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▎   | 1.1MB 16.3MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▋   | 1.1MB 16.3MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▉   | 1.1MB 16.3MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▏  | 1.1MB 16.3MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▍  | 1.1MB 16.3MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▊  | 1.1MB 16.3MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████  | 1.1MB 16.3MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▏ | 1.1MB 16.3MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▌ | 1.1MB 16.3MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▊ | 1.2MB 16.3MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████ | 1.2MB 16.3MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▎| 1.2MB 16.3MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▋| 1.2MB 16.3MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▉| 1.2MB 16.3MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 1.2MB 16.3MB/s \n",
      "\u001b[?25hInstalling collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.1.95\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 114
    },
    "executionInfo": {
     "elapsed": 44481,
     "status": "ok",
     "timestamp": 1619515725299,
     "user": {
      "displayName": "chao wang",
      "photoUrl": "",
      "userId": "16929231235898515362"
     },
     "user_tz": -480
    },
    "id": "FleUeEhpj01T",
    "outputId": "5f6d03b3-cbbe-4efb-beff-ae2b47be620c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "765ca20b15ff4ffe88f6e30039a31169",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=760289.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "963f4c82ba394743b4d0a35fe5b36582",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1312669.0, style=ProgressStyle(descript…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "#from transformers.models.roberta.tokenization_roberta import RobertaTokenizer\n",
    "#import sentencepiece\n",
    "#bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "from transformers.models.albert.tokenization_albert import AlbertTokenizer\n",
    "bert_tokenizer = AlbertTokenizer.from_pretrained('albert-base-v2', do_lower_case=True)\n",
    "#model =BertModel.from_pretrained('bert-base-uncased')\n",
    "output_directory = 'outputs'\n",
    "\n",
    "# If output directory doesn't exist create one\n",
    "if not os.path.exists(output_directory):\n",
    "  os.makedirs(output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d4WXOe-3j4SE"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#from transformers import BartModel\n",
    "#BartPreTrainedModel = transformers.PretrainedBartModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U6rFZIJNHlU3"
   },
   "outputs": [],
   "source": [
    "#Class to store questions, their answers along with the starting index of the answer and end index of answer and tokens in the story\n",
    "class QA(object):\n",
    "  def __init__(\n",
    "            self,\n",
    "            question_answer_id,\n",
    "            question_text,\n",
    "            document_tokens,\n",
    "            original_answer_text=None,\n",
    "            answer_start_position=None,\n",
    "            answer_end_position=None,\n",
    "            additional_answers=None,\n",
    "    ):\n",
    "    self.question_answer_id = question_answer_id\n",
    "    self.question_text = question_text\n",
    "    self.document_tokens = document_tokens\n",
    "    self.original_answer_text = original_answer_text\n",
    "    self.answer_start_position = answer_start_position\n",
    "    self.answer_end_position = answer_end_position\n",
    "    self.additional_answers = additional_answers\n",
    "\n",
    "# Class to store features, input ids, input mask, segment ids, start positions, end positions, etc\n",
    "class DataFeatures(object):\n",
    "  def __init__(self,\n",
    "                 unique_id,\n",
    "                 example_index,\n",
    "                 document_span_index,\n",
    "                 tokens,\n",
    "                 token_to_origin_mapping,\n",
    "                 token_max_context,\n",
    "                 input_ids,\n",
    "                 input_mask,\n",
    "                 segments,\n",
    "                 start_position=None,\n",
    "                 end_position=None,\n",
    "                 class_index=None):            \n",
    "    self.unique_id = unique_id\n",
    "    self.example_index = example_index\n",
    "    self.document_span_index = document_span_index\n",
    "    self.tokens = tokens\n",
    "    self.token_to_origin_mapping = token_to_origin_mapping\n",
    "    self.token_max_context = token_max_context\n",
    "    self.input_ids = input_ids\n",
    "    self.input_mask = input_mask\n",
    "    self.segments = segments\n",
    "    self.start_position = start_position\n",
    "    self.end_position = end_position\n",
    "    self.class_index = class_index\n",
    "\n",
    "\n",
    "# Function to read COQA datasets and process the data\n",
    "def get_data_from_coqa(isTrain, input_file, history_len=2, add_QA_tag=False):\n",
    "\n",
    "# Check if the character is a white space\n",
    "  def check_whitespace(char):\n",
    "        if char == \" \" or char == \"\\t\" or char == \"\\r\" or char == \"\\n\" or ord(char) == 0x202F:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "# Token conversion\n",
    "  def tokenize_string(str):\n",
    "        if (str.lower() == '-lrb-'):\n",
    "            str = '('\n",
    "        elif (str.lower() == '-rrb-'):\n",
    "            str = ')'\n",
    "        elif (str.lower() == '-lsb-'):\n",
    "            str = '['\n",
    "        elif (str.lower() == '-rsb-'):\n",
    "            str = ']'\n",
    "        elif (str.lower() == '-lcb-'):\n",
    "            str = '{'\n",
    "        elif (str.lower() == '-rcb-'):\n",
    "            str = '}'\n",
    "        return str\n",
    "\n",
    "  def space_extension(matchobject):\n",
    "    return ' ' + matchobject.group(0) + ' '  \n",
    "\n",
    " # Preprocessing \n",
    "  def pre_processing(word):\n",
    "    word = re.sub(\n",
    "        u'-|\\u2010|\\u2011|\\u2012|\\u2013|\\u2014|\\u2015|%|\\[|\\]|:|\\(|\\)|/|\\t',\n",
    "        space_extension, word)\n",
    "    word = word.strip(' \\n')\n",
    "    word = re.sub('\\s+', ' ', word)\n",
    "    return word\n",
    "\n",
    "  # Process text to return output as words with their indexes in the sentences \n",
    "  def processing(main_text):\n",
    "          processed_text = {'word': [], 'offsets': [], 'sentences': []}\n",
    " \n",
    "          for token in main_text:\n",
    "              processed_text['word'].append(tokenize_string(token.text))\n",
    "              processed_text['offsets'].append((token.idx, token.idx + len(token.text)))\n",
    "          #print(\"=======offset is\", (token.idx, token.idx + len(token.text)), \"=====word is\", tokenize_string(token.text))\n",
    "\n",
    "          word_index = 0\n",
    "          for sentence in main_text.sents:\n",
    "              processed_text['sentences'].append((word_index, word_index + len(sentence)))\n",
    "              word_index += len(sentence)\n",
    "          #print(\"=======sentence is\",  word_index, word_index + len(sentence) )\n",
    "\n",
    "          assert word_index == len(processed_text['word'])\n",
    "           \n",
    "          return processed_text\n",
    "\n",
    "  # Get the context offsets\n",
    "  def context_offsets(words, raw_text):\n",
    "    #print(words, raw_text)\n",
    "    raw_text_context_offsets = []\n",
    "    r = 0\n",
    "    for token in words:\n",
    "        while r < len(raw_text) and re.match('\\s', raw_text[r]):\n",
    "            r += 1\n",
    "        if raw_text[r:r + len(token)] != token:\n",
    "            print('Error', token, 'Raw Text:', raw_text)\n",
    "\n",
    "        raw_text_context_offsets.append((r, r + len(token)))\n",
    "        r += len(token)\n",
    " \n",
    "    return raw_text_context_offsets\n",
    "\n",
    "  # Function to find span with start and end index provided\n",
    "  def define_span_indices(offsets, start_pos, end_pos):\n",
    "     \n",
    "    span_start_index = -1\n",
    "    span_end_index = -1\n",
    "    #print(\"offset is\", offsets)\n",
    "    for i, offset in enumerate(offsets):\n",
    "        if (span_start_index < 0) or (start_pos >= offset[0]):\n",
    "            span_start_index = i\n",
    "        if (span_end_index < 0) and (end_pos <= offset[1]):\n",
    "            span_end_index = i\n",
    "    #print(\"span\",span_start_index, span_end_index)\n",
    "    return (span_start_index, span_end_index)\n",
    "\n",
    "# Removing punctuations, lowering texts and removing extra white spaces\n",
    "  def pre_process_answer(s):\n",
    "    \n",
    "    \n",
    "    # Remove articles from the text\n",
    "    def remove_articles(text):\n",
    "        reg_expression = re.compile(r'\\b(a|an|the)\\b', re.UNICODE)\n",
    "        return re.sub(reg_expression, ' ', text)\n",
    "    \n",
    "    # Remove white spaces from the text\n",
    "    def adjust_white_space(text):\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    # Remove punctuations from the text\n",
    "    def remove_punctuations(text):\n",
    "        rem = set(string.punctuation)\n",
    "        return ''.join(c for c in text if c not in rem)\n",
    "\n",
    "    # Lower the text characters\n",
    "    def lowering_text(text):\n",
    "        return text.lower()\n",
    "     \n",
    "\n",
    "    return adjust_white_space(remove_articles(remove_punctuations(lowering_text(s))))  \n",
    "\n",
    "# Find the span providing the context and offsets  \n",
    "  def span_with_ground_truth(context, offsets, ground_truth):\n",
    "\n",
    "    best_F1 = 0.0\n",
    "    best_span = (len(offsets) - 1, len(offsets) - 1)\n",
    "    ground_truth_temp = pre_process_answer(pre_processing(ground_truth)).split()\n",
    "\n",
    "    ls = [\n",
    "        i for i in range(len(offsets))\n",
    "        if context[offsets[i][0]:offsets[i][1]].lower() in ground_truth\n",
    "    ]\n",
    "\n",
    "    for i in range(len(ls)):\n",
    "        for j in range(i, len(ls)):\n",
    "            prediction = pre_process_answer(\n",
    "                pre_processing(\n",
    "                    context[offsets[ls[i]][0]:offsets[ls[j]][1]])).split()\n",
    "            common = Counter(prediction) & Counter(ground_truth_temp)\n",
    "            num_same = sum(common.values())\n",
    "\n",
    "            #print(num_same, \"common span\")\n",
    "            if num_same > 0:\n",
    "                precision = 1.0 * num_same / len(prediction)\n",
    "                recall = 1.0 * num_same / len(ground_truth_temp)\n",
    "                F1 = (2 * precision * recall) / (precision + recall)\n",
    "                if F1 > best_F1:\n",
    "                    best_F1 = F1\n",
    "                    best_span = (ls[i], ls[j])\n",
    "     \n",
    "    #print(best_span, \"best span\")\n",
    "\n",
    "    return best_span\n",
    "\n",
    "  nlp = spacy.load(\"en_core_web_sm\")\n",
    "  # Read training file\n",
    "  with open(input_file, \"r\", encoding='utf-8') as reader:\n",
    "        input_text_file = json.load(reader)[\"data\"]\n",
    "  print(len(input_text_file))\n",
    "  samples = []\n",
    "  input_text_file = input_text_file \n",
    "############################################################\n",
    "############################################################\n",
    "############################################################\n",
    "  if isTrain:\n",
    "    data_len =  len(input_text_file) # Restricted training data due to hardware limitations\n",
    "  else:\n",
    "    data_len =  len(input_text_file) # Entire Development Data is loaded \n",
    "  number_yes = 0\n",
    "  number_no = 0\n",
    "  number_unknown = 0\n",
    "  number_span = 0 \n",
    "  number_nonespan = 0 \n",
    "  # Fetch and store story, questions and answers after processing the text\n",
    "  for data_index in tqdm(range(data_len), desc='Generating examples'):\n",
    "    input_data = input_text_file[data_index]\n",
    "    context_string = input_data['story']\n",
    "    input_data_object = {\n",
    "        'context': context_string,\n",
    "        'source': input_data['source'],\n",
    "        'id': input_data['id'],\n",
    "        'filename': input_data['filename']\n",
    "    }\n",
    "     \n",
    "     \n",
    "    nlp_context = nlp(pre_processing(context_string)) \n",
    "     \n",
    "    input_data_object['annotated_context'] = processing(nlp_context)\n",
    "     \n",
    "    input_data_object['raw_context_offsets'] = context_offsets(\n",
    "          input_data_object['annotated_context']['word'], context_string)\n",
    "      \n",
    "    assert len(input_data['questions']) == len(input_data['answers'])\n",
    "    additional_answers = {} \n",
    "    if 'additional_answers' in input_data:\n",
    "      for k, answer in input_data['additional_answers'].items():\n",
    "        if len(answer) == len(input_data['answers']):\n",
    "          for example in answer:\n",
    "            index = example['turn_id']\n",
    "            if index not in additional_answers:\n",
    "              additional_answers[index] = []\n",
    "            additional_answers[index].append(example['input_text'])\n",
    "    \n",
    "    \n",
    "    for i in range(len(input_data['questions'])):\n",
    "      question, answer = input_data['questions'][i], input_data['answers'][i]\n",
    "      assert question['turn_id'] == answer['turn_id']\n",
    "       \n",
    "\n",
    "      index = question['turn_id']\n",
    "      _qas = {\n",
    "          'turn_id': index,\n",
    "          'question': question['input_text'],\n",
    "          'answer': answer['input_text']\n",
    "      }\n",
    "       \n",
    "      if index in additional_answers:\n",
    "        _qas['additional_answers'] = additional_answers[index]\n",
    "      _qas['raw_answer'] = answer['input_text']\n",
    "      \n",
    "      if _qas['raw_answer'].lower() in ['yes', 'yes.']:\n",
    "        _qas['raw_answer'] = 'yes'\n",
    "        number_yes =  number_yes+1\n",
    "      if _qas['raw_answer'].lower() in ['no', 'no.']:\n",
    "        \n",
    "        _qas['raw_answer'] = 'no'\n",
    "        number_no =number_no +1\n",
    "      if _qas['raw_answer'].lower() in ['unknown', 'unknown.']:\n",
    "        \n",
    "        _qas['raw_answer'] = 'unknown'  \n",
    "        number_unknown = number_unknown+1     \n",
    "      _qas['answer_span_start'] = answer['span_start']\n",
    "      _qas['answer_span_end'] = answer['span_end']\n",
    "       \n",
    "      start = answer['span_start']\n",
    "      end = answer['span_end']\n",
    "      chosen_text = input_data_object['context'][start:end].lower()\n",
    "      \n",
    "      while len(chosen_text) > 0 and check_whitespace(chosen_text[0]):\n",
    "        chosen_text = chosen_text[1:]\n",
    "        start += 1\n",
    "       \n",
    "      while len(chosen_text) > 0 and check_whitespace(chosen_text[-1]):\n",
    "        chosen_text = chosen_text[:-1]\n",
    "        end -= 1\n",
    "        \n",
    "      \n",
    "      input_text = _qas['answer'].strip().lower() \n",
    "      if input_text in chosen_text:\n",
    "        #print(input_text, \"=====\",chosen_text)\n",
    "        input_text = _qas['answer'].strip().lower() \n",
    "        number_span = number_span+1\n",
    "        p = chosen_text.find(input_text)\n",
    "         \n",
    "        #get the answer span from raw text when the answer can be extracted exactly\n",
    "        _qas['answer_span'] = define_span_indices(input_data_object['raw_context_offsets'],\n",
    "                                                start + p,\n",
    "                                                start + p + len(input_text))\n",
    "        #print(input_data_object['raw_context_offsets'])\n",
    "        #_qas['raw_long_question'] = question['input_text']\n",
    "###########################here remove the else to every question)\n",
    "      else:\n",
    "#########################################################################\n",
    "        input_text = answer['span_text'].strip().lower()\n",
    "        number_nonespan =number_nonespan +1 \n",
    "#########################################################################\n",
    "         \n",
    "        _qas['answer_span'] = span_with_ground_truth(\n",
    "                      input_data_object['context'], input_data_object['raw_context_offsets'],\n",
    "                      input_text)\n",
    "        #look back maximumly 2 questions \n",
    "     \n",
    "      long_question = ''\n",
    "      for j in range(i - history_len, i + 1):\n",
    "           if j < 0:\n",
    "             continue\n",
    "           long_question += (' <Q> ' if add_QA_tag else\n",
    "                                  ' ') + input_data['questions'][j]['input_text']\n",
    "           \n",
    "           if j < i:\n",
    "                    long_question += (' <A> ' if add_QA_tag else\n",
    "                                      ' ') + input_data['answers'][j]['input_text']\n",
    "           \n",
    "           long_question = long_question.strip()\n",
    "############################################################################################################################################################################           \n",
    "           #add question history \n",
    "           #print(long_question)\n",
    "           _qas['raw_long_question'] = long_question\n",
    "############################################################################################################################################################################\n",
    "           \n",
    "           _qas['annotated_long_question'] = processing(\n",
    "                nlp(pre_processing(long_question)))\n",
    "           #offset is the letter position of the word\n",
    "   # Store questions along with their answers     \n",
    "      sample = QA(\n",
    "                question_answer_id =input_data_object['id'] + ' ' + str(_qas['turn_id']),\n",
    "                question_text =_qas['raw_long_question'],\n",
    "                document_tokens =input_data_object['annotated_context']['word'],\n",
    "                original_answer_text =_qas['raw_answer'],\n",
    "                answer_start_position =_qas['answer_span'][0],\n",
    "                answer_end_position =_qas['answer_span'][1],\n",
    "                additional_answers=_qas['additional_answers'] if 'additional_answers' in _qas else None,\n",
    "                )\n",
    "      \n",
    "      samples.append(sample)\n",
    "  print(\"yes\",number_yes,\"no\", number_no, \"unknown\",number_unknown,\"span\",number_span, number_nonespan)\n",
    "      #print(sample.question_answer_id, sample.question_text, sample.original_answer_text,  sample.answer_start_position, sample.answer_end_position, sample.additional_answers )#, sample.question_text,sample.original_answer_text)\n",
    "  return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 43738,
     "status": "ok",
     "timestamp": 1619489707280,
     "user": {
      "displayName": "chao wang",
      "photoUrl": "",
      "userId": "16929231235898515362"
     },
     "user_tz": -480
    },
    "id": "nouT83wYIUyQ",
    "outputId": "b84f10cd-b6d0-41c4-935e-f202d1cb1df3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Generating examples:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating examples: 100%|██████████| 2/2 [00:03<00:00,  1.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes 0 no 0 unknown 1 span 16 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Read CoQA training file, Needs to provide the training file path here\n",
    "training_samples = get_data_from_coqa(True, input_file=\"data/coqa-train-v1.0.json\", history_len= 2, add_QA_tag= False)\n",
    "#print(training_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DImDVoDhIkK_"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Function to improve the answer by modifying the start and end indexes to appropriate answer\n",
    "def modify_answer_span(document_tokens, input_start_index, input_end_index, tokenizer, original_answer_text):\n",
    "  #print(document_tokens, input_start_index, input_end_index, tokenizer, original_answer_text)\n",
    "  token_answer_text = \" \".join(bert_tokenizer.tokenize(original_answer_text))\n",
    "  \n",
    "  # loop to modify the indexes for appropriate answer\n",
    "  for new_start_index in range(input_start_index, input_end_index + 1):\n",
    "    for new_end_index in range(input_end_index, new_start_index - 1, -1):\n",
    "      text_span = \" \".join(document_tokens[new_start_index:(new_end_index + 1)])\n",
    "      if text_span == token_answer_text:\n",
    "        return (new_start_index, new_end_index)\n",
    "  \n",
    " \n",
    "  return (input_start_index, input_end_index)\n",
    "\n",
    "# Function to select the span with the maximum context for the token\n",
    "def check_max_context(document_spans, current_span_index, current_position):\n",
    "   \n",
    "  best_score = None\n",
    "  best_span_index = None\n",
    "  for (span_index, document_span) in enumerate(document_spans):\n",
    "    end = document_span.start + document_span.length - 1\n",
    "    if current_position < document_span.start:\n",
    "            continue\n",
    "    if current_position > end:\n",
    "            continue\n",
    "    left_context = current_position - document_span.start\n",
    "    right_context = end - current_position\n",
    "    score = min(left_context, right_context) + 0.01 * document_span.length # Score calculation for the current start and end index\n",
    "    if best_score is None or score > best_score:  # selection of indexes based on better score\n",
    "      best_score = score\n",
    "      best_span_index = span_index\n",
    "  \n",
    "  return current_span_index == best_span_index\n",
    "# Function to convert CoQA Data to features\n",
    "def converting_examples_into_features(examples, tokenizer, maximum_sequence_length, document_stride, maximum_query_length):\n",
    "   \n",
    "  unique_id = 1000000000\n",
    "  features = []\n",
    "  for (example_index, example) in enumerate(tqdm(examples, desc=\"Generating features for CoQA...\")):\n",
    "     \n",
    "    query_token = tokenizer.tokenize(example.question_text)\n",
    "     \n",
    "    class_index = 3\n",
    "\n",
    "    # Check for the type of answer whether it is yes/no type otherwise set to unknown\n",
    "    if example.original_answer_text == 'yes':\n",
    "      class_index = 0  \n",
    "    elif example.original_answer_text == 'no':\n",
    "      class_index = 1 \n",
    "    elif example.original_answer_text == 'unknown':\n",
    "      class_index = 2  \n",
    "  \n",
    "    # Check for the length of the query and select the query until uth maximum query length set\n",
    "    if len(query_token) > maximum_query_length:\n",
    "      query_token = query_token[0:maximum_query_length]\n",
    "    \n",
    "    token_to_original_index = []\n",
    "    original_to_token_index = []\n",
    "    all_document_tokens = []\n",
    "\n",
    "    for (i, token) in enumerate(example.document_tokens):\n",
    "      original_to_token_index.append(len(all_document_tokens))\n",
    "      sub_tokens = tokenizer.tokenize(token)\n",
    "      for sub_token in sub_tokens:\n",
    "        token_to_original_index.append(i)\n",
    "        all_document_tokens.append(sub_token)\n",
    "     \n",
    "    token_start_position = None\n",
    "    token_end_position = None\n",
    "    if class_index < 3:\n",
    "      token_start_position, token_end_position = 0,0\n",
    "    else:\n",
    "      token_start_position = original_to_token_index[example.answer_start_position]\n",
    "      if example.answer_end_position < len(example.document_tokens) - 1:\n",
    "        token_end_position = original_to_token_index[example.answer_end_position + 1] - 1\n",
    "      else:\n",
    "        token_end_position = len(all_document_tokens) - 1\n",
    "      (token_start_position, token_end_position) = modify_answer_span(\n",
    "                all_document_tokens, token_start_position, token_end_position, tokenizer,\n",
    "                example.original_answer_text)\n",
    "    \n",
    "     \n",
    "    maximum_tokens_for_document = maximum_sequence_length - len(query_token) - 3\n",
    "     \n",
    "    _DocSpan = collections.namedtuple(  \n",
    "            \"DocSpan\", [\"start\", \"length\"])\n",
    " ######################################################################need to figure out this##################################    \n",
    "    document_spans = []\n",
    "    start_offset = 0\n",
    "    while start_offset < len(all_document_tokens):\n",
    "      length = len(all_document_tokens) - start_offset\n",
    "      if length > maximum_tokens_for_document:\n",
    "        length = maximum_tokens_for_document\n",
    "      document_spans.append(_DocSpan(start=start_offset, length=length))\n",
    "      if start_offset + length == len(all_document_tokens):\n",
    "        break\n",
    "      start_offset += min(length, document_stride)\n",
    "      \n",
    "    \n",
    "    # loop to add the seperator tokens in the input sequence\n",
    "    for (document_span_index, document_span) in enumerate(document_spans):   \n",
    "      slice_class_index = class_index\n",
    "      tokens = []\n",
    "      token_to_origin_mapping = {}\n",
    "      token_max_context = {}\n",
    "      segment_ids = []\n",
    "      tokens.append(\"[CLS]\")\n",
    "      segment_ids.append(0)\n",
    "      \n",
    "      for token in query_token:\n",
    "          \n",
    "          tokens.append(token)\n",
    "          segment_ids.append(0)\n",
    "      #this is for single query examples\n",
    "      tokens.append(\"[SEP]\")\n",
    "\n",
    "      segment_ids.append(0)\n",
    "       \n",
    "      for i in range(document_span.length):\n",
    "        split_token_index = document_span.start + i\n",
    "        token_to_origin_mapping[len(\n",
    "                    tokens)] = token_to_original_index[split_token_index]\n",
    "        is_max_context = check_max_context(document_spans,\n",
    "                                                       document_span_index,\n",
    "                                                       split_token_index)\n",
    "        token_max_context[len(tokens)] = is_max_context\n",
    "        tokens.append(all_document_tokens[split_token_index])\n",
    "        segment_ids.append(1)\n",
    "      #this merges the entire paragraph\n",
    "      tokens.append(\"[SEP]\")\n",
    "      segment_ids.append(1)\n",
    "       \n",
    "      input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "      \n",
    "      input_mask = [1] * len(input_ids)\n",
    "       \n",
    "\n",
    "      while len(input_ids) < maximum_sequence_length:\n",
    "        input_ids.append(0)\n",
    "        input_mask.append(0)\n",
    "        segment_ids.append(0)\n",
    "      #print(input_ids, input_mask,segment_ids )\n",
    "      assert len(input_ids) == maximum_sequence_length\n",
    "      assert len(input_mask) == maximum_sequence_length\n",
    "      assert len(segment_ids) == maximum_sequence_length\n",
    "\n",
    "      # Start and end position calculations\n",
    "      start_position = None\n",
    "      end_position = None\n",
    "      if class_index >= 3:\n",
    "        document_start = document_span.start\n",
    "        document_end = document_span.start + document_span.length - 1\n",
    "         \n",
    "        out_of_span = False\n",
    "  \n",
    "      \n",
    "        if not (token_start_position >= document_start\n",
    "                        and token_end_position <= document_end):\n",
    "           \n",
    "          out_of_span = True\n",
    "        if out_of_span:\n",
    "          start_position = 0\n",
    "          end_position = 0\n",
    "          slice_class_index = 2\n",
    "        else: #why use document offest\n",
    "          document_offset = len(query_token) + 2\n",
    "          start_position = token_start_position - document_start + document_offset\n",
    "          end_position = token_end_position - document_start + document_offset\n",
    "          \n",
    "      else:\n",
    "        start_position = 0\n",
    "        end_position = 0\n",
    "      \n",
    "      # add the current feature calculated to the features list   \n",
    "      features.append(                            \n",
    "          DataFeatures(unique_id=unique_id, #record id\n",
    "                        example_index=example_index, #question id\n",
    "                        document_span_index=document_span_index,#document id\n",
    "                        tokens=tokens,#tokens for the entire questions and context texts\n",
    "                        token_to_origin_mapping=token_to_origin_mapping, #mapping the sequence with original index\n",
    "                        token_max_context=token_max_context,#whether exceed the maximum length\n",
    "                        input_ids=input_ids,#the tokenizer word embedding\n",
    "                        input_mask=input_mask,#the mask for all tokens, with word is 1, the rest padded with 0, 450\n",
    "                        segments=segment_ids,#segments of historical question and answer 0, context 1 \n",
    "                        start_position=start_position,#start position of the answer span\n",
    "                        end_position=end_position,#end position of the answer span\n",
    "                        class_index=slice_class_index)) #whether yes, no, unknow, or usual\n",
    "     \n",
    "      unique_id += 1\n",
    "       \n",
    "  return features  # Return all the features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2347,
     "status": "ok",
     "timestamp": 1619489710206,
     "user": {
      "displayName": "chao wang",
      "photoUrl": "",
      "userId": "16929231235898515362"
     },
     "user_tz": -480
    },
    "id": "lYCklFhiIrpv",
    "outputId": "ea2ac400-6565-4aa6-b863-32fd1ce32a3d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating features for CoQA...: 100%|██████████| 25/25 [00:00<00:00, 67.47it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "training_data_features = converting_examples_into_features(\n",
    "                examples=training_samples,\n",
    "                tokenizer=bert_tokenizer,\n",
    "                maximum_sequence_length=450,\n",
    "                document_stride=128,\n",
    "                maximum_query_length=75,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2342,
     "status": "ok",
     "timestamp": 1619489710206,
     "user": {
      "displayName": "chao wang",
      "photoUrl": "",
      "userId": "16929231235898515362"
     },
     "user_tz": -480
    },
    "id": "yKSK94-iItqo",
    "outputId": "e1a91a52-46b9-4b9e-a19e-c650bb6dfd3b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  2,  76,  23,  ...,   0,   0,   0],\n",
      "        [  2,  76,  23,  ...,   0,   0,   0],\n",
      "        [  2,  76,  23,  ...,   0,   0,   0],\n",
      "        ...,\n",
      "        [  2, 113,  23,  ...,   0,   0,   0],\n",
      "        [  2, 184, 212,  ...,   0,   0,   0],\n",
      "        [  2, 184, 212,  ...,   0,   0,   0]])\n",
      "tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])\n",
      "tensor([ 48, 136, 137, 141, 196, 254, 262,  25,  57, 121, 247, 271, 176,   0,\n",
      "        183, 177,  62, 331, 214, 229,  77,  75, 114, 361, 378])\n",
      "tensor([ 52, 136, 147, 146, 217, 264, 262,  28,  59, 124, 256, 271, 186,   0,\n",
      "        185, 188,  64, 334, 233, 236,  79,  79, 116, 362, 378])\n",
      "<torch.utils.data.dataset.TensorDataset object at 0x7fa4bda819d0>\n",
      "<torch.utils.data.sampler.RandomSampler object at 0x7fa4bc913310>\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x7fa4bc8ca3d0>\n"
     ]
    }
   ],
   "source": [
    "# Tensor construction for input ids\n",
    "dataset_input_ids = torch.tensor([f.input_ids for f in training_data_features], dtype=torch.long)\n",
    "print(dataset_input_ids)\n",
    "\n",
    "# Tensor construction for input masks3\n",
    "dataset_input_masks = torch.tensor([f.input_mask for f in training_data_features], dtype=torch.long)\n",
    "print(dataset_input_masks)\n",
    "\n",
    "# Tensor construction for segment ids\n",
    "dataset_segment_ids = torch.tensor([f.segments for f in training_data_features], dtype=torch.long)\n",
    "print(dataset_segment_ids)\n",
    "\n",
    "#Tensor construction for start positions\n",
    "dataset_start_positions = torch.tensor([f.start_position for f in training_data_features], dtype=torch.long)\n",
    "print(dataset_start_positions)\n",
    "\n",
    "#Tensor construction for end positions\n",
    "dataset_end_positions = torch.tensor([f.end_position for f in training_data_features], dtype=torch.long)\n",
    "print(dataset_end_positions)\n",
    "\n",
    "dataset_class_index = torch.tensor([f.class_index for f in training_data_features], dtype=torch.long)\n",
    "\n",
    "# Wrapping tensors in a tensor dataset\n",
    "training_data = TensorDataset(dataset_input_ids, dataset_input_masks, dataset_segment_ids, dataset_start_positions, dataset_end_positions,dataset_class_index)\n",
    "print(training_data)\n",
    "\n",
    "torch.save(training_data, 'train_tensor_testing.pt')\n",
    "\n",
    "\n",
    "# Train sampler to return random indices\n",
    "training_data_sampler = RandomSampler(training_data)\n",
    "print(training_data_sampler)\n",
    "\n",
    "# Creating python iterable over tensor datasets\n",
    "training_dataloader = DataLoader(training_data, sampler=training_data_sampler, batch_size=8) # has to be little else the server will have runtime out error\n",
    "print(training_dataloader)\n",
    "from math import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 907
    },
    "executionInfo": {
     "elapsed": 10124,
     "status": "ok",
     "timestamp": 1619515782874,
     "user": {
      "displayName": "chao wang",
      "photoUrl": "",
      "userId": "16929231235898515362"
     },
     "user_tz": -480
    },
    "id": "ubsX67_VOXWN",
    "outputId": "a36fc237-4791-4691-a224-2b47f3ffbdd4"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "574cca850a8d43e2878d5de26db40663",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=684.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4d9e1aac8cb4ad89ba39b4622194e62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=47376696.0, style=ProgressStyle(descrip…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at albert-base-v2 were not used when initializing CoQAwithAlbert: ['predictions.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.decoder.bias']\n",
      "- This IS expected if you are initializing CoQAwithAlbert from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CoQAwithAlbert from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CoQAwithAlbert were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['linear.weight', 'linear.bias', 'qa_outputs.weight', 'qa_outputs.bias', 'class_outputs.weight', 'class_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CoQAwithAlbert(\n",
       "  (albert): AlbertModel(\n",
       "    (embeddings): AlbertEmbeddings(\n",
       "      (word_embeddings): Embedding(30000, 128, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 128)\n",
       "      (token_type_embeddings): Embedding(2, 128)\n",
       "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0, inplace=False)\n",
       "    )\n",
       "    (encoder): AlbertTransformer(\n",
       "      (embedding_hidden_mapping_in): Linear(in_features=128, out_features=768, bias=True)\n",
       "      (albert_layer_groups): ModuleList(\n",
       "        (0): AlbertLayerGroup(\n",
       "          (albert_layers): ModuleList(\n",
       "            (0): AlbertLayer(\n",
       "              (full_layer_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (attention): AlbertAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (attention_dropout): Dropout(p=0, inplace=False)\n",
       "                (output_dropout): Dropout(p=0, inplace=False)\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              )\n",
       "              (ffn): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (ffn_output): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (pooler_activation): Tanh()\n",
       "  )\n",
       "  (linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (relu): LeakyReLU(negative_slope=0.01)\n",
       "  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (class_outputs): Linear(in_features=768, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import AlbertModel,AlbertConfig    \n",
    "from transformers import AlbertModel, AdamW, AlbertConfig\n",
    "AlbertPreTrainedModel = transformers.AlbertPreTrainedModel \n",
    "# Model creation using Bert Pretrained Model as a base\n",
    "class CoQAwithAlbert(AlbertPreTrainedModel):\n",
    "\n",
    "  # Configurations passed to BERT model\n",
    "  def __init__(\n",
    "            self,\n",
    "            config,\n",
    "            output_attentions=False,\n",
    "            keep_multihead_output=False,\n",
    "            class_alpha=1.0,\n",
    "            mask_p=0.0,\n",
    "    ):\n",
    "    super(CoQAwithAlbert, self).__init__(config)\n",
    "    self.class_alpha = class_alpha\n",
    "    self.mask_p = mask_p\n",
    "    self.albert = AlbertModel(\n",
    "    config,\n",
    "    )\n",
    "    self.linear = nn.Linear(config.hidden_size,config.hidden_size)\n",
    "    self.relu =nn.LeakyReLU()\n",
    "    self.qa_outputs = nn.Linear(config.hidden_size, 2)\n",
    "    self.output_attentions = False\n",
    "    self.class_outputs = nn.Linear(config.hidden_size, 4)\n",
    "    model_config = AlbertConfig.from_pretrained('albert-base-v2', output_hidden_states=True)\n",
    "    self.albert = AlbertModel.from_pretrained('albert-base-v2', config=model_config)\n",
    "    #self.apply(self.init_bert_weights)\n",
    "    #self.apply(self.init_bert_weights)\n",
    "  \n",
    "  # Forward pass for the BERT model\n",
    "  def forward(\n",
    "            self,\n",
    "            input_ids,  # Input seq indices \n",
    "            token_type_ids=None,\n",
    "            attention_mask=None, # Masking to avoid attention\n",
    "            start_positions=None, # Starting position of the span\n",
    "            end_positions=None,  # End position of the span\n",
    "            class_index = None,\n",
    "    ):\n",
    "    outputs = self.albert(\n",
    "            input_ids,\n",
    "            token_type_ids=token_type_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            #head_mask=head_mask,\n",
    "        )\n",
    "     \n",
    "    # outputs consists of the elements based on the configurations provided to BERT\n",
    "    sequence_output= outputs[0]\n",
    "    class_outputs = outputs[1] \n",
    "     \n",
    "    span_logits_0 = self.linear(sequence_output)\n",
    "    span_logits_0 = self.relu(span_logits_0)\n",
    "    span_logits = self.qa_outputs(span_logits_0)\n",
    "    #span_logits = self.qa_outputs(sequence_output)\n",
    "    class_logits_0 = self.linear(class_outputs)\n",
    "    class_logits_0 = self.relu(class_logits_0)\n",
    "    class_logits = self.class_outputs(class_logits_0)\n",
    "    #class_logits =self.class_outputs(class_outputs)\n",
    "     \n",
    "    start_logits, end_logits = span_logits.split(1, dim=-1)\n",
    "    start_logits = start_logits.squeeze(-1)\n",
    "    end_logits = end_logits.squeeze(-1)\n",
    "    #print(class_logits.shape)\n",
    "\n",
    "    # Span extraction based on start positions and end positions\n",
    "    if start_positions is not None and end_positions is not None:\n",
    "      if len(start_positions.size()) > 1:\n",
    "        start_positions = start_positions.squeeze(-1)\n",
    "      if len(end_positions.size()) > 1:\n",
    "        end_positions = end_positions.squeeze(-1)\n",
    "      ignored_index = start_logits.size(1)\n",
    "       \n",
    "      start_positions.clamp_(0, ignored_index)\n",
    "      end_positions.clamp_(0, ignored_index)\n",
    "      #print(start_logits.shape, start_positions)\n",
    "      #print(end_logits, end_positions)\n",
    "      span_loss_factor = CrossEntropyLoss(ignore_index=ignored_index)\n",
    "      class_loss_factor = CrossEntropyLoss()\n",
    "      #here need to have the argmax for start_logits\n",
    "      #this loss is still based on the text span, there is no text generation component, need to see more how others do this piece, the model is not quite right\n",
    "      #data preprocessing done, but there is no ground truth answer therefore can't train, refer to other models, or otherwise this is still using squad method to do coqa\n",
    "      start_loss = span_loss_factor(start_logits, start_positions)\n",
    "      end_loss = span_loss_factor(end_logits, end_positions)\n",
    "      class_loss = class_loss_factor(class_logits, class_index)\n",
    "      #########################################################\n",
    "      #########################################################\n",
    "      #index = class_logits.data.cpu().numpy().argmax()\n",
    "     #index_tensor = torch.argmax(class_logits)\n",
    "      loss=0\n",
    "      #add_loss =nn.L1Loss()\n",
    "      for i, index in enumerate(class_index):\n",
    "        if index == torch.tensor(1).to(device):\n",
    "          #print(class_logits[i].argmax(),index, \"###############\")\n",
    "          #loss =  add_loss(class_logits[i].argmax()/index + 0.001,torch.tensor(1.0).to(device)+0.001).detach()\n",
    "          loss = abs((class_logits[i].argmax()/index + 0.001)-(torch.tensor(1.0).to(device)+0.001))\n",
    "          \n",
    "        else:\n",
    "          loss = 0\n",
    "       \n",
    "   \n",
    "     #print(class_index)\n",
    "      ########################################################################################################\n",
    "      total_loss = (start_loss + end_loss) / 2 + self.class_alpha * class_loss + 0.05* loss \n",
    "      \n",
    "       \n",
    "      return total_loss\n",
    "    return start_logits, end_logits, class_logits\n",
    "model1 = CoQAwithAlbert.from_pretrained('albert-base-v2', output_hidden_states=True)\n",
    " # Set model in training mode\n",
    "device=\"cuda\"\n",
    "model1.to(device)\n",
    "model1.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "output_embedded_package_id": "1SZLr3UALcanTBzoV9JLGuobHzLvfGsdJ"
    },
    "id": "--r0V8nTR3kt",
    "outputId": "1fc7bc80-42d0-421a-b6ef-bb0c0f5f9f40"
   },
   "outputs": [],
   "source": [
    "#for albert\n",
    "RawResult = collections.namedtuple(\"RawResult\",\n",
    "                                   [\"unique_id\", \"start_logits\", \"end_logits\", \"cls_logits\"])\n",
    "\n",
    "# Calculation for optimization steps accordingly training length, which will be used for learning rate for BERT model\n",
    "train_optimization_steps = len(\n",
    "            training_dataloader\n",
    "        )\n",
    "\n",
    "# Fetch the training hyperparameters\n",
    "parameter_optimizer = list(model1.named_parameters())\n",
    "parameter_optimizer = [n for n in parameter_optimizer if 'pooler' not in n[0]]\n",
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in parameter_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in parameter_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}]\n",
    "\n",
    "\n",
    "# Load apex optimizers\n",
    "optimizer = AdamW(optimizer_grouped_parameters,\n",
    "                                  lr=2e-5)\n",
    "#optimizer = FP16_Optimizer(optimizer, dynamic_loss_scale=True)\n",
    "\n",
    "\n",
    "n_gpu = torch.cuda.device_count()\n",
    "# Accuracy and loss for plotting\n",
    "plot_data = []\n",
    "for epoch in trange(2, desc=\"Epoch\"): # Epoch provided: 3, restricted for now to avoid the memory issue but can be increased to improve the model\n",
    "  for step, batch in enumerate(\n",
    "          tqdm(training_dataloader,\n",
    "                desc=\"Iteration\",\n",
    "                disable=-1 not in [-1, 0])):\n",
    "    if n_gpu == 1: # check for gpu count \n",
    "          batch = tuple(\n",
    "              t.to(device)\n",
    "              for t in batch)\n",
    "    # Get the batch data to be provided to the model\n",
    "    dataset_input_ids,dataset_input_masks,dataset_segment_ids, dataset_start_positions, dataset_end_positions, dataset_class_index = batch\n",
    "    loss = model1(dataset_input_ids, dataset_segment_ids, dataset_input_masks,\n",
    "                    dataset_start_positions, dataset_end_positions, dataset_class_index)\n",
    "     # Adding the loss to plot data to plot the graph in the end \n",
    "    loss.backward()\n",
    "    plot_data.append(loss.detach().item())\n",
    "    print(loss.detach().item())\n",
    "    if step < 5000:\n",
    "          lr_this_step = np.linspace(0, 3e-5, 5001)[1:]\n",
    "          for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr_this_step[step]\n",
    "    else:\n",
    "          param_group['lr']= (1+ cos(step*pi/len(training_dataloader)))*1/2*(3e-5)\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    #global_step += 1\n",
    "torch.save(model1, 'outputs/albert_base_16_l1.pth')\n",
    "import matplotlib.pyplot as pplot\n",
    "\n",
    "# Graph plotting code X axis is Batch and Y axis is loss\n",
    "pplot.figure(figsize= (15, 10))\n",
    "pplot.title(\"Training Loss\")\n",
    "pplot.xlabel(\"Batch\")\n",
    "pplot.ylabel(\"Loss\")\n",
    "pplot.plot(plot_data)\n",
    "pplot.savefig('outputs/albert_base_16_l1.png', dpi=300, bbox_inches='tight')\n",
    "pplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "sFKOempeI64w"
   },
   "outputs": [],
   "source": [
    "output_directory = 'outputs'\n",
    "\n",
    "# If output directory doesn't exist create one\n",
    "if not os.path.exists(output_directory):\n",
    "  os.makedirs(output_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "lrF9uAXK0-4n"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Generating examples:   0%|          | 0/500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating examples: 100%|██████████| 500/500 [24:25<00:00,  2.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes 871 no 742 unknown 67 span 4971 3012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Read COQA Dev file, File path needs to be provided where COQA dev file is stored\n",
    "testing_samples = get_data_from_coqa(False, input_file=\"data/coqa-dev-v1.0.json\",\n",
    "                                                history_len= 2,\n",
    "                                                add_QA_tag= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "G0R8XuNK2x07"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating features for CoQA...: 100%|██████████| 7983/7983 [02:09<00:00, 61.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.DataFeatures object at 0x7f2418196510>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Converting the development examples to features\n",
    "testing_features = converting_examples_into_features(\n",
    "                examples=testing_samples,\n",
    "                tokenizer=bert_tokenizer,\n",
    "                maximum_sequence_length=450,\n",
    "                document_stride=128,\n",
    "                maximum_query_length=75,\n",
    "            )\n",
    "print(testing_features[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "GNiuz1PW4Iy7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   2,   98, 1665,  ...,    0,    0,    0],\n",
      "        [   2,   98, 1665,  ...,    0,    0,    0],\n",
      "        [   2,   98, 1665,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [   2,  113,  630,  ...,    0,    0,    0],\n",
      "        [   2,   56,  146,  ...,    0,    0,    0],\n",
      "        [   2,   25,   32,  ...,    0,    0,    0]])\n",
      "tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])\n",
      "<torch.utils.data.dataset.TensorDataset object at 0x7f207d8ed290>\n",
      "<torch.utils.data.sampler.SequentialSampler object at 0x7f207d779bd0>\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x7f209764c890>\n"
     ]
    }
   ],
   "source": [
    "#Tensor construction for input ids\n",
    "dataset_input_ids = torch.tensor([f.input_ids for f in testing_features], dtype=torch.long)\n",
    "print(dataset_input_ids)\n",
    "\n",
    "#Tensor construction for input masks\n",
    "dataset_input_masks = torch.tensor([f.input_mask for f in testing_features], dtype=torch.long)\n",
    "print(dataset_input_masks)\n",
    "\n",
    "#Tensor construction for segment ids\n",
    "dataset_segment_ids = torch.tensor([f.segments for f in testing_features], dtype=torch.long)\n",
    "print(dataset_segment_ids)\n",
    "\n",
    "dataset_example_index  = torch.arange(dataset_input_ids.size(0), dtype=torch.long)\n",
    "\n",
    "# Wrapping tensors in a tensor dataset\n",
    "testing_data = TensorDataset(dataset_input_ids, dataset_input_masks, dataset_segment_ids, dataset_example_index)\n",
    "print(testing_data)\n",
    "\n",
    "# Sampling the elements in the same order they are(sequentially)\n",
    "testing_data_sampler = SequentialSampler(testing_data)\n",
    "print(testing_data_sampler)\n",
    "\n",
    "# Creating python iterable over tensor dataset\n",
    "testing_dataloader = DataLoader(testing_data, sampler=testing_data_sampler, batch_size=8)\n",
    "print(testing_dataloader) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gRNxTklH49I7"
   },
   "outputs": [],
   "source": [
    "\n",
    "#bert = torch.load('output_models/bert_test1.pth')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0QErttMO3_xx"
   },
   "outputs": [],
   "source": [
    "#bert_i = torch.load('output_models/bert_i_test1.pth')\n",
    "#albert = torch.load('output_models/albert_test1.pth')\n",
    "#roberta = torch.load('output_models/roberta_test1.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q5XTt4nO3_2w"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eKLdPffg3__y"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w_cHAj-88Mue"
   },
   "outputs": [],
   "source": [
    "# Fetch the results for predictions using the trained model\n",
    "results_for_predictions = []\n",
    "for tqdm_input_ids, tqdm_input_mask, tqdm_segment_ids, tqdm_example_indices in tqdm(\n",
    "                testing_dataloader,\n",
    "                desc=\"Evaluation\",\n",
    "                disable=-1 not in [-1, 0]):\n",
    "   \n",
    "  tqdm_input_ids = tqdm_input_ids.to(device)\n",
    "  tqdm_input_mask = tqdm_input_mask.to(device)\n",
    "  tqdm_segment_ids = tqdm_segment_ids.to(device)\n",
    "\n",
    "  # Get all the results from the model\n",
    "  # Ensemble the results from Albert and BERT\n",
    "  with torch.no_grad():\n",
    "    model_batch_start_logits, model_batch_end_logits, model_batch_cls_logits = model1(tqdm_input_ids, tqdm_segment_ids, tqdm_input_mask)  \n",
    "    #model_batch_start_logits, model_batch_end_logits, model_batch_cls_logits = bert_i(tqdm_input_ids, tqdm_segment_ids, tqdm_input_mask)  \n",
    "    #model_batch_start_logits, model_batch_end_logits, model_batch_cls_logits = albert(tqdm_input_ids, tqdm_segment_ids, tqdm_input_mask)  \n",
    "    #model_batch_start_logits, model_batch_end_logits, model_batch_cls_logits = roberta(tqdm_input_ids, tqdm_segment_ids, tqdm_input_mask)    \n",
    "   # robertamodel_batch_start_logits, robertamodel_batch_end_logits, robertamodel_batch_cls_logits = roberta(tqdm_input_ids, tqdm_segment_ids, tqdm_input_mask)\n",
    "   # bertmodel_batch_start_logits, bertmodel_batch_end_logits, bertmodel_batch_cls_logits = bert(tqdm_input_ids, tqdm_segment_ids, tqdm_input_mask)\n",
    "   # albertmodel_batch_start_logits, albertmodel_batch_end_logits, albertmodel_batch_cls_logits = albert(tqdm_input_ids, tqdm_segment_ids, tqdm_input_mask)\n",
    "  #model_batch_start_logits = (bertmodel_batch_start_logits +albertmodel_batch_start_logits)/2\n",
    "  #model_batch_end_logits = (bertmodel_batch_end_logits+albertmodel_batch_end_logits)/2 \n",
    "  #model_batch_cls_logits =(bertmodel_batch_cls_logits+ albertmodel_batch_cls_logits)/2\n",
    " \n",
    "   \n",
    "  # Get the start end logists from the model and store it in results\n",
    "  for i, tqdm_example_index in enumerate(tqdm_example_indices):\n",
    "    this_start_logits = model_batch_start_logits[i].detach().cpu().tolist()\n",
    "    this_end_logits = model_batch_end_logits[i].detach().cpu().tolist()\n",
    "    this_cls_logits = model_batch_cls_logits[i].detach().cpu().tolist()\n",
    "    testing_feature = testing_features[tqdm_example_index.item()]\n",
    "    unique_id = int(testing_feature.unique_id)\n",
    " \n",
    "    # Store the prediction in the results list\n",
    "    results_for_predictions.append(\n",
    "                    RawResult(unique_id=unique_id,\n",
    "                    start_logits= this_start_logits,\n",
    "                    end_logits= this_end_logits,\n",
    "                    cls_logits= this_cls_logits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wZ99u1kI-oNZ"
   },
   "outputs": [],
   "source": [
    "#op_pred_file = os.path.join(output_directory, \"Output_Preds_bert.json\")\n",
    "#output_nbest_file = os.path.join(output_directory, \"nbest_predictions_bert.json\")\n",
    "#op_pred_file = os.path.join(output_directory, \"Output_Preds_bert_i.json\")\n",
    "#output_nbest_file = os.path.join(output_directory, \"nbest_predictions_bert_i.json\")\n",
    "op_pred_file = os.path.join(output_directory, \"Output_Preds_albert_base_16_l1.json\")\n",
    "output_nbest_file = os.path.join(output_directory, \"nbest_predictions_albert_base_16_l1.json\")\n",
    "#op_pred_file = os.path.join(output_directory, \"Output_Preds_roberta.json\")\n",
    "#output_nbest_file = os.path.join(output_directory, \"nbest_predictions_roberta.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a5-x49P1AHnb"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Get the appropriate index for the answer text\n",
    "def compute_best_indices(logits, n):\n",
    "  index_with_score = sorted(enumerate(logits),\n",
    "                             key=lambda x: x[1],\n",
    "                             reverse=True)\n",
    "   \n",
    "  best_indices = []\n",
    "  for i in range(len(index_with_score)):\n",
    "    if i >= n:\n",
    "      break\n",
    "    best_indices.append(index_with_score[i][0])\n",
    "  return best_indices\n",
    "# Get the final text fetching it from the span using the predicted answer\n",
    "def output_final_answer_text(predicted_text, original_text, low_case, v_log=False):\n",
    "  def remove_spaces(text):\n",
    "    non_space_chars = []\n",
    "    non_space_char_to_space_char_map = collections.OrderedDict()\n",
    "    for (i, c) in enumerate(text):\n",
    "      if c == \" \":\n",
    "        continue\n",
    "      non_space_char_to_space_char_map[len(non_space_chars)] = i\n",
    "      non_space_chars.append(c)\n",
    "    non_space_text = \"\".join(non_space_chars)\n",
    "    return (non_space_text, non_space_char_to_space_char_map)\n",
    "\n",
    "  # Get the BERT tokenizer to tokenize the text\n",
    "  tokenizer = BasicTokenizer(do_lower_case=low_case)\n",
    "  tokenized_text = \" \".join(tokenizer.tokenize(original_text))\n",
    "\n",
    "  # Find predicted text in the tokenized text to get the start and end positions\n",
    "  start_position = tokenized_text.find(predicted_text)\n",
    "  if start_position == -1:\n",
    "      return original_text\n",
    "  end_position = start_position + len(predicted_text) - 1\n",
    "\n",
    "  # Remove spaces if any\n",
    "  (original_non_space_text, original_non_space_to_space_map) = remove_spaces(original_text)\n",
    "  (tokenized_non_space_text, tokenized_non_space_to_space_map) = remove_spaces(tokenized_text)\n",
    "\n",
    "  if len(original_non_space_text) != len(tokenized_non_space_text):\n",
    "    return original_text\n",
    "\n",
    "  tokenized_non_space_to_space_map = {}\n",
    "  for (i, _tokenized_index) in tokenized_non_space_to_space_map.items():\n",
    "    tokenized_non_space_to_space_map[_tokenized_index] = i\n",
    "\n",
    "  # Get the start position\n",
    "  original_start_position = None\n",
    "  if start_position in tokenized_non_space_to_space_map:\n",
    "    non_space_start_position = tokenized_non_space_to_space_map[start_position]\n",
    "    if non_space_start_position in original_non_space_to_space_map:\n",
    "      original_start_position = original_non_space_to_space_map[non_space_start_position]\n",
    "  \n",
    "  # Check if the start position is None\n",
    "  if original_start_position is None:\n",
    "    return original_text\n",
    "\n",
    "  # Get the End position\n",
    "  original_text_end_position = None\n",
    "  if end_position in tokenized_non_space_to_space_map:\n",
    "    non_space_end_position = tokenized_non_space_to_space_map[end_position]\n",
    "    if non_space_end_position in original_non_space_to_space_map:\n",
    "      original_text_end_position = original_non_space_to_space_map[non_space_end_position]\n",
    "\n",
    "  # Check if the end position is None\n",
    "  if original_text_end_position is None:\n",
    "    return original_text\n",
    "  \n",
    "  # Get the answer using start position and end position in the text\n",
    "  final_output_text = original_text[original_start_position:(original_text_end_position + 1)]\n",
    "  return final_output_text\n",
    "# Calculates the probability of the span found\n",
    "def compute_softmax_score(scores):\n",
    "  if not scores:\n",
    "    return []\n",
    "\n",
    "  maximum_softmax_score = None\n",
    "  for score in scores:\n",
    "    if maximum_softmax_score is None or score > maximum_softmax_score:\n",
    "      maximum_softmax_score = score\n",
    "\n",
    "  expected_scores = []\n",
    "  total_score_sum = 0.0\n",
    "  for score in scores:\n",
    "    x = math.exp(score - maximum_softmax_score)\n",
    "    expected_scores.append(x)\n",
    "    total_score_sum += x\n",
    "\n",
    "  probabilities = []\n",
    "  for score in expected_scores:\n",
    "    probabilities.append(score / total_score_sum) # Probability calculation using the softmax score\n",
    "  return probabilities\n",
    "\n",
    "# Removing punctuations, lowering texts and removing extra white spaces\n",
    "def normalize_answer1(s):\n",
    "    \n",
    "    # Remove articles from the text\n",
    "    def remove_articles(text):\n",
    "        regex = re.compile(r'\\b(a|an|the)\\b', re.UNICODE)\n",
    "        return re.sub(regex, ' ', text)\n",
    "    \n",
    "    # Remove white spaces from the text\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    # Remove punctuations from the text\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    # Lower the text characters\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "# Check if the predictions are numbers or boolean values then set those to string equivalent\n",
    "def confirm_predictions(json_best_predictions):\n",
    "  # Number strings that will be used to represent numbers in the answers instead of actual numbers\n",
    "  subs = ['one', 'two', 'three','four','five','six','seven','eight','nine','ten','eleven','twelve','true','false']\n",
    "  original = json_best_predictions[0]['text']\n",
    "  if len(original) < 2:\n",
    "    for e in json_best_predictions[1:]:\n",
    "      if normalize_answer1(e['text']) in subs:\n",
    "        return e['text']\n",
    "    return 'unknown'\n",
    "  return original\n",
    "# Function to Predict answers and write those predictions to predictions file \n",
    "def predict_answers(test_samples, test_sample_features, results_for_predictions, best_size,\n",
    "                  maximum_answer_length, low_case, op_pred_file, v_log,\n",
    "                  null_score_threshold):\n",
    "  \n",
    "  ex_index_to_feat_index = collections.defaultdict(list)\n",
    "  \n",
    "  # Create the dictionary of all the features in test features keeping feature index as key\n",
    "  for feature in test_sample_features:\n",
    "    ex_index_to_feat_index[feature.example_index].append(feature)\n",
    " \n",
    "  ids_for_results = {}\n",
    "  for result in results_for_predictions:\n",
    "    ids_for_results[result.unique_id] = result\n",
    "   \n",
    "  # Naming the tuples for predictions\n",
    "  Preliminary_Predictions = collections.namedtuple(\n",
    "      \"Preliminary_Predictions\", [\n",
    "                           \"feature_index\",\n",
    "                           \"start_index\",\n",
    "                           \"end_index\",\n",
    "                           \"start_logit\",\n",
    "                           \"end_logit\",\n",
    "                           \"class_logit\",\n",
    "                           \"class_index\",\n",
    "      ])\n",
    "  \n",
    "  complete_predictions = []\n",
    "  best_n_predictions_json = collections.OrderedDict() #in this case best 30 predictions\n",
    "  prediction_scores_json = collections.OrderedDict()\n",
    "  \n",
    "  for (example_index, example) in enumerate(\n",
    "      tqdm(test_samples, desc=\"Predicting...\")):\n",
    "    features = ex_index_to_feat_index[example_index]\n",
    "     \n",
    "    preliminary_predictions = []\n",
    "\n",
    "    part_preliminary_predictions = []\n",
    "\n",
    "    # Indices initialization\n",
    "    score_of_answer_yes, score_of_answer_no, score_span, score_of_no_answer = -float('INF'), -float('INF'), -float('INF'), float('INF')\n",
    "\n",
    "    \n",
    "    minimum_no_answer_feature_index, maximum_yes_feature_index, maximum_no_feature_index, maximum_span_feature_index = 0, 0, 0, 0\n",
    "    max_span_start_indexes, max_span_end_indexes = [], []\n",
    "     \n",
    "    # get the best start and end indices\n",
    "    for (feature_index, feature) in enumerate(features):\n",
    "       \n",
    "      result = ids_for_results[feature.unique_id]\n",
    "      # check the score for each class and determine the output yes, no, or span, or unknown \n",
    "      feature_yes_score, feature_no_score, feature_noanswer_score, feature_span_score = result.cls_logits \n",
    "       \n",
    "      if feature_noanswer_score < score_of_no_answer:\n",
    "        score_of_no_answer = feature_noanswer_score\n",
    "        minimum_no_answer_feature_index = feature_index\n",
    "      if feature_yes_score > score_of_answer_yes:\n",
    "        score_of_answer_yes = feature_yes_score\n",
    "        maximum_yes_feature_index = feature_index\n",
    "      if feature_no_score > score_of_answer_no:\n",
    "        score_of_answer_no = feature_no_score\n",
    "        maximum_no_feature_index = feature_index\n",
    "      # Here hasn't assign the correct class yet\n",
    "      if feature_span_score > score_span:\n",
    "        score_span = feature_span_score\n",
    "        maximum_span_feature_index = feature_index\n",
    "        start_indices = compute_best_indices(result.start_logits,\n",
    "                                                  best_size)\n",
    "        end_indices = compute_best_indices(result.end_logits, best_size)\n",
    "        maximum_span_start_indices, maximum_span_end_indices = start_indices, end_indices\n",
    "      \n",
    "         \n",
    "    \n",
    "    preliminary_predictions.append(\n",
    "        Preliminary_Predictions(feature_index=minimum_no_answer_feature_index,\n",
    "                                start_index=0,\n",
    "                                end_index=0,\n",
    "                                start_logit=-float('INF'),\n",
    "                                end_logit=-float('INF'),\n",
    "                                class_logit=score_of_no_answer,\n",
    "                                class_index=2))\n",
    "    preliminary_predictions.append(\n",
    "              Preliminary_Predictions(feature_index=maximum_yes_feature_index,\n",
    "                                start_index=0,\n",
    "                                end_index=0,\n",
    "                                start_logit=-float('INF'),\n",
    "                                end_logit=-float('INF'),\n",
    "                                class_logit=score_of_answer_yes,\n",
    "                                class_index=0))\n",
    "    preliminary_predictions.append(\n",
    "              Preliminary_Predictions(feature_index=maximum_no_feature_index,\n",
    "                                start_index=0,\n",
    "                                end_index=0,\n",
    "                                start_logit=-float('INF'),\n",
    "                                end_logit=-float('INF'),\n",
    "                                class_logit=score_of_answer_no,\n",
    "                                class_index=1))\n",
    "    preliminary_predictions.append(\n",
    "              Preliminary_Predictions(feature_index=maximum_span_feature_index,\n",
    "                                start_index=0,\n",
    "                                end_index=0,\n",
    "                                start_logit=-float('INF'),\n",
    "                                end_logit=-float('INF'),\n",
    "                                class_logit=score_span,\n",
    "                                class_index=3))\n",
    "   \n",
    "    feature = features[maximum_span_feature_index]\n",
    "    for start_index in maximum_span_start_indices:\n",
    "      for end_index in maximum_span_end_indices:\n",
    "        \n",
    "        if start_index >= len(feature.tokens):\n",
    "          continue\n",
    "        if end_index >= len(feature.tokens):\n",
    "          continue\n",
    "        if start_index not in feature.token_to_origin_mapping:\n",
    "          continue\n",
    "        if end_index not in feature.token_to_origin_mapping:\n",
    "          continue\n",
    "        if not feature.token_max_context.get(start_index, False):\n",
    "          continue\n",
    "        if end_index < start_index:\n",
    "          continue\n",
    "        length = end_index - start_index + 1\n",
    "        if length > maximum_answer_length:\n",
    "          continue\n",
    "        \n",
    "        part_preliminary_predictions.append(\n",
    "                      Preliminary_Predictions(\n",
    "                          feature_index=maximum_span_feature_index,\n",
    "                          start_index=start_index,\n",
    "                          end_index=end_index,\n",
    "                          start_logit=ids_for_results[\n",
    "                              feature.unique_id].start_logits[start_index],\n",
    "                          end_logit=ids_for_results[\n",
    "                              feature.unique_id].end_logits[end_index],\n",
    "                          class_logit=score_span,\n",
    "                          class_index=3))\n",
    "    ##this is to sort the largest score value for start and end pair    \n",
    "    part_preliminary_predictions = sorted(\n",
    "              part_preliminary_predictions,\n",
    "              key=lambda p: p.start_logit + p.end_logit,\n",
    "              reverse=True)\n",
    "    ##this is to sort the largest score value for class \n",
    "    preliminary_predictions = sorted(preliminary_predictions,\n",
    "                                      key=lambda p: p.class_logit,\n",
    "                                      reverse=True)\n",
    "    \n",
    "    Best_Predictions = collections.namedtuple(  \n",
    "              \"Best_Predictions\",\n",
    "              [\"text\", \"start_logit\", \"end_logit\", \"class_logit\", \"class_index\"])\n",
    "    \n",
    "    known_predictions = {}\n",
    "    best = []\n",
    "    class_rank = []\n",
    "    for prediction in part_preliminary_predictions:\n",
    "      if len(best) >= best_size:\n",
    "        break\n",
    "      feature = features[prediction.feature_index]\n",
    "      if prediction.class_index == 3:\n",
    "        tokenized_tokens = feature.tokens[prediction.start_index:(prediction.end_index + 1)]\n",
    "        original_document_start = feature.token_to_origin_mapping[prediction.start_index]\n",
    "        original_document_end = feature.token_to_origin_mapping[prediction.end_index]\n",
    "        original_tokens = example.document_tokens[original_document_start:(original_document_end + 1)]\n",
    "        \n",
    "        tokenized_text = \" \".join(tokenized_tokens)\n",
    "\n",
    "        tokenized_text = tokenized_text.replace(\" ##\", \"\")\n",
    "        tokenized_text = tokenized_text.replace(\"##\", \"\")\n",
    "\n",
    "        tokenized_text = tokenized_text.strip()\n",
    "        tokenized_text = \" \".join(tokenized_text.split())\n",
    "        original_text = \" \".join(original_tokens)\n",
    "\n",
    "        final_output_text = output_final_answer_text(tokenized_text, original_text, low_case, v_log)\n",
    "         \n",
    "        if final_output_text in known_predictions:\n",
    "          continue\n",
    "\n",
    "        known_predictions[final_output_text] = True\n",
    "        best.append(\n",
    "                      Best_Predictions(text=final_output_text,\n",
    "                                      start_logit=prediction.start_logit,\n",
    "                                      end_logit=prediction.end_logit,\n",
    "                                      class_logit=prediction.class_logit,\n",
    "                                      class_index=prediction.class_index))\n",
    "    \n",
    "    # Writing the approriate answers in predictions which will be written to json file\n",
    "    if not best or len(best) < 1: \n",
    "      best.append(\n",
    "                  Best_Predictions(text=\"unknown\",\n",
    "                                  start_logit=-float('INF'),\n",
    "                                  end_logit=-float('INF'),\n",
    "                                  class_logit=score_span,\n",
    "                                  class_index=3))\n",
    "    for prediction in preliminary_predictions:\n",
    "      if prediction.class_index == 3:\n",
    "        final_output_text = best[0].text\n",
    "        class_rank.append(\n",
    "                      Best_Predictions(text=final_output_text,\n",
    "                                      start_logit=best[0].start_logit,\n",
    "                                      end_logit=best[0].end_logit,\n",
    "                                      class_logit=prediction.class_logit,\n",
    "                                      class_index=prediction.class_index))\n",
    "      elif prediction.class_index == 0:\n",
    "        final_output_text = \"yes\"\n",
    "        class_rank.append(\n",
    "                      Best_Predictions(text=final_output_text,\n",
    "                                      start_logit=-float('INF'),\n",
    "                                      end_logit=-float('INF'),\n",
    "                                      class_logit=prediction.class_logit,\n",
    "                                      class_index=prediction.class_index))\n",
    "      elif prediction.class_index == 1:\n",
    "                  final_output_text = \"no\"\n",
    "                  class_rank.append(\n",
    "                      Best_Predictions(text=final_output_text,\n",
    "                                      start_logit=-float('INF'),\n",
    "                                      end_logit=-float('INF'),\n",
    "                                      class_logit=prediction.class_logit,\n",
    "                                      class_index=prediction.class_index))\n",
    "      elif prediction.class_index == 2:\n",
    "                  final_output_text = \"unknown\"\n",
    "                  class_rank.append(\n",
    "                      Best_Predictions(text=final_output_text,\n",
    "                                      start_logit=-float('INF'),\n",
    "                                      end_logit=-float('INF'),\n",
    "                                      class_logit=prediction.class_logit,\n",
    "                                      class_index=prediction.class_index))\n",
    "                  \n",
    "    assert len(best) >= 1\n",
    "\n",
    "    total_scores = []\n",
    "    class_scores = []\n",
    "    for item in best:\n",
    "      total_scores.append(item.start_logit + item.end_logit)\n",
    "    for rank in class_rank:\n",
    "      class_scores.append(rank.class_logit)\n",
    "    \n",
    "    # calculate Softmax\n",
    "    span_probabilities = compute_softmax_score(total_scores)\n",
    "    class_probabilities = compute_softmax_score(class_scores)\n",
    "    best_predictions_json = []\n",
    "\n",
    "    current_rank, current_probabilities, current_scores = (\n",
    "        best, span_probabilities,\n",
    "        total_scores) if class_rank[0].class_index == 3 and len(best) > 1 else (\n",
    "            class_rank, class_probabilities, class_scores)\n",
    "    \n",
    "    \n",
    "    # Store the answer text, probability and score for each entry\n",
    "    for i, entry in enumerate(current_rank): \n",
    "      predicted_outputs = collections.OrderedDict()\n",
    "      predicted_outputs[\"text\"] = entry.text\n",
    "      predicted_outputs[\"probability\"] = current_probabilities[i]\n",
    "      predicted_outputs[\"score\"] = current_scores[i]\n",
    "      best_predictions_json.append(predicted_outputs)\n",
    "\n",
    "    assert len(best_predictions_json) >= 1\n",
    "\n",
    "    _id, _turn_id = example.question_answer_id.split()\n",
    "    complete_predictions.append({'id': _id, 'turn_id': int(_turn_id), 'answer': confirm_predictions(best_predictions_json)})\n",
    "    best_n_predictions_json[example.question_answer_id] = best_predictions_json\n",
    "\n",
    "  # Write the prediction files\n",
    "  with open(op_pred_file, \"w\") as writer:\n",
    "        writer.write(json.dumps(complete_predictions, indent=4) + \"\\n\")\n",
    "  \n",
    "  with open(output_nbest_file, \"w\") as writer:\n",
    "        writer.write(json.dumps(best_n_predictions_json, indent=4) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 32928,
     "status": "ok",
     "timestamp": 1619502226617,
     "user": {
      "displayName": "chao wang",
      "photoUrl": "",
      "userId": "16929231235898515362"
     },
     "user_tz": -480
    },
    "id": "HxhzcNKEAw1Z",
    "outputId": "125e0a62-6b8a-4680-d727-3dc856ec8c51"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:   0%|          | 0/7983 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:   0%|          | 30/7983 [00:00<00:26, 297.48it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:   1%|          | 57/7983 [00:00<00:27, 286.63it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:   1%|          | 84/7983 [00:00<00:28, 279.09it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:   1%|▏         | 113/7983 [00:00<00:28, 279.69it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:   2%|▏         | 139/7983 [00:00<00:28, 271.39it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:   2%|▏         | 168/7983 [00:00<00:28, 276.44it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:   2%|▏         | 197/7983 [00:00<00:27, 278.83it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:   3%|▎         | 225/7983 [00:00<00:27, 278.99it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:   3%|▎         | 252/7983 [00:00<00:28, 272.84it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:   3%|▎         | 279/7983 [00:01<00:28, 269.96it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:   4%|▍         | 307/7983 [00:01<00:28, 271.15it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:   4%|▍         | 334/7983 [00:01<00:28, 270.12it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:   5%|▍         | 362/7983 [00:01<00:27, 272.89it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:   5%|▍         | 390/7983 [00:01<00:27, 272.61it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:   5%|▌         | 418/7983 [00:01<00:28, 270.08it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:   6%|▌         | 445/7983 [00:01<00:28, 267.37it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:   6%|▌         | 472/7983 [00:01<00:28, 263.21it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:   6%|▋         | 499/7983 [00:01<00:28, 261.94it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:   7%|▋         | 528/7983 [00:01<00:27, 267.81it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:   7%|▋         | 556/7983 [00:02<00:27, 270.30it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:   7%|▋         | 584/7983 [00:02<00:27, 271.67it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:   8%|▊         | 612/7983 [00:02<00:27, 263.54it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:   8%|▊         | 641/7983 [00:02<00:27, 269.93it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:   8%|▊         | 669/7983 [00:02<00:26, 271.33it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:   9%|▊         | 697/7983 [00:02<00:27, 266.40it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:   9%|▉         | 724/7983 [00:02<00:27, 264.49it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:   9%|▉         | 752/7983 [00:02<00:26, 268.50it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  10%|▉         | 779/7983 [00:02<00:27, 266.23it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  10%|█         | 806/7983 [00:02<00:27, 264.16it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  10%|█         | 837/7983 [00:03<00:25, 275.57it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  11%|█         | 865/7983 [00:03<00:25, 275.89it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  11%|█         | 893/7983 [00:03<00:26, 269.10it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  12%|█▏        | 921/7983 [00:03<00:26, 267.85it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  12%|█▏        | 948/7983 [00:03<00:26, 267.11it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  12%|█▏        | 975/7983 [00:03<00:26, 265.80it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  13%|█▎        | 1003/7983 [00:03<00:26, 267.79it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  13%|█▎        | 1031/7983 [00:03<00:25, 269.21it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  13%|█▎        | 1058/7983 [00:03<00:26, 257.86it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  14%|█▎        | 1084/7983 [00:04<00:27, 252.91it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  14%|█▍        | 1113/7983 [00:04<00:26, 261.33it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  14%|█▍        | 1142/7983 [00:04<00:25, 267.43it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  15%|█▍        | 1169/7983 [00:04<00:25, 263.72it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  15%|█▌        | 1198/7983 [00:04<00:25, 270.99it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  15%|█▌        | 1226/7983 [00:04<00:24, 272.07it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  16%|█▌        | 1256/7983 [00:04<00:24, 278.69it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  16%|█▌        | 1285/7983 [00:04<00:23, 281.75it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  16%|█▋        | 1314/7983 [00:04<00:23, 282.24it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  17%|█▋        | 1346/7983 [00:04<00:22, 291.43it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  17%|█▋        | 1376/7983 [00:05<00:23, 283.54it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  18%|█▊        | 1405/7983 [00:05<00:23, 278.94it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  18%|█▊        | 1435/7983 [00:05<00:23, 284.13it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  18%|█▊        | 1466/7983 [00:05<00:22, 289.66it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  19%|█▊        | 1496/7983 [00:05<00:22, 283.14it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  19%|█▉        | 1525/7983 [00:05<00:23, 274.92it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  19%|█▉        | 1553/7983 [00:05<00:23, 273.35it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  20%|█▉        | 1581/7983 [00:05<00:23, 272.81it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  20%|██        | 1609/7983 [00:05<00:23, 269.25it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  21%|██        | 1637/7983 [00:06<00:23, 271.00it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  21%|██        | 1668/7983 [00:06<00:22, 280.71it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  21%|██▏       | 1697/7983 [00:06<00:22, 273.93it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  22%|██▏       | 1725/7983 [00:06<00:23, 265.14it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  22%|██▏       | 1752/7983 [00:06<00:23, 260.03it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  22%|██▏       | 1781/7983 [00:06<00:23, 265.09it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  23%|██▎       | 1808/7983 [00:06<00:23, 264.38it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  23%|██▎       | 1838/7983 [00:06<00:22, 272.71it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  23%|██▎       | 1866/7983 [00:06<00:22, 268.63it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  24%|██▎       | 1895/7983 [00:06<00:22, 273.02it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  24%|██▍       | 1923/7983 [00:07<00:22, 272.13it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  24%|██▍       | 1954/7983 [00:07<00:21, 278.92it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  25%|██▍       | 1982/7983 [00:07<00:22, 271.59it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  25%|██▌       | 2012/7983 [00:07<00:21, 277.63it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  26%|██▌       | 2040/7983 [00:07<00:21, 273.84it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  26%|██▌       | 2068/7983 [00:07<00:21, 273.07it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  26%|██▋       | 2096/7983 [00:07<00:21, 272.98it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  27%|██▋       | 2124/7983 [00:07<00:22, 265.17it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  27%|██▋       | 2151/7983 [00:07<00:22, 262.45it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  27%|██▋       | 2179/7983 [00:08<00:21, 267.10it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  28%|██▊       | 2207/7983 [00:08<00:21, 270.80it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  28%|██▊       | 2235/7983 [00:08<00:21, 268.89it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  28%|██▊       | 2263/7983 [00:08<00:21, 271.23it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  29%|██▊       | 2291/7983 [00:08<00:21, 270.59it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  29%|██▉       | 2319/7983 [00:08<00:21, 265.70it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  29%|██▉       | 2346/7983 [00:08<00:21, 260.73it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  30%|██▉       | 2373/7983 [00:08<00:21, 262.38it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  30%|███       | 2400/7983 [00:08<00:22, 252.94it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  30%|███       | 2426/7983 [00:08<00:22, 251.25it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  31%|███       | 2455/7983 [00:09<00:21, 261.02it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  31%|███       | 2482/7983 [00:09<00:21, 253.70it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  31%|███▏      | 2508/7983 [00:09<00:21, 254.64it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  32%|███▏      | 2534/7983 [00:09<00:53, 101.55it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  32%|███▏      | 2565/7983 [00:10<00:42, 127.14it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  32%|███▏      | 2594/7983 [00:10<00:35, 151.94it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  33%|███▎      | 2626/7983 [00:10<00:29, 180.23it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  33%|███▎      | 2656/7983 [00:10<00:26, 202.83it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  34%|███▎      | 2684/7983 [00:10<00:24, 214.27it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  34%|███▍      | 2711/7983 [00:10<00:23, 227.43it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  34%|███▍      | 2738/7983 [00:10<00:22, 236.93it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  35%|███▍      | 2765/7983 [00:10<00:21, 244.87it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  35%|███▍      | 2793/7983 [00:10<00:20, 252.51it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  35%|███▌      | 2820/7983 [00:10<00:20, 246.57it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  36%|███▌      | 2850/7983 [00:11<00:19, 260.16it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  36%|███▌      | 2877/7983 [00:11<00:19, 259.77it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  36%|███▋      | 2906/7983 [00:11<00:18, 267.98it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  37%|███▋      | 2934/7983 [00:11<00:18, 269.01it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  37%|███▋      | 2964/7983 [00:11<00:18, 274.68it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  37%|███▋      | 2992/7983 [00:11<00:18, 271.88it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  38%|███▊      | 3020/7983 [00:11<00:18, 266.92it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  38%|███▊      | 3047/7983 [00:11<00:18, 265.75it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  39%|███▊      | 3079/7983 [00:11<00:17, 279.42it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  39%|███▉      | 3108/7983 [00:11<00:17, 278.78it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  39%|███▉      | 3137/7983 [00:12<00:17, 279.30it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  40%|███▉      | 3166/7983 [00:12<00:17, 268.82it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  40%|████      | 3194/7983 [00:12<00:17, 268.52it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  40%|████      | 3221/7983 [00:12<00:17, 265.99it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  41%|████      | 3249/7983 [00:12<00:17, 267.12it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  41%|████      | 3278/7983 [00:12<00:17, 271.24it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  41%|████▏     | 3309/7983 [00:12<00:16, 279.85it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  42%|████▏     | 3339/7983 [00:12<00:16, 283.72it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  42%|████▏     | 3368/7983 [00:12<00:16, 281.36it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  43%|████▎     | 3397/7983 [00:13<00:16, 280.31it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  43%|████▎     | 3426/7983 [00:13<00:16, 278.49it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  43%|████▎     | 3456/7983 [00:13<00:16, 282.72it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  44%|████▎     | 3485/7983 [00:13<00:16, 272.60it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  44%|████▍     | 3513/7983 [00:13<00:16, 265.65it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  44%|████▍     | 3544/7983 [00:13<00:16, 276.11it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  45%|████▍     | 3572/7983 [00:13<00:15, 276.82it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  45%|████▌     | 3600/7983 [00:13<00:15, 277.50it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  45%|████▌     | 3630/7983 [00:13<00:15, 283.41it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  46%|████▌     | 3659/7983 [00:13<00:15, 281.12it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  46%|████▌     | 3688/7983 [00:14<00:15, 276.76it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  47%|████▋     | 3716/7983 [00:14<00:15, 274.68it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  47%|████▋     | 3747/7983 [00:14<00:14, 283.97it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  47%|████▋     | 3776/7983 [00:14<00:14, 283.69it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  48%|████▊     | 3805/7983 [00:14<00:15, 277.33it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  48%|████▊     | 3833/7983 [00:14<00:15, 275.03it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  48%|████▊     | 3861/7983 [00:14<00:15, 271.27it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  49%|████▊     | 3889/7983 [00:14<00:15, 272.19it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  49%|████▉     | 3918/7983 [00:14<00:14, 275.04it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  49%|████▉     | 3946/7983 [00:15<00:14, 271.47it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  50%|████▉     | 3975/7983 [00:15<00:14, 274.41it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  50%|█████     | 4003/7983 [00:15<00:14, 269.15it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  51%|█████     | 4032/7983 [00:15<00:14, 273.15it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  51%|█████     | 4061/7983 [00:15<00:14, 276.41it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  51%|█████     | 4089/7983 [00:15<00:14, 275.50it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  52%|█████▏    | 4118/7983 [00:15<00:13, 277.33it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  52%|█████▏    | 4146/7983 [00:15<00:14, 271.76it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  52%|█████▏    | 4174/7983 [00:15<00:13, 272.13it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  53%|█████▎    | 4202/7983 [00:15<00:13, 273.54it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  53%|█████▎    | 4230/7983 [00:16<00:13, 269.49it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  53%|█████▎    | 4262/7983 [00:16<00:13, 281.51it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  54%|█████▍    | 4291/7983 [00:16<00:13, 279.49it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  54%|█████▍    | 4321/7983 [00:16<00:12, 285.18it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  54%|█████▍    | 4350/7983 [00:16<00:12, 280.48it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  55%|█████▍    | 4379/7983 [00:16<00:13, 276.19it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  55%|█████▌    | 4407/7983 [00:16<00:12, 275.18it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  56%|█████▌    | 4435/7983 [00:16<00:13, 269.55it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  56%|█████▌    | 4463/7983 [00:16<00:13, 269.67it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  56%|█████▋    | 4491/7983 [00:17<00:13, 268.53it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  57%|█████▋    | 4521/7983 [00:17<00:12, 275.29it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  57%|█████▋    | 4549/7983 [00:17<00:12, 266.65it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  57%|█████▋    | 4577/7983 [00:17<00:12, 270.45it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  58%|█████▊    | 4605/7983 [00:17<00:12, 263.16it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  58%|█████▊    | 4632/7983 [00:17<00:13, 254.73it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  58%|█████▊    | 4658/7983 [00:17<00:13, 253.02it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  59%|█████▊    | 4685/7983 [00:17<00:12, 256.36it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  59%|█████▉    | 4715/7983 [00:17<00:12, 265.88it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  59%|█████▉    | 4742/7983 [00:17<00:12, 260.08it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  60%|█████▉    | 4770/7983 [00:18<00:12, 265.25it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  60%|██████    | 4797/7983 [00:18<00:12, 262.29it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  60%|██████    | 4826/7983 [00:18<00:11, 268.59it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  61%|██████    | 4860/7983 [00:18<00:10, 286.23it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  61%|██████▏   | 4890/7983 [00:18<00:11, 277.67it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  62%|██████▏   | 4919/7983 [00:18<00:10, 279.41it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  62%|██████▏   | 4948/7983 [00:18<00:10, 277.38it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  62%|██████▏   | 4978/7983 [00:18<00:10, 282.16it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  63%|██████▎   | 5007/7983 [00:18<00:10, 280.99it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  63%|██████▎   | 5036/7983 [00:19<00:10, 278.85it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  63%|██████▎   | 5064/7983 [00:19<00:10, 271.39it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  64%|██████▍   | 5092/7983 [00:19<00:10, 273.72it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  64%|██████▍   | 5123/7983 [00:19<00:10, 282.09it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  65%|██████▍   | 5152/7983 [00:19<00:09, 283.33it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  65%|██████▍   | 5181/7983 [00:19<00:09, 282.19it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  65%|██████▌   | 5210/7983 [00:19<00:09, 278.51it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  66%|██████▌   | 5239/7983 [00:19<00:09, 280.77it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  66%|██████▌   | 5268/7983 [00:19<00:09, 275.10it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  66%|██████▋   | 5296/7983 [00:19<00:10, 267.59it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  67%|██████▋   | 5323/7983 [00:20<00:10, 263.43it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  67%|██████▋   | 5351/7983 [00:20<00:09, 265.71it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  67%|██████▋   | 5378/7983 [00:20<00:09, 264.90it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  68%|██████▊   | 5405/7983 [00:20<00:09, 261.28it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  68%|██████▊   | 5432/7983 [00:20<00:09, 262.70it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  68%|██████▊   | 5462/7983 [00:20<00:09, 271.02it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  69%|██████▉   | 5490/7983 [00:20<00:09, 266.53it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  69%|██████▉   | 5518/7983 [00:20<00:09, 267.82it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  69%|██████▉   | 5545/7983 [00:20<00:09, 260.70it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  70%|██████▉   | 5572/7983 [00:21<00:09, 250.66it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  70%|███████   | 5598/7983 [00:21<00:09, 252.33it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  70%|███████   | 5624/7983 [00:21<00:09, 250.31it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  71%|███████   | 5650/7983 [00:21<00:09, 248.40it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  71%|███████   | 5676/7983 [00:21<00:09, 248.83it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  71%|███████▏  | 5701/7983 [00:21<00:09, 242.29it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  72%|███████▏  | 5726/7983 [00:21<00:09, 243.70it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  72%|███████▏  | 5752/7983 [00:21<00:08, 248.35it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  72%|███████▏  | 5777/7983 [00:21<00:08, 247.89it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  73%|███████▎  | 5803/7983 [00:21<00:08, 249.67it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  73%|███████▎  | 5833/7983 [00:22<00:08, 262.35it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  73%|███████▎  | 5860/7983 [00:22<00:08, 264.44it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  74%|███████▎  | 5887/7983 [00:22<00:08, 261.69it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  74%|███████▍  | 5918/7983 [00:22<00:07, 272.09it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  74%|███████▍  | 5946/7983 [00:22<00:07, 272.03it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  75%|███████▍  | 5974/7983 [00:22<00:07, 256.49it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  75%|███████▌  | 6000/7983 [00:22<00:07, 255.50it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  75%|███████▌  | 6026/7983 [00:22<00:07, 253.83it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  76%|███████▌  | 6052/7983 [00:22<00:07, 252.52it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  76%|███████▌  | 6078/7983 [00:23<00:07, 250.56it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  76%|███████▋  | 6104/7983 [00:23<00:07, 250.20it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  77%|███████▋  | 6131/7983 [00:23<00:07, 252.96it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  77%|███████▋  | 6158/7983 [00:23<00:07, 255.19it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  77%|███████▋  | 6184/7983 [00:23<00:07, 255.18it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  78%|███████▊  | 6214/7983 [00:23<00:06, 265.15it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  78%|███████▊  | 6243/7983 [00:23<00:06, 270.57it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  79%|███████▊  | 6271/7983 [00:23<00:06, 256.53it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  79%|███████▉  | 6299/7983 [00:23<00:06, 261.73it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  79%|███████▉  | 6328/7983 [00:23<00:06, 268.90it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  80%|███████▉  | 6356/7983 [00:24<00:06, 264.88it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  80%|███████▉  | 6386/7983 [00:24<00:05, 274.15it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  80%|████████  | 6417/7983 [00:24<00:05, 282.45it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  81%|████████  | 6446/7983 [00:24<00:05, 277.13it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  81%|████████  | 6474/7983 [00:24<00:05, 274.97it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  81%|████████▏ | 6502/7983 [00:24<00:05, 275.73it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  82%|████████▏ | 6532/7983 [00:24<00:05, 280.21it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  82%|████████▏ | 6561/7983 [00:24<00:05, 265.22it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  83%|████████▎ | 6590/7983 [00:24<00:05, 271.20it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  83%|████████▎ | 6618/7983 [00:25<00:05, 268.79it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  83%|████████▎ | 6646/7983 [00:25<00:04, 269.42it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  84%|████████▎ | 6676/7983 [00:25<00:04, 277.80it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  84%|████████▍ | 6704/7983 [00:25<00:04, 274.46it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  84%|████████▍ | 6732/7983 [00:25<00:04, 272.10it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  85%|████████▍ | 6760/7983 [00:25<00:04, 273.96it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  85%|████████▌ | 6788/7983 [00:25<00:04, 266.77it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  85%|████████▌ | 6816/7983 [00:25<00:04, 269.52it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  86%|████████▌ | 6845/7983 [00:25<00:04, 274.52it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  86%|████████▌ | 6873/7983 [00:25<00:04, 274.51it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  86%|████████▋ | 6901/7983 [00:26<00:04, 260.28it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  87%|████████▋ | 6928/7983 [00:26<00:04, 252.02it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  87%|████████▋ | 6955/7983 [00:26<00:04, 254.67it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  87%|████████▋ | 6981/7983 [00:26<00:03, 253.24it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  88%|████████▊ | 7010/7983 [00:26<00:03, 261.83it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  88%|████████▊ | 7037/7983 [00:26<00:03, 255.87it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  88%|████████▊ | 7064/7983 [00:26<00:03, 256.46it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  89%|████████▉ | 7092/7983 [00:26<00:03, 261.26it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  89%|████████▉ | 7119/7983 [00:26<00:03, 261.10it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  90%|████████▉ | 7146/7983 [00:27<00:03, 258.31it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  90%|████████▉ | 7172/7983 [00:27<00:03, 258.81it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  90%|█████████ | 7198/7983 [00:27<00:03, 258.76it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  91%|█████████ | 7225/7983 [00:27<00:02, 260.33it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  91%|█████████ | 7254/7983 [00:27<00:02, 268.06it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  91%|█████████ | 7281/7983 [00:27<00:02, 268.59it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  92%|█████████▏| 7308/7983 [00:27<00:02, 261.07it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  92%|█████████▏| 7336/7983 [00:27<00:02, 266.29it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  92%|█████████▏| 7363/7983 [00:27<00:02, 266.11it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  93%|█████████▎| 7390/7983 [00:27<00:02, 264.00it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  93%|█████████▎| 7417/7983 [00:28<00:02, 258.94it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  93%|█████████▎| 7446/7983 [00:28<00:02, 264.49it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  94%|█████████▎| 7474/7983 [00:28<00:01, 266.68it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  94%|█████████▍| 7502/7983 [00:28<00:01, 269.43it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  94%|█████████▍| 7529/7983 [00:28<00:01, 261.69it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  95%|█████████▍| 7556/7983 [00:28<00:01, 259.98it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  95%|█████████▍| 7583/7983 [00:28<00:01, 262.84it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  95%|█████████▌| 7610/7983 [00:28<00:01, 259.21it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  96%|█████████▌| 7636/7983 [00:28<00:01, 256.48it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  96%|█████████▌| 7667/7983 [00:28<00:01, 270.26it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  96%|█████████▋| 7695/7983 [00:29<00:01, 265.27it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  97%|█████████▋| 7722/7983 [00:29<00:01, 256.37it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  97%|█████████▋| 7750/7983 [00:29<00:00, 260.94it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  97%|█████████▋| 7777/7983 [00:29<00:00, 257.95it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  98%|█████████▊| 7806/7983 [00:29<00:00, 265.90it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  98%|█████████▊| 7833/7983 [00:29<00:00, 264.30it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  98%|█████████▊| 7860/7983 [00:29<00:00, 253.68it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  99%|█████████▉| 7889/7983 [00:29<00:00, 262.13it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...:  99%|█████████▉| 7919/7983 [00:29<00:00, 271.01it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...: 100%|█████████▉| 7947/7983 [00:30<00:00, 269.35it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Predicting...: 100%|██████████| 7983/7983 [00:30<00:00, 264.34it/s]\n"
     ]
    }
   ],
   "source": [
    "# Call to predict answers functions to write them to predictions file\n",
    "predict_answers(testing_samples, testing_features, results_for_predictions, 20, 30, True, \n",
    "                  op_pred_file, True, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1619502231243,
     "user": {
      "displayName": "chao wang",
      "photoUrl": "",
      "userId": "16929231235898515362"
     },
     "user_tz": -480
    },
    "id": "LDWdmf71f8sI",
    "outputId": "3b8d9054-4f5c-44d9-ebaa-f385ca5294de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"children_stories\": {\n",
      "    \"em\": 60.7,\n",
      "    \"f1\": 72.7,\n",
      "    \"turns\": 1425\n",
      "  },\n",
      "  \"literature\": {\n",
      "    \"em\": 59.3,\n",
      "    \"f1\": 70.0,\n",
      "    \"turns\": 1630\n",
      "  },\n",
      "  \"mid-high_school\": {\n",
      "    \"em\": 56.4,\n",
      "    \"f1\": 67.8,\n",
      "    \"turns\": 1653\n",
      "  },\n",
      "  \"news\": {\n",
      "    \"em\": 61.5,\n",
      "    \"f1\": 72.9,\n",
      "    \"turns\": 1649\n",
      "  },\n",
      "  \"wikipedia\": {\n",
      "    \"em\": 65.5,\n",
      "    \"f1\": 76.5,\n",
      "    \"turns\": 1626\n",
      "  },\n",
      "  \"reddit\": {\n",
      "    \"em\": 0.0,\n",
      "    \"f1\": 0.0,\n",
      "    \"turns\": 0\n",
      "  },\n",
      "  \"science\": {\n",
      "    \"em\": 0.0,\n",
      "    \"f1\": 0.0,\n",
      "    \"turns\": 0\n",
      "  },\n",
      "  \"in_domain\": {\n",
      "    \"em\": 60.7,\n",
      "    \"f1\": 72.0,\n",
      "    \"turns\": 7983\n",
      "  },\n",
      "  \"out_domain\": {\n",
      "    \"em\": 0.0,\n",
      "    \"f1\": 0.0,\n",
      "    \"turns\": 0\n",
      "  },\n",
      "  \"overall\": {\n",
      "    \"em\": 60.7,\n",
      "    \"f1\": 72.0,\n",
      "    \"turns\": 7983\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "#!python3 evaluate.py --data-file data/coqa-dev-v1.0.json --pred-file outputs/Output_Preds_bert_i.json\n",
    "!python3 evaluate.py --data-file data/coqa-dev-v1.0.json --pred-file outputs/Output_Preds_albert_base_16_l1.json\n",
    "#!python3 evaluate.py --data-file data/coqa-dev-v1.0_.json --pred-file outputs/Output_Preds_roberta.json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xxuOCB3gA0Qi"
   },
   "outputs": [],
   "source": [
    "torch.save(testing_data, 'test_tensor_albert.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SD1NQowwuZKh"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "albert_base_16_l1.ipynb",
   "version": ""
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0682a180e3334774ab0655eaa4ee982e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4a700d000b1b4eaeb0ee823f7ed5e888",
      "max": 684,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_915f9db48a674af599f77a26d2b5a737",
      "value": 684
     }
    },
    "0e2e5777e91f42c8be33f1b25794364e": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "130ed4ef2c8c4e29859a55141c2a5551": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4a004d12a13046a1bfce8a5fd585f4c6",
      "max": 760289,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_bce26919344f4e99b8d84d4096da8ba7",
      "value": 760289
     }
    },
    "18860b6d27504f73bbb4db2134496c47": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "20b0f6d986fb4db7aca1302a9d7707e7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_eae35f31c5f14ea681c15a1e75ecc29f",
      "placeholder": "​",
      "style": "IPY_MODEL_ae5a785e7bc54b0daf6ad452789375d6",
      "value": " 1.31M/1.31M [00:02&lt;00:00, 528kB/s]"
     }
    },
    "36627eed0b93483fbd723274d2232446": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3b6377a21838439892782847fbba74ce": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4a004d12a13046a1bfce8a5fd585f4c6": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4a700d000b1b4eaeb0ee823f7ed5e888": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "508d822197f244e9a9dac0e74db435ce": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "574cca850a8d43e2878d5de26db40663": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0682a180e3334774ab0655eaa4ee982e",
       "IPY_MODEL_58b6746b0d7c4804a5d1e9fcc88bcdfa"
      ],
      "layout": "IPY_MODEL_508d822197f244e9a9dac0e74db435ce"
     }
    },
    "58b6746b0d7c4804a5d1e9fcc88bcdfa": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0e2e5777e91f42c8be33f1b25794364e",
      "placeholder": "​",
      "style": "IPY_MODEL_3b6377a21838439892782847fbba74ce",
      "value": " 684/684 [00:00&lt;00:00, 2.28kB/s]"
     }
    },
    "5aaaeb2f83dd48d8bcbb986465b4086e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_752476ea90b442678f2a27b8f6c2b3c1",
      "placeholder": "​",
      "style": "IPY_MODEL_cbd6c8b91f134470a5d6ea1be53fa2eb",
      "value": " 760k/760k [00:04&lt;00:00, 153kB/s]"
     }
    },
    "66eb660b51e84814ac5ae6e5b6e3d34b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "752476ea90b442678f2a27b8f6c2b3c1": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "765ca20b15ff4ffe88f6e30039a31169": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_130ed4ef2c8c4e29859a55141c2a5551",
       "IPY_MODEL_5aaaeb2f83dd48d8bcbb986465b4086e"
      ],
      "layout": "IPY_MODEL_36627eed0b93483fbd723274d2232446"
     }
    },
    "8bf06953f247413aa3a63c899452602e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "915f9db48a674af599f77a26d2b5a737": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "963f4c82ba394743b4d0a35fe5b36582": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c497c10023364f6894fdb89960ea1984",
       "IPY_MODEL_20b0f6d986fb4db7aca1302a9d7707e7"
      ],
      "layout": "IPY_MODEL_d6f43618c55d4af4ab6dbb876fd6b42b"
     }
    },
    "978d1d5d6c9a4f8f977beb63b0cf8c51": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b67242eba0e244fa99b27892f7798302",
      "placeholder": "​",
      "style": "IPY_MODEL_66eb660b51e84814ac5ae6e5b6e3d34b",
      "value": " 47.4M/47.4M [00:01&lt;00:00, 30.2MB/s]"
     }
    },
    "a72b556931ec4caca0ad51a102ee753e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "ae5a785e7bc54b0daf6ad452789375d6": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b1df670fea4445d18dedfadef045d7ec": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b67242eba0e244fa99b27892f7798302": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bab687c5f1264a7ba9ff8401a130f730": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bce26919344f4e99b8d84d4096da8ba7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "c497c10023364f6894fdb89960ea1984": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bab687c5f1264a7ba9ff8401a130f730",
      "max": 1312669,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a72b556931ec4caca0ad51a102ee753e",
      "value": 1312669
     }
    },
    "c4d9e1aac8cb4ad89ba39b4622194e62": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f459ce4071ec471ab61e21345915f2b6",
       "IPY_MODEL_978d1d5d6c9a4f8f977beb63b0cf8c51"
      ],
      "layout": "IPY_MODEL_18860b6d27504f73bbb4db2134496c47"
     }
    },
    "cbd6c8b91f134470a5d6ea1be53fa2eb": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d6f43618c55d4af4ab6dbb876fd6b42b": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "eae35f31c5f14ea681c15a1e75ecc29f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f459ce4071ec471ab61e21345915f2b6": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b1df670fea4445d18dedfadef045d7ec",
      "max": 47376696,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8bf06953f247413aa3a63c899452602e",
      "value": 47376696
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
